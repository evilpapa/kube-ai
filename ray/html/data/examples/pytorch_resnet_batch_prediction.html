
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Image Classification Batch Inference with PyTorch &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../_static/js/csat.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/data/examples/pytorch_resnet_batch_prediction.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Object Detection Batch Inference with PyTorch" href="batch_inference_object_detection.html" />
    <link rel="prev" title="Image Classification Batch Inference with Huggingface Vision Transformer" href="huggingface_vit_batch_prediction.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   入门「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../data.html">
   Ray 数据「85%」
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../overview.html">
     概述
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../key-concepts.html">
     关键概念
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../user-guide.html">
     用户指南
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     Ray Data 示例
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="huggingface_vit_batch_prediction.html">
       Image Classification Batch Inference with Huggingface Vision Transformer
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Image Classification Batch Inference with PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="batch_inference_object_detection.html">
       Object Detection Batch Inference with PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="nyc_taxi_basic_processing.html">
       Processing NYC taxi data using Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="batch_training.html">
       Batch Training with Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ocr_example.html">
       Scaling OCR using Ray Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="random-access.html">
       Random Data Access (Experimental)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="custom-datasource.html">
       Implementing a Custom Datasource
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/api.html">
     Ray 数据 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-internals.html">
     Ray Data 内部结构
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../train/train.html">
   Ray 训练「95%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Fdata/examples/pytorch_resnet_batch_prediction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/data/examples/pytorch_resnet_batch_prediction.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/data/examples/pytorch_resnet_batch_prediction.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-reading-the-dataset-from-s3">
   Step 1: Reading the Dataset from S3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-inference-on-a-single-batch">
   Step 2: Inference on a single batch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-3-scaling-up-to-the-full-dataset-with-ray-data">
   Step 3: Scaling up to the full Dataset with Ray Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing">
     Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-inference">
     Model Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#verify-and-save-results">
     Verify and Save Results
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Image Classification Batch Inference with PyTorch</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-reading-the-dataset-from-s3">
   Step 1: Reading the Dataset from S3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-inference-on-a-single-batch">
   Step 2: Inference on a single batch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-3-scaling-up-to-the-full-dataset-with-ray-data">
   Step 3: Scaling up to the full Dataset with Ray Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing">
     Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-inference">
     Model Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#verify-and-save-results">
     Verify and Save Results
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="image-classification-batch-inference-with-pytorch">
<h1>Image Classification Batch Inference with PyTorch<a class="headerlink" href="#image-classification-batch-inference-with-pytorch" title="Permalink to this headline">#</a></h1>
<p>In this example, we will introduce how to use the <a class="reference internal" href="../data.html#data"><span class="std std-ref">Ray Data</span></a> for <strong>large-scale batch inference with multiple GPU workers.</strong></p>
<p>In particular, we will:</p>
<ul class="simple">
<li><p>Load Imagenette dataset from S3 bucket and create a Ray Dataset.</p></li>
<li><p>Load a pretrained ResNet model.</p></li>
<li><p>Use Ray Data to preprocess the dataset and do model inference parallelizing across multiple GPUs</p></li>
<li><p>Evaluate the predictions and save results to S3/local disk.</p></li>
</ul>
<p>This example will still work even if you do not have GPUs available, but overall performance will be slower.</p>
<p><strong>See <a class="reference internal" href="../batch_inference.html#batch-inference-home"><span class="std std-ref">this guide on batch inference</span></a> for tips and troubleshooting when adapting this example to use your own model and dataset!</strong></p>
<p>To run this example, you will need the following packages:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install -q <span class="s2">&quot;ray[data]&quot;</span> torch torchvision
</pre></div>
</div>
</div>
</div>
<section id="step-1-reading-the-dataset-from-s3">
<h2>Step 1: Reading the Dataset from S3<a class="headerlink" href="#step-1-reading-the-dataset-from-s3" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://github.com/fastai/imagenette">Imagenette</a> is a subset of Imagenet with 10 classes. We have this dataset hosted publicly in an S3 bucket. Since we are only doing inference here, we load in just the validation split.</p>
<p>Here, we use <a class="reference internal" href="../api/doc/ray.data.read_images.html#ray.data.read_images" title="ray.data.read_images"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.read_images</span></code></a> to load the validation set from S3. Ray Data also supports reading from a variety of other <a class="reference internal" href="../loading-data.html#loading-data"><span class="std std-ref">datasources and formats</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">s3_uri</span> <span class="o">=</span> <span class="s2">&quot;s3://anonymous@air-example-data-2/imagenette2/train/&quot;</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_images</span><span class="p">(</span><span class="n">s3_uri</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
<span class="n">ds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:23:57,184	INFO worker.py:1452 -- Connecting to existing Ray cluster at address: 10.0.5.141:6379...
2023-06-27 23:23:57,228	INFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at <span class=" -Color -Color-Bold -Color-Bold-Green">https://session-kncgqf3p7w2j7qcsnz2safl4tj.i.anyscaleuserdata-staging.com </span>
2023-06-27 23:23:57,243	INFO packaging.py:347 -- Pushing file package &#39;gcs://_ray_pkg_32ef287a3a39e82021e70d2413880a69.zip&#39; (4.49MiB) to Ray cluster...
2023-06-27 23:23:57,257	INFO packaging.py:360 -- Successfully pushed file package &#39;gcs://_ray_pkg_32ef287a3a39e82021e70d2413880a69.zip&#39;.
2023-06-27 23:23:59,629	WARNING dataset.py:253 -- <span class=" -Color -Color-Yellow">Important: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.</span>

<span class=" -Color -Color-Yellow">Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode</span>
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "77b499e70ec04abdae9d7aa368cf01ab", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<p>Inspecting the schema, we can see that there is 1 column in the dataset containing the images stored as Numpy arrays.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span><span class="o">.</span><span class="n">schema</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column  Type
------  ----
image   numpy.ndarray(ndim=3, dtype=uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-inference-on-a-single-batch">
<h2>Step 2: Inference on a single batch<a class="headerlink" href="#step-2-inference-on-a-single-batch" title="Permalink to this headline">#</a></h2>
<p>Next, we can do inference on a single batch of data, using a pre-trained ResNet18 model and following <a class="reference external" href="https://pytorch.org/vision/main/models.html#classification">this PyTorch example</a>.</p>
<p>Let’s get a batch of 10 from our dataset. Each image in the batch is represented as a Numpy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">single_batch</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can visualize 1 image from this batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">single_batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">img</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/pytorch_resnet_batch_prediction_12_0.png" src="../../_images/pytorch_resnet_batch_prediction_12_0.png" />
</div>
</div>
<p>Now, let’s download a pre-trained PyTorch Resnet model and get the required preprocessing transforms to preprocess the images prior to prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">ResNet152_Weights</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet152_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span>

<span class="c1"># Load the pretrained resnet model and move to GPU if one is available.</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet152</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">imagenet_transforms</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">imagenet_transforms</span><span class="p">()])</span>
</pre></div>
</div>
</div>
</div>
<p>Then, we apply the transforms to our batch of images, and pass the batch to the model for inference, making sure to use the GPU device for inference.</p>
<p>We can see that most of the images in the batch have been correctly classified as “tench” which is a type of fish.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformed_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">single_batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]]</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">prediction_results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">transformed_batch</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">prediction_results</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="k">del</span> <span class="n">model</span>  <span class="c1"># Free up GPU memory</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">]</span>
<span class="n">labels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;tench&#39;,
 &#39;bittern&#39;,
 &#39;tench&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-scaling-up-to-the-full-dataset-with-ray-data">
<h2>Step 3: Scaling up to the full Dataset with Ray Data<a class="headerlink" href="#step-3-scaling-up-to-the-full-dataset-with-ray-data" title="Permalink to this headline">#</a></h2>
<p>By using Ray Data, we can apply the same logic in the previous section to scale up to the entire dataset, leveraging all the GPUs in our cluster.</p>
<section id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">#</a></h3>
<p>First let’s convert the preprocessing code to Ray Data. We’ll package the preprocessing code within a <code class="docutils literal notranslate"><span class="pre">preprocess_image</span></code> function. This function should take only one argument, which is a dict that contains a single image in the dataset, represented as a numpy array. We use the same <code class="docutils literal notranslate"><span class="pre">transform</span></code> function that was defined above and store the transformed image in a new <code class="docutils literal notranslate"><span class="pre">transformed_image</span></code> field.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">row</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;original_image&quot;</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span>
        <span class="s2">&quot;transformed_image&quot;</span><span class="p">:</span> <span class="n">transform</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]),</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Then we use the <a class="reference internal" href="../api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map" title="ray.data.Dataset.map"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map()</span></code></a> API to apply the function to the whole dataset row by row. We use this instead of <a class="reference internal" href="../api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_batches()</span></code></a> because the <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> transforms must be applied one image at a time due to the dataset containing images of different sizes.</p>
<p>By using Ray Data’s <code class="docutils literal notranslate"><span class="pre">map</span></code>, we can scale out the preprocessing to utilize all the resources in our Ray cluster.</p>
<p>Note: the <code class="docutils literal notranslate"><span class="pre">map</span></code> method is lazy, it won’t perform execution until we consume the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformed_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">preprocess_image</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:25:59,387	WARNING dataset.py:4384 -- The `map`, `flat_map`, and `filter` operations are unvectorized and can be very slow. If you&#39;re using a vectorized transformation, consider using `.map_batches()` instead.
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-inference">
<h3>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline">#</a></h3>
<p>Next, let’s convert the model inference part. Compared with preprocessing, model inference has 2 differences:</p>
<ol class="simple">
<li><p>Model loading and initialization is usually expensive.</p></li>
<li><p>Model inference can be optimized with hardware acceleration if we process data in batches. Using larger batches improves GPU utilization and the overall runtime of the inference job.</p></li>
</ol>
<p>Thus, we convert the model inference code to the following <code class="docutils literal notranslate"><span class="pre">ResnetModel</span></code> class. In this class, we put the expensive model loading and initialization code in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> constructor, which will run only once. And we put the model inference code in the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method, which will be called for each batch.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method takes a batch of data items, instead of a single one. In this case, the batch is also a dict that has the <code class="docutils literal notranslate"><span class="pre">&quot;transformed_image&quot;</span></code> key populated by our preprocessing step, and the value is a Numpy array of images represented in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format. We reuse the same inferencing logic from step 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">class</span> <span class="nc">ResnetModel</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet152_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet152</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]):</span>
        <span class="c1"># Convert the numpy array of images into a PyTorch tensor.</span>
        <span class="c1"># Move the tensor batch to GPU if available.</span>
        <span class="n">torch_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;transformed_image&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">torch_batch</span><span class="p">)</span>
            <span class="n">predicted_classes</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">predicted_labels</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">predicted_classes</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s2">&quot;predicted_label&quot;</span><span class="p">:</span> <span class="n">predicted_labels</span><span class="p">,</span>
                <span class="s2">&quot;original_image&quot;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;original_image&quot;</span><span class="p">],</span>
            <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Then we use the <a class="reference internal" href="../api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_batches()</span></code></a> API to apply the model to the whole dataset.</p>
<p>The first parameter of <code class="docutils literal notranslate"><span class="pre">map_batches</span></code> is the user-defined function (UDF), which can either be a function or a class. Since we are using a class in this case, the UDF will run as long-running <a class="reference internal" href="../../ray-core/actors.html#actor-guide"><span class="std std-ref">Ray actors</span></a>. For class-based UDFs, we use the <code class="docutils literal notranslate"><span class="pre">compute</span></code> argument to specify <a class="reference internal" href="../api/doc/ray.data.ActorPoolStrategy.html#ray.data.ActorPoolStrategy" title="ray.data.ActorPoolStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">ActorPoolStrategy</span></code></a> with the number of parallel actors.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> argument indicates the number of images in each batch. See the Ray dashboard
for GPU memory usage to experiment with the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> when using your own model and dataset.
You should aim to max out the batch size without running out of GPU memory.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> argument specifies the number of GPUs needed for each <code class="docutils literal notranslate"><span class="pre">ResnetModel</span></code> instance. In this case, we want 1 GPU for each model replica. If you are doing CPU inference, you can remove the <code class="docutils literal notranslate"><span class="pre">num_gpus=1</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">transformed_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span>
    <span class="n">ResnetModel</span><span class="p">,</span>
    <span class="n">compute</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ActorPoolStrategy</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="mi">4</span>
    <span class="p">),</span>  <span class="c1"># Use 4 GPUs. Change this number based on the number of GPUs in your cluster.</span>
    <span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Specify 1 GPU per model replica.</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">720</span><span class="p">,</span>  <span class="c1"># Use the largest batch size that can fit on our GPUs</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="verify-and-save-results">
<h3>Verify and Save Results<a class="headerlink" href="#verify-and-save-results" title="Permalink to this headline">#</a></h3>
<p>Let’s take a small batch of predictions and verify the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction_batch</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">take_batch</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:26:04,893	INFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[ReadImage-&gt;Map] -&gt; ActorPoolMapOperator[MapBatches(ResnetModel)]
2023-06-27 23:26:04,894	INFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2023-06-27 23:26:04,895	INFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2023-06-27 23:26:04,950	INFO actor_pool_map_operator.py:114 -- MapBatches(ResnetModel): Waiting for 4 pool actors to start...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c8f50ee338cc4e088d5630193cf1e201", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:26:29,120	INFO streaming_executor.py:149 -- Shutting down &lt;StreamingExecutor(Thread-36, started daemon 140560158410496)&gt;.
2023-06-27 23:26:29,335	WARNING actor_pool_map_operator.py:264 -- To ensure full parallelization across an actor pool of size 4, the specified batch size should be at most 360. Your configured batch size for this operator was 720.
</pre></div>
</div>
</div>
</div>
<p>We see that all the images are correctly classified as “tench”, which is a type of fish.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="n">prediction_batch</span><span class="p">[</span><span class="s2">&quot;original_image&quot;</span><span class="p">],</span> <span class="n">prediction_batch</span><span class="p">[</span><span class="s2">&quot;predicted_label&quot;</span><span class="p">]</span>
<span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label: &quot;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/pytorch_resnet_batch_prediction_33_0.png" src="../../_images/pytorch_resnet_batch_prediction_33_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench
</pre></div>
</div>
<img alt="../../_images/pytorch_resnet_batch_prediction_33_2.png" src="../../_images/pytorch_resnet_batch_prediction_33_2.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench
</pre></div>
</div>
<img alt="../../_images/pytorch_resnet_batch_prediction_33_4.png" src="../../_images/pytorch_resnet_batch_prediction_33_4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench
</pre></div>
</div>
<img alt="../../_images/pytorch_resnet_batch_prediction_33_6.png" src="../../_images/pytorch_resnet_batch_prediction_33_6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench
</pre></div>
</div>
<img alt="../../_images/pytorch_resnet_batch_prediction_33_8.png" src="../../_images/pytorch_resnet_batch_prediction_33_8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Label:  tench
</pre></div>
</div>
</div>
</div>
<p>If the samples look good, we can proceed with saving the results to an external storage, e.g., S3 or local disks. See <a class="reference internal" href="../saving-data.html#saving-data"><span class="std std-ref">the guide on saving data</span></a> for all supported storage and file formats.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tempfile</span>

<span class="n">temp_dir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>

<span class="c1"># First, drop the original images to avoid them being saved as part of the predictions.</span>
<span class="c1"># Then, write the predictions in parquet format to a path with the `local://` prefix</span>
<span class="c1"># to make sure all results get written on the head node.</span>
<span class="n">predictions</span><span class="o">.</span><span class="n">drop_columns</span><span class="p">([</span><span class="s2">&quot;original_image&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;local://</span><span class="si">{</span><span class="n">temp_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictions saved to `</span><span class="si">{</span><span class="n">temp_dir</span><span class="si">}</span><span class="s2">`!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:26:38,105	INFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[ReadImage-&gt;Map] -&gt; ActorPoolMapOperator[MapBatches(ResnetModel)] -&gt; TaskPoolMapOperator[MapBatches(&lt;lambda&gt;)] -&gt; TaskPoolMapOperator[Write]
2023-06-27 23:26:38,106	INFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
2023-06-27 23:26:38,106	INFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
2023-06-27 23:26:38,141	INFO actor_pool_map_operator.py:114 -- MapBatches(ResnetModel): Waiting for 4 pool actors to start...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6e48ded298584db880f6e68ddcb9772c", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-06-27 23:27:27,855	INFO streaming_executor.py:149 -- Shutting down &lt;StreamingExecutor(Thread-74, stopped daemon 140560149755648)&gt;.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions saved to `/tmp/tmp0y52g_f5`!
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="huggingface_vit_batch_prediction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Image Classification Batch Inference with Huggingface Vision Transformer</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="batch_inference_object_detection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Object Detection Batch Inference with PyTorch</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>