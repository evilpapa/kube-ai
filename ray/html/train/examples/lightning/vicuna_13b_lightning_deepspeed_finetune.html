
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Fine-tune vicuna-13b with Lightning and DeepSpeed &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/versionwarning.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../../_static/js/csat.js"></script>
    <script defer="defer" src="../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../_static/js/top-navigation.js"></script>
    <script src="../../../_static/js/tags.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/getting-started.html">
   入门「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data/data.html">
   Ray 数据「85%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../train.html">
   Ray 训练「95%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../serve/index.html">
   Ray Serve「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rllib/index.html">
   Ray RLlib「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/train/examples/lightning/vicuna_13b_lightning_deepspeed_finetune.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cluster-setting">
   Cluster Setting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compute-instances">
     Compute instances
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cloud-storage">
     Cloud Storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-storage">
     Local Storage
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup-ray-environment">
   Setup Ray Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-preprocess-datasets">
   Load and preprocess datasets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-a-lightning-module">
   Define a Lightning Module
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deepspeed-configurations">
   DeepSpeed Configurations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-your-training-function">
   Define your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fine-tuning">
   Model Fine-tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#llm-inference">
   LLM Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-and-process-your-checkpoints">
     Download and Process your checkpoints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-generation-pipeline">
     Initialize Generation Pipeline
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-study">
     Case Study
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-the-generated-code-snippets">
     Test the Generated Code Snippets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References:
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fine-tune vicuna-13b with Lightning and DeepSpeed</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cluster-setting">
   Cluster Setting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compute-instances">
     Compute instances
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cloud-storage">
     Cloud Storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#local-storage">
     Local Storage
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup-ray-environment">
   Setup Ray Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-preprocess-datasets">
   Load and preprocess datasets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-a-lightning-module">
   Define a Lightning Module
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deepspeed-configurations">
   DeepSpeed Configurations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-your-training-function">
   Define your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-fine-tuning">
   Model Fine-tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#llm-inference">
   LLM Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-and-process-your-checkpoints">
     Download and Process your checkpoints
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialize-generation-pipeline">
     Initialize Generation Pipeline
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-study">
     Case Study
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-the-generated-code-snippets">
     Test the Generated Code Snippets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References:
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="fine-tune-vicuna-13b-with-lightning-and-deepspeed">
<span id="vicuna-lightning-deepspeed-finetuning"></span><h1>Fine-tune <code class="docutils literal notranslate"><span class="pre">vicuna-13b</span></code> with Lightning and DeepSpeed<a class="headerlink" href="#fine-tune-vicuna-13b-with-lightning-and-deepspeed" title="Permalink to this headline">#</a></h1>
<p>In this example, we will demonstrate how to perform full fine-tuning for a <a class="reference external" href="https://huggingface.co/lmsys/vicuna-13b-v1.3"><code class="docutils literal notranslate"><span class="pre">vicuna-13b-v1.3</span></code></a> model using Ray Train PyTorch Lightning integrations with the DeepSpeed ZeRO-3 strategy.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> is an open-source deep learning optimization library for PyTorch. It’s designed to reduce computing power and memory usage, and to train large distributed models by leveraging state-of-the-art innovations like ZeRO, 3D-Parallelism, DeepSpeed-MoE, and ZeRO-Infinity.</p></li>
<li><p>PyTorch Lightning offers a <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/api/pytorch_lightning.strategies.DeepSpeedStrategy.html">DeepSpeed integration</a>, which provides a simple interface to configure the knobs for DeepSpeed and automatically trigger your training process with the DeepSpeed Engine.</p></li>
<li><p><a class="reference internal" href="../../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">TorchTrainer</span></code></a> allows you to easily scale your PyTorch Lightning job across multiple nodes in a Ray cluster, without worrying about the underlying cluster management, autoscaling, and distributed process group settings.</p></li>
</ul>
<p>Our demo aims to illustrate how these three tools can be combined effectively to finetune the Vicuna-13B model, leveraging the strengths of each to create an efficient and high-performance deep learning solution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an advanced example of Large Language Model fine-tuning with Ray Train. If you’re a beginner or new to the concepts of Ray Train and our Lightning integrations, it would be beneficial to first explore the introductory documentation below to build a foundational understanding.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../overview.html#train-key-concepts"><span class="std std-ref">Ray Train Key Concepts</span></a></p></li>
<li><p><a class="reference internal" href="../../../data/key-concepts.html#data-key-concepts"><span class="std std-ref">Ray Data Key Concepts</span></a></p></li>
<li><p><a class="reference internal" href="lightning_mnist_example.html#lightning-mnist-example"><span class="std std-ref">[Basic] Image Classification with PyTorch Lightning and Ray Train</span></a></p></li>
<li><p><a class="reference internal" href="lightning_cola_advanced.html#lightning-advanced-example"><span class="std std-ref">[Intermediate] Fine-tuning Lightning Modules with with Ray Data</span></a></p></li>
</ul>
</div>
<section id="cluster-setting">
<h2>Cluster Setting<a class="headerlink" href="#cluster-setting" title="Permalink to this headline">#</a></h2>
<section id="compute-instances">
<h3>Compute instances<a class="headerlink" href="#compute-instances" title="Permalink to this headline">#</a></h3>
<p>In this example, we set up a Ray cluster on AWS with the following settings:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>num</p></th>
<th class="head"><p>instance type</p></th>
<th class="head"><p>GPU per node</p></th>
<th class="head"><p>GPU Memory</p></th>
<th class="head"><p>CPU Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Head node</p></td>
<td><p>1</p></td>
<td><p>g5.16xlarge</p></td>
<td><p>1 x A10G</p></td>
<td><p>24 GB</p></td>
<td><p>256 GB</p></td>
</tr>
<tr class="row-odd"><td><p>Worker node</p></td>
<td><p>15</p></td>
<td><p>g5.4xlarge</p></td>
<td><p>1 x A10G</p></td>
<td><p>24 GB</p></td>
<td><p>64 GB</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this example, we used 16 A10G GPUs for model training and tuned the DeepSpeed configurations for this setup. If you have a different cluster setup or GPUs with lower memory capacities, you may need to modify the DeepSpeed configurations and batch size to fit the model into the GPUs.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We selected a GPU instance with additional CPU memory for the head node to demonstrate single-node offline inference. If you are training only, you can still opt for the g5.4xlarge instance for the head node.</p>
</div>
</section>
<section id="cloud-storage">
<h3>Cloud Storage<a class="headerlink" href="#cloud-storage" title="Permalink to this headline">#</a></h3>
<p>Additionally, since the checkpoint size for this 13B parameter model can be large (~140GB), we choose to store the checkpoints in AWS S3. Thanks to the newly introduced distributed checkpointing feature in Ray 2.5, each worker can upload its own shards individually to the S3 bucket, greatly reducing the latency and network traffic of checkpoint syncing.</p>
</section>
<section id="local-storage">
<h3>Local Storage<a class="headerlink" href="#local-storage" title="Permalink to this headline">#</a></h3>
<p>To demonstrate offline inference, we need to download and consolidate the model checkpoint onto the head node. This action requires around 200GB disk storage. Therefore, we mounted the NVMe SSD provided by g5 instances at <code class="docutils literal notranslate"><span class="pre">/dev/nvme1n1</span></code> to <code class="docutils literal notranslate"><span class="pre">/mnt/local_storage</span></code>, and we will save the checkpoints in this folder.</p>
<p>For more details, see <a class="reference external" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/nvme-ebs-volumes.html">Amazon EBS and NVMe on Linux instances</a>.</p>
</section>
</section>
<section id="setup-ray-environment">
<h2>Setup Ray Environment<a class="headerlink" href="#setup-ray-environment" title="Permalink to this headline">#</a></h2>
<p>We define a runtime environment to ensure that the Ray workers have access to all necessary packages. If you have already included these dependencies in your Docker image or installed them on each node, you can ignore the <code class="docutils literal notranslate"><span class="pre">runtime_env</span></code> argument.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that the codebases of <code class="docutils literal notranslate"><span class="pre">transformers</span></code>, <code class="docutils literal notranslate"><span class="pre">accelerate</span></code>, and <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> are all rapidly changing, so we have pinned the package versions here to ensure testing stability. You can try other version combinations and feel free to report any issues you encounter.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">BATCH_SIZE_PER_WORKER</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;lmsys/vicuna-13b-v1.3&quot;</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
    <span class="n">runtime_env</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;pip&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;datasets==2.13.1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;torch&gt;=1.13.0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;deepspeed==0.9.4&quot;</span><span class="p">,</span>
            <span class="s2">&quot;accelerate==0.20.3&quot;</span><span class="p">,</span>
            <span class="s2">&quot;transformers==4.30.2&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pytorch_lightning==2.0.3&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="s2">&quot;env_vars&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;RAY_AIR_NEW_PERSISTENCE_MODE&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}</span> <span class="c1"># TODO(@justinvyu): Remove it</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-and-preprocess-datasets">
<h2>Load and preprocess datasets<a class="headerlink" href="#load-and-preprocess-datasets" title="Permalink to this headline">#</a></h2>
<p>We were impressed by LLM’s ability of zero-shot text-generation, while some LLMs may not perform well in code generation due to the lack of code in the training corpus. The CMU <a class="reference external" href="https://conala-corpus.github.io/">CoNaLa</a>(The Code/Natural Language Challenge) was designed to test systems for generating program snippets from natural language. Each data record contains an intent sentence and a one-line code snippet. The goal is to fine-tune the Vicuna model on this dataset, enabling the model to generate correct and runnable code snippets, thereby achieving natural language intent. Here are some examples:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>intent</p></th>
<th class="head"><p>code snippet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“convert a list of integers into a single integer”</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">=</span> <span class="pre">int(''.join(map(str,</span> <span class="pre">x)))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>“normalize a pandas dataframe <code class="docutils literal notranslate"><span class="pre">df</span></code> by row”</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">df.div(df.sum(axis=1),</span> <span class="pre">axis=0)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>“Convert string ‘03:55’ into datetime.time object”</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">datetime.datetime.strptime('03:55',</span> <span class="pre">'%H:%M').time()</span></code></p></td>
</tr>
</tbody>
</table>
<p>The CoNaLa team has released a dataset crawled from Stack Overflow, automatically filtered, then curated by annotators, split into 2379 training and 500 test examples. In addition, they also included an automatically-mined dataset with 600k examples. In this demo, we take all the curated data and the top 5000 mined data for fine-tuning.</p>
<p>Here we preprocess the CoNaLa dataset with Ray Data. You can also use HuggingFace Datasets and pass it directly to <code class="docutils literal notranslate"><span class="pre">LightningConfigBuilder.fit_params()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">concatenate_datasets</span><span class="p">,</span> <span class="n">load_dataset</span>

<span class="c1"># Combine the curated dataset and automatically-mined dataset</span>
<span class="n">hf_dataset_curated</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;neulab/conala&quot;</span><span class="p">)</span>
<span class="n">hf_dataset_mined</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;neulab/conala&quot;</span><span class="p">,</span> <span class="s2">&quot;mined&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train[:5000]&quot;</span><span class="p">)</span>
<span class="n">hf_dataset_merged</span> <span class="o">=</span> <span class="n">concatenate_datasets</span><span class="p">(</span>
    <span class="p">[</span><span class="n">hf_dataset_curated</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">hf_dataset_mined</span><span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hf_dataset_merged</span><span class="p">)</span>

<span class="c1"># Convert it into Ray Dataset</span>
<span class="n">ray_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">hf_dataset_merged</span><span class="p">)</span>

<span class="c1"># Build a prompt template for Vicuna-13b model</span>
<span class="n">PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="s2">&quot;Intent: </span><span class="si">{intent}</span><span class="se">\n</span><span class="s2">One-line code snippet: </span><span class="si">{snippet}</span><span class="s2">&quot;</span>


<span class="k">def</span> <span class="nf">fill_prompt</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_sentence&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">PROMPT_TEMPLATE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">intent</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;rewritten_intent&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;rewritten_intent&quot;</span><span class="p">]</span>
            <span class="k">else</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;intent&quot;</span><span class="p">],</span>
            <span class="n">snippet</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;snippet&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span><span class="p">[[</span><span class="s2">&quot;input_sentence&quot;</span><span class="p">]]</span>


<span class="c1"># Tokenize input sentences to tensors</span>
<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_sentence&quot;</span><span class="p">]),</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ret</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ret</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

<span class="c1"># Preprocess train dataset</span>
<span class="n">processed_ds</span> <span class="o">=</span> <span class="n">ray_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">fill_prompt</span><span class="p">,</span> <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "535afe3e183b4cdfa61c39cbae788608", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataset({
    features: [&#39;question_id&#39;, &#39;intent&#39;, &#39;rewritten_intent&#39;, &#39;snippet&#39;, &#39;parent_answer_post_id&#39;, &#39;prob&#39;, &#39;id&#39;],
    num_rows: 7379
})
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-a-lightning-module">
<h2>Define a Lightning Module<a class="headerlink" href="#define-a-lightning-module" title="Permalink to this headline">#</a></h2>
<p>Here we load the pre-trained model weights from HuggingFace Model Hub, and wrap them into <code class="docutils literal notranslate"><span class="pre">pl.LightningModule</span></code>. We adopted the efficient model initialization techniques introduced in <a class="reference external" href="https://github.com/Lightning-Universe/lightning-transformers">Lightning-transformers</a> to avoid unnecessary full weights loading.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">deepspeed.ops.adam</span> <span class="kn">import</span> <span class="n">DeepSpeedCPUAdam</span>


<span class="k">class</span> <span class="nc">ZeRO3Config</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">pl_module</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">is_zero3</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">enable_transformers_pretrained_deepspeed_sharding</span><span class="p">(</span>
    <span class="n">pl_module</span><span class="p">:</span> <span class="s2">&quot;pl.LightningModule&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">transformers</span><span class="o">.</span><span class="n">deepspeed</span><span class="o">.</span><span class="n">_hf_deepspeed_config_weak_ref</span> <span class="o">=</span> <span class="n">ZeRO3Config</span><span class="p">(</span><span class="n">pl_module</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Vicuna13BModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Enable tf32 for better performance</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Defer model initialization to inject deepspeed configs to HF.</span>
        <span class="c1"># During initialization, HF transformers can immediately partition </span>
        <span class="c1"># the model across all gpus avoid the overhead in time and memory </span>
        <span class="c1"># copying it on CPU or each GPU first.</span>
        <span class="n">enable_transformers_pretrained_deepspeed_sharding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;DeepSpeed Configs: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Archetecture: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DeepSpeedCPUAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2023-06-30 17:39:35,109] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
</pre></div>
</div>
</div>
</div>
</section>
<section id="deepspeed-configurations">
<h2>DeepSpeed Configurations<a class="headerlink" href="#deepspeed-configurations" title="Permalink to this headline">#</a></h2>
<p>Before training, let’s calculate the memory usage of finetuning a <code class="docutils literal notranslate"><span class="pre">vicuna-13b</span></code> model. Assume we are using FP16 mixed-precision training, and the optimizer is Adam with FP32 states.</p>
<ul class="simple">
<li><p>Model parameters: 13(billion parameters) * 2(FP16) ≈ 26GB</p></li>
<li><p>Optimizer states: 13(billion parameters)  * 2(momentums per param) * 4 (FP32) ≈ 52GB</p></li>
</ul>
<p>As we can see, the model parameters themselves require 26GB, which cannot fit in a single A10G GPU, let alone the activations and optimizers states. Here, we use ZeRO stage-3 to partition the model, gradients, and optimizer states across 16 nodes. Additionally, we employ optimizer CPU offloading to reduce GRAM usage and increase throughput with larger batch sizes. We also disabled parameter offloading and activation checkpointing to improve the training speed.</p>
<p>Regarding other knobs such as <code class="docutils literal notranslate"><span class="pre">reduce_bucket_size</span></code>, <code class="docutils literal notranslate"><span class="pre">stage3_prefetch_bucket_size</span></code> and <code class="docutils literal notranslate"><span class="pre">stage3_param_persistence_threshold</span></code>, we kept them as the <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-config">default values in HuggingFace</a>. Feel free to further adjust them to speed up the training process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">HIDDEN_SIZE</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

<span class="n">deepspeed_configs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;zero_allow_untested_optimizer&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;bf16&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;offload_optimizer&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
        <span class="s2">&quot;overlap_comm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;contiguous_gradients&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;reduce_bucket_size&quot;</span><span class="p">:</span> <span class="n">HIDDEN_SIZE</span> <span class="o">*</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span>
        <span class="s2">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">HIDDEN_SIZE</span> <span class="o">*</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span>
        <span class="s2">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-your-training-function">
<h2>Define your training function<a class="headerlink" href="#define-your-training-function" title="Permalink to this headline">#</a></h2>
<p>Finally, define the training function that will be launched on multiple workers. The training function is generally the same as the pure pytorch Lightning training code, with additional Ray Train utilities:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../api/doc/ray.train.lightning.RayDeepSpeedStrategy.html#ray.train.lightning.RayDeepSpeedStrategy" title="ray.train.lightning.RayDeepSpeedStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayDeepSpeedStrategy</span></code></a>: Same argument list as Lightning DeepSpeedStrategy but integrated with Ray Train.</p></li>
<li><p><a class="reference internal" href="../../api/doc/ray.train.lightning.RayLightningEnvironment.html#ray.train.lightning.RayLightningEnvironment" title="ray.train.lightning.RayLightningEnvironment"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayLightningEnvironment</span></code></a>: Lightning environments for Ray cluster.</p></li>
<li><p><a class="reference internal" href="../../api/doc/ray.train.lightning.RayTrainReportCallback.html#ray.train.lightning.RayTrainReportCallback" title="ray.train.lightning.RayTrainReportCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayTrainReportCallback</span></code></a>: On each epoch end, it reports the checkpoint from each worker to the ray train (distributed checkpointing).</p></li>
<li><p><a class="reference internal" href="../../api/doc/ray.train.lightning.prepare_trainer.html#ray.train.lightning.prepare_trainer" title="ray.train.lightning.prepare_trainer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_trainer()</span></code></a>: Validate your lightning Trainer configurations.</p></li>
</ul>
<p>For Ray Data ingestion, we fetched the preprocessed and sharded dataset with <a class="reference internal" href="../../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_dataset_shard()</span></code></a>, and created a dataloader with <a class="reference internal" href="../../../data/api/doc/ray.data.Dataset.iter_torch_batches.html#ray.data.Dataset.iter_torch_batches" title="ray.data.Dataset.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches()</span></code></a>. It returns a custom iterator that replaces the Torch DataLoader.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray.train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train.lightning</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">prepare_trainer</span><span class="p">,</span>
    <span class="n">RayDeepSpeedStrategy</span><span class="p">,</span> 
    <span class="n">RayLightningEnvironment</span><span class="p">,</span> 
    <span class="n">RayTrainReportCallback</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Training function for each worker.&quot;&quot;&quot;</span>

    <span class="c1"># Unpack the `train_loop_config`</span>
    <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_epochs&quot;</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span>
    <span class="n">accumulate_grad_batches</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;accumulate_grad_batches&quot;</span><span class="p">]</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Vicuna13BModel</span><span class="p">()</span>
    
    <span class="c1"># Prepare Ray Data Ingestion</span>
    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    
    <span class="n">pl_trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="n">devices</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">strategy</span><span class="o">=</span><span class="n">RayDeepSpeedStrategy</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">deepspeed_configs</span><span class="p">),</span>
        <span class="n">plugins</span><span class="o">=</span><span class="p">[</span><span class="n">RayLightningEnvironment</span><span class="p">()],</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">RayTrainReportCallback</span><span class="p">()],</span>
        <span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># RayTrainReportCallback will save the checkpoints</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">max_epochs</span><span class="p">,</span>
        <span class="n">precision</span><span class="o">=</span><span class="s2">&quot;bf16-mixed&quot;</span><span class="p">,</span>
        <span class="n">accumulate_grad_batches</span><span class="o">=</span><span class="n">accumulate_grad_batches</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">pl_trainer</span> <span class="o">=</span> <span class="n">prepare_trainer</span><span class="p">(</span><span class="n">pl_trainer</span><span class="p">)</span>

    <span class="n">pl_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">)</span>
    

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;max_epochs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">BATCH_SIZE_PER_WORKER</span><span class="p">,</span>
        <span class="s2">&quot;accumulate_grad_batches&quot;</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">},</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;vicuna-13b-finetune&quot;</span><span class="p">,</span>
        <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/air-release-tests&quot;</span><span class="p">,</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">num_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
        <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">resources_per_worker</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
    <span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">processed_ds</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-fine-tuning">
<h2>Model Fine-tuning<a class="headerlink" href="#model-fine-tuning" title="Permalink to this headline">#</a></h2>
<p>Once everything is configured in TorchTrainer, training becomes easy. Simply call <code class="docutils literal notranslate"><span class="pre">trainer.fit()</span></code>, and your workload will be scaled to the Ray cluster, initiating ZeRO-3 parallel training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div class="tuneStatus">
  <div style="display: flex;flex-direction: row">
    <div style="display: flex;flex-direction: column;">
      <h3>Tune Status</h3>
      <table>
<tbody>
<tr><td>Current time:</td><td>2023-06-30 18:21:59</td></tr>
<tr><td>Running for: </td><td>00:42:22.75        </td></tr>
<tr><td>Memory:      </td><td>10.7/249.1 GiB     </td></tr>
</tbody>
</table>
    </div>
    <div class="vDivider"></div>
    <div class="systemInfo">
      <h3>System Info</h3>
      Using FIFO scheduling algorithm.<br>Logical resource usage: 241.0/304 CPUs, 16.0/16 GPUs (0.0/16.0 accelerator_type:A10G)
    </div>

  </div>
  <div class="hDivider"></div>
  <div class="trialStatus">
    <h3>Trial Status</h3>
    <table>
<thead>
<tr><th>Trial name                  </th><th>status    </th><th>loc              </th><th style="text-align: right;">  iter</th><th style="text-align: right;">  total time (s)</th><th style="text-align: right;">  train_loss</th><th style="text-align: right;">  epoch</th><th style="text-align: right;">  step</th></tr>
</thead>
<tbody>
<tr><td>LightningTrainer_c1544_00000</td><td>TERMINATED</td><td>10.0.55.20:134103</td><td style="text-align: right;">     1</td><td style="text-align: right;">         2473.94</td><td style="text-align: right;">    0.523438</td><td style="text-align: right;">      0</td><td style="text-align: right;">    29</td></tr>
</tbody>
</table>
  </div>
</div>
<style>
.tuneStatus {
  color: var(--jp-ui-font-color1);
}
.tuneStatus .systemInfo {
  display: flex;
  flex-direction: column;
}
.tuneStatus td {
  white-space: nowrap;
}
.tuneStatus .trialStatus {
  display: flex;
  flex-direction: column;
}
.tuneStatus h3 {
  font-weight: bold;
}
.tuneStatus .hDivider {
  border-bottom-width: var(--jp-border-width);
  border-bottom-color: var(--jp-border-color0);
  border-bottom-style: solid;
}
.tuneStatus .vDivider {
  border-left-width: var(--jp-border-width);
  border-left-color: var(--jp-border-color0);
  border-left-style: solid;
  margin: 0.5em 1em 0.5em 1em;
}
</style>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(pid=134103)</span> [2023-06-30 17:39:41,637] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> <span class=" -Color -Color-Yellow">Important: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Starting distributed worker processes: [&#39;134267 (10.0.55.20)&#39;, &#39;74152 (10.0.63.141)&#39;, &#39;75476 (10.0.51.205)&#39;, &#39;75547 (10.0.42.158)&#39;, &#39;74711 (10.0.45.211)&#39;, &#39;75132 (10.0.20.140)&#39;, &#39;74502 (10.0.60.86)&#39;, &#39;75695 (10.0.53.69)&#39;, &#39;74457 (10.0.47.2)&#39;, &#39;74569 (10.0.33.23)&#39;, &#39;74341 (10.0.29.61)&#39;, &#39;74274 (10.0.36.152)&#39;, &#39;74561 (10.0.35.16)&#39;, &#39;74427 (10.0.16.236)&#39;, &#39;74273 (10.0.54.55)&#39;, &#39;74996 (10.0.9.249)&#39;]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Setting up process group for: env:// [rank=0, world_size=16]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)-&gt;MapBatches(BatchMapper._transform_pandas)] -&gt; AllToAllOperator[RandomizeBlockOrder]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "da7f200767b448d7b409fcdd07daecce", "version_major": 2, "version_minor": 0}
</script><script type="application/vnd.jupyter.widget-view+json">
{"model_id": "d9c76218373645cc99438e1f14133e74", "version_major": 2, "version_minor": 0}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading (…)okenizer_config.json: 100%|██████████| 727/727 [00:00&lt;00:00, 8.86MB/s]m_pandas) pid=74329, ip=10.0.54.55) 
Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00&lt;00:00, 18.2MB/s]ansform_pandas) pid=74329, ip=10.0.54.55) 
Downloading (…)cial_tokens_map.json: 100%|██████████| 435/435 [00:00&lt;00:00, 3.33MB/s]m_pandas) pid=74329, ip=10.0.54.55) 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> [2023-06-30 17:39:54,612] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading (…)okenizer_config.json: 100%|██████████| 727/727 [00:00&lt;00:00, 7.86MB/s]
Downloading (…)okenizer_config.json: 100%|██████████| 727/727 [00:00&lt;00:00, 7.57MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> GPU available: True (cuda), used: True
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> TPU available: False, using: 0 TPU cores
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> IPU available: False, using: 0 IPUs
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> HPU available: False, using: 0 HPUs
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> `Trainer(limit_val_batches=1)` was configured so 1 batch will be used.
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00&lt;?, ?B/s]
Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00&lt;00:00, 14.9MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/16
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74273, ip=10.0.54.55)</span> Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/rank_all/lightning_logs
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:39:55,589] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00&lt;00:00, 18.2MB/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 435/435 [00:00&lt;00:00, 6.49MB/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/585 [00:00&lt;?, ?B/s]
Downloading (…)lve/main/config.json: 100%|██████████| 585/585 [00:00&lt;00:00, 7.81MB/s]
Downloading (…)lve/main/config.json: 100%|██████████| 585/585 [00:00&lt;00:00, 7.09MB/s]
Downloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00&lt;00:00, 35.1MB/s]
Downloading shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75547, ip=10.0.42.158)</span> 
Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00&lt;?, ?B/s]
Downloading (…)l-00001-of-00003.bin:   0%|          | 21.0M/9.95G [00:00&lt;00:59, 167MB/s]
Downloading (…)l-00001-of-00003.bin:   0%|          | 41.9M/9.95G [00:00&lt;00:58, 170MB/s] 
Downloading (…)okenizer_config.json: 100%|██████████| 727/727 [00:00&lt;00:00, 8.33MB/s]<span class=" -Color -Color-Green"> [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)</span>
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00&lt;?, ?B/s]
Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00&lt;00:00, 17.5MB/s]<span class=" -Color -Color-Green"> [repeated 8x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74561, ip=10.0.35.16)</span> initializing deepspeed distributed: GLOBAL_RANK: 12, MEMBER: 13/16<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74561, ip=10.0.35.16)</span> Missing logger folder: /home/ray/ray_results/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/rank_all/lightning_logs<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00&lt;00:00, 8.85MB/s]
Downloading (…)cial_tokens_map.json: 100%|██████████| 435/435 [00:00&lt;00:00, 5.23MB/s]<span class=" -Color -Color-Green"> [repeated 10x across cluster]</span>
Downloading (…)lve/main/config.json: 100%|██████████| 585/585 [00:00&lt;00:00, 7.03MB/s]<span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
Downloading (…)model.bin.index.json: 100%|██████████| 33.4k/33.4k [00:00&lt;00:00, 87.9MB/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74341, ip=10.0.29.61)</span> <span class=" -Color -Color-Green"> [repeated 650x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00&lt;?, ?B/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  13%|█▎        | 1.31G/9.95G [00:05&lt;00:36, 239MB/s]<span class=" -Color -Color-Green"> [repeated 636x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:   1%|          | 105M/9.95G [00:00&lt;00:41, 239MB/s] <span class=" -Color -Color-Green"> [repeated 17x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74711, ip=10.0.45.211)</span> <span class=" -Color -Color-Green"> [repeated 640x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  26%|██▌       | 2.58G/9.95G [00:10&lt;00:28, 256MB/s]<span class=" -Color -Color-Green"> [repeated 635x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74502, ip=10.0.60.86)</span> <span class=" -Color -Color-Green"> [repeated 638x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  37%|███▋      | 3.70G/9.95G [00:15&lt;00:26, 238MB/s]<span class=" -Color -Color-Green"> [repeated 638x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74274, ip=10.0.36.152)</span> <span class=" -Color -Color-Green"> [repeated 643x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  51%|█████▏    | 5.12G/9.95G [00:20&lt;00:18, 255MB/s]<span class=" -Color -Color-Green"> [repeated 649x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75476, ip=10.0.51.205)</span> <span class=" -Color -Color-Green"> [repeated 638x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  65%|██████▌   | 6.48G/9.95G [00:25&lt;00:14, 246MB/s]<span class=" -Color -Color-Green"> [repeated 633x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74457, ip=10.0.47.2)</span> <span class=" -Color -Color-Green"> [repeated 645x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  76%|███████▌  | 7.52G/9.95G [00:29&lt;00:09, 247MB/s]<span class=" -Color -Color-Green"> [repeated 644x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  91%|█████████▏| 9.10G/9.95G [00:34&lt;00:03, 263MB/s]
Downloading (…)l-00001-of-00003.bin:  92%|█████████▏| 9.13G/9.95G [00:34&lt;00:03, 257MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74711, ip=10.0.45.211)</span> <span class=" -Color -Color-Green"> [repeated 634x across cluster]</span>
Downloading (…)l-00001-of-00003.bin:  82%|████████▏ | 8.17G/9.95G [00:35&lt;00:07, 228MB/s]<span class=" -Color -Color-Green"> [repeated 628x across cluster]</span>
Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [00:37&lt;00:00, 262MB/s]
Downloading shards:  33%|███▎      | 1/3 [00:38&lt;01:16, 38.09s/it]
Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00&lt;?, ?B/s]
Downloading (…)l-00002-of-00003.bin:   1%|▏         | 126M/9.90G [00:00&lt;00:35, 273MB/s] 
Downloading (…)l-00001-of-00003.bin:  93%|█████████▎| 9.27G/9.95G [00:39&lt;00:02, 228MB/s]<span class=" -Color -Color-Green"> [repeated 394x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75547, ip=10.0.42.158)</span> <span class=" -Color -Color-Green"> [repeated 633x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:   2%|▏         | 241M/9.90G [00:01&lt;00:38, 252MB/s]<span class=" -Color -Color-Green"> [repeated 213x across cluster]</span>
Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [00:40&lt;00:00, 243MB/s]<span class=" -Color -Color-Green"> [repeated 8x across cluster]</span>
Downloading shards:  33%|███▎      | 1/3 [00:42&lt;01:25, 42.77s/it]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00&lt;?, ?B/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:   1%|          | 115M/9.90G [00:00&lt;00:46, 209MB/s] <span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [00:42&lt;00:00, 233MB/s]<span class=" -Color -Color-Green"> [repeated 50x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74341, ip=10.0.29.61)</span> <span class=" -Color -Color-Green"> [repeated 636x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  19%|█▊        | 1.86G/9.90G [00:06&lt;00:29, 275MB/s]<span class=" -Color -Color-Green"> [repeated 589x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74996, ip=10.0.9.249)</span> <span class=" -Color -Color-Green"> [repeated 649x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  18%|█▊        | 1.75G/9.90G [00:07&lt;00:34, 234MB/s]<span class=" -Color -Color-Green"> [repeated 643x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74502, ip=10.0.60.86)</span> <span class=" -Color -Color-Green"> [repeated 645x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  41%|████▏     | 4.09G/9.90G [00:15&lt;00:21, 271MB/s]<span class=" -Color -Color-Green"> [repeated 644x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74273, ip=10.0.54.55)</span> <span class=" -Color -Color-Green"> [repeated 652x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  53%|█████▎    | 5.25G/9.90G [00:21&lt;00:19, 242MB/s]<span class=" -Color -Color-Green"> [repeated 656x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> <span class=" -Color -Color-Green"> [repeated 647x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  67%|██████▋   | 6.66G/9.90G [00:25&lt;00:13, 246MB/s]<span class=" -Color -Color-Green"> [repeated 646x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75132, ip=10.0.20.140)</span> <span class=" -Color -Color-Green"> [repeated 629x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  84%|████████▍ | 8.30G/9.90G [00:31&lt;00:06, 234MB/s]<span class=" -Color -Color-Green"> [repeated 627x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  91%|█████████▏| 9.06G/9.90G [00:34&lt;00:03, 241MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74457, ip=10.0.47.2)</span> <span class=" -Color -Color-Green"> [repeated 627x across cluster]</span>
Downloading (…)l-00002-of-00003.bin:  89%|████████▉ | 8.84G/9.90G [00:36&lt;00:04, 228MB/s]<span class=" -Color -Color-Green"> [repeated 567x across cluster]</span>
Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [00:38&lt;00:00, 257MB/s]
Downloading shards:  67%|██████▋   | 2/3 [01:16&lt;00:38, 38.38s/it]
Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00&lt;?, ?B/s]
Downloading (…)l-00003-of-00003.bin:   2%|▏         | 126M/6.18G [00:00&lt;00:22, 266MB/s] 
Downloading (…)l-00002-of-00003.bin:  98%|█████████▊| 9.69G/9.90G [00:38&lt;00:00, 236MB/s]<span class=" -Color -Color-Green"> [repeated 310x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75476, ip=10.0.51.205)</span> <span class=" -Color -Color-Green"> [repeated 629x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:   2%|▏         | 94.4M/6.18G [00:00&lt;00:24, 247MB/s]<span class=" -Color -Color-Green"> [repeated 275x across cluster]</span>
Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [00:39&lt;00:00, 253MB/s]<span class=" -Color -Color-Green"> [repeated 10x across cluster]</span>
Downloading shards:  67%|██████▋   | 2/3 [01:20&lt;00:40, 40.01s/it]<span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00&lt;?, ?B/s]<span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:   2%|▏         | 126M/6.18G [00:00&lt;00:24, 243MB/s] <span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
Downloading (…)l-00002-of-00003.bin: 100%|█████████▉| 9.88G/9.90G [00:41&lt;00:00, 242MB/s]<span class=" -Color -Color-Green"> [repeated 122x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74273, ip=10.0.54.55)</span> <span class=" -Color -Color-Green"> [repeated 638x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  21%|██        | 1.31G/6.18G [00:05&lt;00:20, 243MB/s]<span class=" -Color -Color-Green"> [repeated 569x across cluster]</span>
Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [00:40&lt;00:00, 242MB/s]<span class=" -Color -Color-Green"> [repeated 2x across cluster]</span>
Downloading shards:  67%|██████▋   | 2/3 [01:23&lt;00:41, 41.78s/it]<span class=" -Color -Color-Green"> [repeated 2x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00&lt;?, ?B/s]<span class=" -Color -Color-Green"> [repeated 2x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:   2%|▏         | 105M/6.18G [00:00&lt;00:24, 248MB/s] <span class=" -Color -Color-Green"> [repeated 2x across cluster]</span>
Downloading (…)l-00002-of-00003.bin: 100%|█████████▉| 9.87G/9.90G [00:40&lt;00:00, 260MB/s]<span class=" -Color -Color-Green"> [repeated 3x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74274, ip=10.0.36.152)</span> <span class=" -Color -Color-Green"> [repeated 638x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  41%|████▏     | 2.56G/6.18G [00:10&lt;00:14, 256MB/s]<span class=" -Color -Color-Green"> [repeated 635x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> <span class=" -Color -Color-Green"> [repeated 629x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  62%|██████▏   | 3.84G/6.18G [00:15&lt;00:08, 279MB/s]<span class=" -Color -Color-Green"> [repeated 627x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  92%|█████████▏| 5.66G/6.18G [00:22&lt;00:01, 268MB/s]
Downloading (…)l-00003-of-00003.bin:  92%|█████████▏| 5.69G/6.18G [00:22&lt;00:01, 265MB/s]
Downloading (…)l-00003-of-00003.bin:  93%|█████████▎| 5.73G/6.18G [00:22&lt;00:01, 268MB/s]
Downloading (…)l-00003-of-00003.bin:  93%|█████████▎| 5.76G/6.18G [00:22&lt;00:01, 270MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75547, ip=10.0.42.158)</span> <span class=" -Color -Color-Green"> [repeated 644x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  85%|████████▌ | 5.25G/6.18G [00:20&lt;00:03, 270MB/s]<span class=" -Color -Color-Green"> [repeated 618x across cluster]</span>
Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [00:24&lt;00:00, 257MB/s]
Downloading shards: 100%|██████████| 3/3 [01:40&lt;00:00, 33.61s/it]
Downloading (…)l-00003-of-00003.bin:  98%|█████████▊| 6.03G/6.18G [00:23&lt;00:00, 269MB/s]<span class=" -Color -Color-Green"> [repeated 166x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74274, ip=10.0.36.152)</span> <span class=" -Color -Color-Green"> [repeated 426x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  86%|████████▌ | 5.30G/6.18G [00:21&lt;00:03, 246MB/s]<span class=" -Color -Color-Green"> [repeated 222x across cluster]</span>
Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [00:25&lt;00:00, 239MB/s]<span class=" -Color -Color-Green"> [repeated 7x across cluster]</span>
Downloading shards: 100%|██████████| 3/3 [01:45&lt;00:00, 35.27s/it]<span class=" -Color -Color-Green"> [repeated 11x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  98%|█████████▊| 6.04G/6.18G [00:25&lt;00:00, 231MB/s]<span class=" -Color -Color-Green"> [repeated 98x across cluster]</span>
Loading checkpoint shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74274, ip=10.0.36.152)</span> <span class=" -Color -Color-Green"> [repeated 74x across cluster]</span>
Downloading (…)l-00003-of-00003.bin:  91%|█████████ | 5.63G/6.18G [00:23&lt;00:02, 242MB/s]<span class=" -Color -Color-Green"> [repeated 23x across cluster]</span>
Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [00:24&lt;00:00, 249MB/s]
Downloading shards: 100%|██████████| 3/3 [01:49&lt;00:00, 36.47s/it]<span class=" -Color -Color-Green"> [repeated 4x across cluster]</span>
Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [00:25&lt;00:00, 241MB/s]<span class=" -Color -Color-Green"> [repeated 5x across cluster]</span>
Loading checkpoint shards:  33%|███▎      | 1/3 [00:12&lt;00:24, 12.11s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00&lt;?, ?it/s]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Loading checkpoint shards:  33%|███▎      | 1/3 [00:18&lt;00:37, 18.54s/it]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:30&lt;00:15, 15.63s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:30&lt;00:15, 15.71s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:35&lt;00:17, 17.73s/it]<span class=" -Color -Color-Green"> [repeated 14x across cluster]</span>
Loading checkpoint shards: 100%|██████████| 3/3 [00:40&lt;00:00, 13.47s/it]
Downloading (…)neration_config.json: 100%|██████████| 132/132 [00:00&lt;00:00, 458kB/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:45&lt;00:00, 15.29s/it]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74996, ip=10.0.9.249)</span> LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Downloading (…)neration_config.json: 100%|██████████| 132/132 [00:00&lt;00:00, 542kB/s]<span class=" -Color -Color-Green"> [repeated 14x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> DeepSpeed Configs:  {&#39;zero_allow_untested_optimizer&#39;: True, &#39;bf16&#39;: {&#39;enabled&#39;: True}, &#39;zero_optimization&#39;: {&#39;stage&#39;: 3, &#39;offload_optimizer&#39;: {&#39;device&#39;: &#39;cpu&#39;, &#39;pin_memory&#39;: True}, &#39;overlap_comm&#39;: True, &#39;contiguous_gradients&#39;: True, &#39;reduce_bucket_size&#39;: 26214400, &#39;stage3_prefetch_bucket_size&#39;: 23592960.0, &#39;stage3_param_persistence_threshold&#39;: 51200}, &#39;gradient_accumulation_steps&#39;: 2, &#39;train_micro_batch_size_per_gpu&#39;: 1, &#39;gradient_clipping&#39;: 0.0}
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Model Archetecture:  LlamaForCausalLM(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   (model): LlamaModel(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>     (embed_tokens): Embedding(32000, 5120, padding_idx=0)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>     (layers): ModuleList(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>       (0-39): 40 x LlamaDecoderLayer(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         (self_attn): LlamaAttention(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (rotary_emb): LlamaRotaryEmbedding()
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         (mlp): LlamaMLP(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>           (act_fn): SiLUActivation()
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         (input_layernorm): LlamaRMSNorm()
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>         (post_attention_layernorm): LlamaRMSNorm()
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>       )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>     )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>     (norm): LlamaRMSNorm()
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> )
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74274, ip=10.0.36.152)</span> [2023-06-30 17:39:54,688] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74561, ip=10.0.35.16)</span> [2023-06-30 17:39:56,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> ninja: no work to do.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Time to load cpu_adam op: 2.403524875640869 seconds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Detected CUDA files, patching ldflags
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Building extension module cpu_adam...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Loading extension module cpu_adam...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74502, ip=10.0.60.86)</span> LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
Downloading (…)neration_config.json: 100%|██████████| 132/132 [00:00&lt;00:00, 1.72MB/s]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74996, ip=10.0.9.249)</span> Building extension module utils...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Loading extension module utils...
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Time to load utils op: 0.0775597095489502 seconds
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Parameter Offload: Total persistent parameters: 414720 in 81 params
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> No modifications detected for re-loaded extension module utils, skipping build step...
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...<span class=" -Color -Color-Green"> [repeated 32x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74561, ip=10.0.35.16)</span> Detected CUDA files, patching ldflags<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Emitting ninja build file /home/ray/.cache/torch_extensions/py310_cu118/utils/build.ninja...<span class=" -Color -Color-Green"> [repeated 31x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74561, ip=10.0.35.16)</span> Building extension module cpu_adam...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)<span class=" -Color -Color-Green"> [repeated 31x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75132, ip=10.0.20.140)</span> Loading extension module cpu_adam...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Building extension module utils...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Loading extension module utils...<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> ninja: no work to do.<span class=" -Color -Color-Green"> [repeated 31x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75132, ip=10.0.20.140)</span> Time to load cpu_adam op: 2.3851447105407715 seconds<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Time to load utils op: 0.0005815029144287109 seconds<span class=" -Color -Color-Green"> [repeated 16x across cluster]</span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   | Name  | Type             | Params | Params per Device
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> ---------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 0 | model | LlamaForCausalLM | 13.0 B | 813 M            
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> ---------------------------------------------------------------
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 13.0 B    Trainable params
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 0         Non-trainable params
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 13.0 B    Total params
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> 52,063.457Total estimated model params size (MB)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0:   0%|          | 0/57 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> /home/ray/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   rank_zero_warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0:   2%|▏         | 1/57 [00:38&lt;35:42, 38.26s/it, v_num=0, train_loss=11.50]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Time to load utils op: 0.00030732154846191406 seconds<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:44:33,395] [WARNING] [stage3.py:1851:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:   4%|▎         | 2/57 [01:19&lt;36:23, 39.69s/it, v_num=0, train_loss=10.70]
Epoch 0:   5%|▌         | 3/57 [01:52&lt;33:52, 37.65s/it, v_num=0, train_loss=1.710]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:45:48,054] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:   7%|▋         | 4/57 [02:34&lt;34:01, 38.51s/it, v_num=0, train_loss=1.610]
Epoch 0:   9%|▉         | 5/57 [03:08&lt;32:35, 37.60s/it, v_num=0, train_loss=0.914]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:47:03,011] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  11%|█         | 6/57 [03:49&lt;32:26, 38.17s/it, v_num=0, train_loss=0.973]
Epoch 0:  12%|█▏        | 7/57 [04:24&lt;31:30, 37.81s/it, v_num=0, train_loss=0.801]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:48:19,362] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  14%|█▍        | 8/57 [05:05&lt;31:10, 38.17s/it, v_num=0, train_loss=0.844]
Epoch 0:  16%|█▌        | 9/57 [05:39&lt;30:12, 37.75s/it, v_num=0, train_loss=0.652]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:49:36,571] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  18%|█▊        | 10/57 [06:22&lt;29:58, 38.26s/it, v_num=0, train_loss=0.633]
Epoch 0:  19%|█▉        | 11/57 [06:59&lt;29:13, 38.12s/it, v_num=0, train_loss=0.629]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/arrow/cpp/src/arrow/filesystem/s3fs.cc:663: CompletedMultipartUpload got error embedded in a 200 OK response: InternalError (&quot;We encountered an internal error. Please try again.&quot;), retry = 1
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:50:54,177] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  21%|██        | 12/57 [07:40&lt;28:45, 38.35s/it, v_num=0, train_loss=0.609]
Epoch 0:  23%|██▎       | 13/57 [08:14&lt;27:53, 38.04s/it, v_num=0, train_loss=0.680]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:52:10,002] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  25%|██▍       | 14/57 [08:55&lt;27:26, 38.29s/it, v_num=0, train_loss=0.648]
Epoch 0:  26%|██▋       | 15/57 [09:29&lt;26:33, 37.95s/it, v_num=0, train_loss=0.645]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:53:23,209] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  28%|██▊       | 16/57 [10:09&lt;26:01, 38.08s/it, v_num=0, train_loss=0.664]
Epoch 0:  30%|██▉       | 17/57 [10:43&lt;25:13, 37.83s/it, v_num=0, train_loss=0.625]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:54:36,660] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  32%|███▏      | 18/57 [11:22&lt;24:39, 37.93s/it, v_num=0, train_loss=0.617]
Epoch 0:  33%|███▎      | 19/57 [11:56&lt;23:53, 37.71s/it, v_num=0, train_loss=0.609]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:55:51,289] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  35%|███▌      | 20/57 [12:37&lt;23:20, 37.86s/it, v_num=0, train_loss=0.602]
Epoch 0:  37%|███▋      | 21/57 [13:11&lt;22:36, 37.69s/it, v_num=0, train_loss=0.590]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:57:07,919] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  39%|███▊      | 22/57 [13:53&lt;22:06, 37.91s/it, v_num=0, train_loss=0.555]
Epoch 0:  40%|████      | 23/57 [14:27&lt;21:22, 37.72s/it, v_num=0, train_loss=0.598]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:58:22,349] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  42%|████▏     | 24/57 [15:08&lt;20:48, 37.85s/it, v_num=0, train_loss=0.625]
Epoch 0:  44%|████▍     | 25/57 [15:43&lt;20:07, 37.74s/it, v_num=0, train_loss=0.625]
Epoch 0:  44%|████▍     | 25/57 [15:43&lt;20:07, 37.74s/it, v_num=0, train_loss=0.582]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 17:59:40,125] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  46%|████▌     | 26/57 [16:26&lt;19:35, 37.93s/it, v_num=0, train_loss=0.535]
Epoch 0:  47%|████▋     | 27/57 [17:02&lt;18:56, 37.88s/it, v_num=0, train_loss=0.578]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:00:58,164] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  49%|████▉     | 28/57 [17:44&lt;18:22, 38.01s/it, v_num=0, train_loss=0.582]
Epoch 0:  51%|█████     | 29/57 [18:20&lt;17:42, 37.93s/it, v_num=0, train_loss=0.578]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:02:15,097] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  53%|█████▎    | 30/57 [19:01&lt;17:06, 38.04s/it, v_num=0, train_loss=0.598]
Epoch 0:  54%|█████▍    | 31/57 [19:36&lt;16:26, 37.95s/it, v_num=0, train_loss=0.586]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:03:30,632] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  56%|█████▌    | 32/57 [20:16&lt;15:50, 38.02s/it, v_num=0, train_loss=0.605]
Epoch 0:  58%|█████▊    | 33/57 [20:49&lt;15:08, 37.87s/it, v_num=0, train_loss=0.594]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:04:45,362] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  60%|█████▉    | 34/57 [21:31&lt;14:33, 37.98s/it, v_num=0, train_loss=0.598]
Epoch 0:  61%|██████▏   | 35/57 [22:08&lt;13:54, 37.95s/it, v_num=0, train_loss=0.574]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:06:02,727] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  63%|██████▎   | 36/57 [22:48&lt;13:18, 38.02s/it, v_num=0, train_loss=0.586]
Epoch 0:  65%|██████▍   | 37/57 [23:23&lt;12:38, 37.94s/it, v_num=0, train_loss=0.562]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:07:19,126] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  67%|██████▋   | 38/57 [24:05&lt;12:02, 38.03s/it, v_num=0, train_loss=0.535]
Epoch 0:  68%|██████▊   | 39/57 [24:38&lt;11:22, 37.91s/it, v_num=0, train_loss=0.598]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:08:36,683] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  70%|███████   | 40/57 [25:22&lt;10:47, 38.07s/it, v_num=0, train_loss=0.562]
Epoch 0:  72%|███████▏  | 41/57 [25:57&lt;10:07, 37.98s/it, v_num=0, train_loss=0.555]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:09:52,426] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  74%|███████▎  | 42/57 [26:38&lt;09:30, 38.06s/it, v_num=0, train_loss=0.555]
Epoch 0:  75%|███████▌  | 43/57 [27:13&lt;08:51, 37.99s/it, v_num=0, train_loss=0.547]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:11:08,855] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  77%|███████▋  | 44/57 [27:54&lt;08:14, 38.06s/it, v_num=0, train_loss=0.562]
Epoch 0:  79%|███████▉  | 45/57 [28:29&lt;07:35, 37.98s/it, v_num=0, train_loss=0.535]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:12:25,181] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  81%|████████  | 46/57 [29:11&lt;06:58, 38.07s/it, v_num=0, train_loss=0.531]
Epoch 0:  82%|████████▏ | 47/57 [29:45&lt;06:19, 37.99s/it, v_num=0, train_loss=0.504]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:13:40,300] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  84%|████████▍ | 48/57 [30:26&lt;05:42, 38.05s/it, v_num=0, train_loss=0.520]
Epoch 0:  86%|████████▌ | 49/57 [31:01&lt;05:03, 37.99s/it, v_num=0, train_loss=0.523]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:14:55,542] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  88%|████████▊ | 50/57 [31:41&lt;04:26, 38.03s/it, v_num=0, train_loss=0.520]
Epoch 0:  89%|████████▉ | 51/57 [32:16&lt;03:47, 37.98s/it, v_num=0, train_loss=0.527]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:16:12,131] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  91%|█████████ | 52/57 [32:58&lt;03:10, 38.04s/it, v_num=0, train_loss=0.562]
Epoch 0:  93%|█████████▎| 53/57 [33:34&lt;02:32, 38.00s/it, v_num=0, train_loss=0.539]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:17:29,752] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  95%|█████████▍| 54/57 [34:15&lt;01:54, 38.07s/it, v_num=0, train_loss=0.535]
Epoch 0:  96%|█████████▋| 55/57 [34:50&lt;01:16, 38.01s/it, v_num=0, train_loss=0.512]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:18:45,986] [WARNING] [stage3.py:1851:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0:  98%|█████████▊| 56/57 [35:31&lt;00:38, 38.07s/it, v_num=0, train_loss=0.516]
Epoch 0: 100%|██████████| 57/57 [36:06&lt;00:00, 38.00s/it, v_num=0, train_loss=0.461]
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> [2023-06-30 18:20:01,817] [WARNING] [stage3.py:1851:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Epoch 0: : 58it [36:47, 38.07s/it, v_num=0, train_loss=0.523]                      
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74427, ip=10.0.16.236)</span> /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74427, ip=10.0.16.236)</span>   warnings.warn(
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> No modifications detected for re-loaded extension module utils, skipping build step...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Using /home/ray/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Loading extension module utils...<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Uploading checkpoint files from worker rank 0 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/checkpoint_000000.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> /home/ray/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span>   warnings.warn(<span class=" -Color -Color-Green"> [repeated 15x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75547, ip=10.0.42.158)</span> Uploading checkpoint files from worker rank 3 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/checkpoint_000000.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Uploading checkpoint files from worker rank 1 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/checkpoint_000000.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> Done uploading checkpoint files.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74341, ip=10.0.29.61)</span> Uploading checkpoint files from worker rank 10 to cloud URI s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/checkpoint_000000.<span class=" -Color -Color-Green"> [repeated 13x across cluster]</span>
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74427, ip=10.0.16.236)</span> Done uploading checkpoint files.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74152, ip=10.0.63.141)</span> Done uploading checkpoint files.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=74711, ip=10.0.45.211)</span> Done uploading checkpoint files.<span class=" -Color -Color-Green"> [repeated 11x across cluster]</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0: : 58it [37:42, 39.00s/it, v_num=0, train_loss=0.523]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=134267)</span> `Trainer.fit` stopped: `max_epochs=1` reached.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(LightningTrainer pid=134103)</span> Uploading trial artifacts took 26.651 s, which may be a performance bottleneck. Consider saving fewer/smaller artifacts to the trial log directory, or disable artifact syncing with `SyncConfig(sync_artifacts=False)`.
<span class=" -Color -Color-Faint -Color-Faint-Cyan">(RayTrainWorker pid=75547, ip=10.0.42.158)</span> Done uploading checkpoint files.<span class=" -Color -Color-Green"> [repeated 2x across cluster]</span>
2023-06-30 18:21:59,316	INFO tune.py:1148 -- Total run time: 2542.82 seconds (2511.95 seconds for the tuning loop).
</pre></div>
</div>
</div>
</div>
<p>In summary:</p>
<ul class="simple">
<li><p>Training takes: 36:06 = 2166s</p></li>
<li><p>Training + initialization + checkpointing takes 2473s</p></li>
</ul>
<p>Model initialization and checkpoint synchronization took 307 seconds. It will be amortized as you have larger datasets and take more time to train.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Result(
  metrics={&#39;_report_on&#39;: &#39;train_epoch_end&#39;, &#39;train_loss&#39;: 0.5234375, &#39;epoch&#39;: 0, &#39;step&#39;: 29, &#39;should_checkpoint&#39;: True, &#39;done&#39;: True, &#39;trial_id&#39;: &#39;c1544_00000&#39;, &#39;experiment_tag&#39;: &#39;0&#39;},
  path=&#39;s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36&#39;,
  checkpoint=LightningCheckpoint(uri=s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/yunxuanx-test/vicuna-13b-test/vicuna-13b-relation-extraction/LightningTrainer_c1544_00000_0_2023-06-30_17-39-36/checkpoint_000000)
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="llm-inference">
<h2>LLM Inference<a class="headerlink" href="#llm-inference" title="Permalink to this headline">#</a></h2>
<p>Now, it’s time to play with our fine-tuned Vicuna code generator!</p>
<section id="download-and-process-your-checkpoints">
<h3>Download and Process your checkpoints<a class="headerlink" href="#download-and-process-your-checkpoints" title="Permalink to this headline">#</a></h3>
<p>First, download the checkpoints to your local machine using the AWS CLI.</p>
<p>Note that adding the following configurations can significantly increase the syncing throughput compared to the default configurations. On a g5 instance with NVME SSD, the download speed improved from <code class="docutils literal notranslate"><span class="pre">200MB/s</span></code> to around <code class="docutils literal notranslate"><span class="pre">1.5GB/s</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>awsv2 configure <span class="nb">set</span> s3.max_concurrent_requests <span class="m">32</span>
<span class="o">!</span>awsv2 configure <span class="nb">set</span> default.s3.preferred_transfer_client crt
<span class="o">!</span>awsv2 configure <span class="nb">set</span> default.s3.target_bandwidth 100Gb/s
<span class="o">!</span>awsv2 configure <span class="nb">set</span> default.s3.multipart_chunksize 8MB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;awsv2 s3 sync s3://</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2"> /mnt/local_storage&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The deepspeed ZeRO-3 checkpoint is a directory containing of k shards (k=16 in our case).</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">zero_pp_rank_k_mp_rank_00_model_states.pt</span></code>: contains the model parameter skeleton of shard k.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bf16_zero_pp_rank_k_mp_rank_00_optim_states.pt</span></code>: contains the actual flattened model parameters and optimizer states of shard k.</p></li>
</ul>
<p>Next, we removed the optimizer states and consolidate the checkpoint into a single binary file using DeepSpeed utilities. Also, since we wrapped vicuna-13b within a <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>, we need to remove the prefix <code class="docutils literal notranslate"><span class="pre">_forward_module.model.model</span></code> so that we can directly load the checkpoint into a HF vicuna model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">get_fp32_state_dict_from_zero_checkpoint</span>

<span class="k">def</span> <span class="nf">extract_fp32_ckpt_from_zero</span><span class="p">(</span><span class="n">zero_ckpt_dir</span><span class="p">):</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">get_fp32_state_dict_from_zero_checkpoint</span><span class="p">(</span><span class="n">zero_ckpt_dir</span><span class="p">)</span>
    <span class="n">vicuna_state_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;_forward_module.model.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">vicuna_state_dict</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">zero_ckpt_dir</span><span class="p">,</span> <span class="s2">&quot;full_model.pt&quot;</span><span class="p">))</span>


<span class="n">full_model_ckpt_path</span> <span class="o">=</span> <span class="s2">&quot;/mnt/local_storage/checkpoint.ckpt/full_model.pt&quot;</span>
<span class="n">extract_fp32_ckpt_from_zero</span><span class="p">(</span><span class="s2">&quot;/mnt/local_storage/checkpoint.ckpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processing zero checkpoint &#39;/mnt/local_storage/checkpoint/model/checkpoint&#39;
Detected checkpoint of type zero stage 3, world_size: 16
Parsing checkpoint created by deepspeed==0.9.4
Reconstructed Trainable fp32 state dict with 363 params 13015864320 elements
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-generation-pipeline">
<h3>Initialize Generation Pipeline<a class="headerlink" href="#initialize-generation-pipeline" title="Permalink to this headline">#</a></h3>
<p>Here, we leverage the Accelerate library to efficiently load the model onto a suitable device(GPU and CPU) and generate a HF text generation pipeline.</p>
<ul class="simple">
<li><p>Initialize an empty model on metadevice</p></li>
<li><p>Create valid device mappings for the vicuna-13b model</p></li>
<li><p>Load and distribute model weights to target devices</p></li>
</ul>
<p>This ensures that only 1x model size of RAM is used for model initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">init_empty_weights</span><span class="p">,</span>
    <span class="n">infer_auto_device_map</span><span class="p">,</span>
    <span class="n">load_checkpoint_and_dispatch</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Initialize a model on meta device</span>
<span class="k">with</span> <span class="n">init_empty_weights</span><span class="p">():</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
    <span class="n">meta_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">meta_model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

<span class="c1"># Define the device mapping</span>
<span class="n">device_map</span> <span class="o">=</span> <span class="n">infer_auto_device_map</span><span class="p">(</span>
    <span class="n">meta_model</span><span class="p">,</span>
    <span class="n">max_memory</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;15GB&quot;</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="s2">&quot;60GB&quot;</span><span class="p">},</span>
    <span class="n">no_split_module_classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;LlamaDecoderLayer&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Load the model parameters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_checkpoint_and_dispatch</span><span class="p">(</span>
    <span class="n">meta_model</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="o">=</span><span class="n">full_model_ckpt_path</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="case-study">
<h3>Case Study<a class="headerlink" href="#case-study" title="Permalink to this headline">#</a></h3>
<p>We took 3 examples from the CoNaLa’s test split for demo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">testcases</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;replace white spaces in colunm &#39;col&#39; of dataframe `df` with &#39;_&#39;&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;search for occurrences of regex pattern &#39;&gt;.*&lt;&#39; in xml string `line`&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;send a signal `signal.SIGUSR1` to the current process&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s begin by examining the generated outputs without fine-tuning. In this case study, we utilize <a class="reference external" href="https://aviary.anyscale.com">Aviary Explorer</a>, an open-source multi-LLM serving platform supported by Ray and Anyscale. You can easily select from a variety of open-source LLMs and compare their generation quality, cost, latency, and many other metrics.</p>
<p>We constructed a prompt in a zero-shot learning manner and feed it into 3 OSS LLMs.</p>
<p><img alt="" src="https://user-images.githubusercontent.com/26745457/250704232-65a20f1b-6752-4d6c-bba1-8296a373162f.png" /></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vicuna-13b-v1.3</span></code> begins to speak Chinese.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mpt-7b-chat</span></code> generates a reasonable code snippet, but with multiple lines.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">falcon-7b-sft</span></code> generates a one line snippet, but it doesn’t seem to work.</p></li>
</ul>
<p>As we can see, none of them generate a satisfactory code snippet.</p>
<p>Now let’s check the performance of our fine-tuned <code class="docutils literal notranslate"><span class="pre">vicuna-13b-v1.3</span></code> model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">case</span> <span class="ow">in</span> <span class="n">testcases</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PROMPT_TEMPLATE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">intent</span><span class="o">=</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;intent&quot;</span><span class="p">],</span> <span class="n">snippet</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ray/anaconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Intent: replace white spaces in colunm &#39;col&#39; of dataframe `df` with &#39;_&#39;
One-line code snippet:  `df[&#39;col&#39;] = df[&#39;col&#39;].str.replace(&#39; &#39;, &#39;_&#39;)`

Intent: search for occurrences of regex pattern &#39;&gt;.*&lt;&#39; in xml string `line`
One-line code snippet:  `re.findall(&#39;&gt;.*&lt;&#39;, line)``

Intent: send a signal `signal.SIGUSR1` to the current process
One-line code snippet:  `os.kill(os.getpid(), signal.SIGUSR1)``
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-the-generated-code-snippets">
<h3>Test the Generated Code Snippets<a class="headerlink" href="#test-the-generated-code-snippets" title="Permalink to this headline">#</a></h3>
<p>The generated code snippets look pretty reasonable. The results covered Pandas operations, regular expressions, and Linux commands. Let’s test them one by one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;col&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;abc def ghi&quot;</span><span class="p">,</span> <span class="s2">&quot; 12 3 456&quot;</span><span class="p">,</span> <span class="s2">&quot;     &quot;</span><span class="p">]})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&quot;col&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;col&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before
            col
0  abc def ghi
1     12 3 456
2             
After
            col
0  abc_def_ghi
1    _12_3_456
2        _____
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>

<span class="n">line</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">&lt;bookstore&gt;</span>
<span class="s2">  &lt;book category=&quot;fiction&quot;&gt;</span>
<span class="s2">    &lt;title&gt;The Great Gatsby&lt;/title&gt;</span>
<span class="s2">    &lt;author&gt;F. Scott Fitzgerald&lt;/author&gt;</span>
<span class="s2">    &lt;year&gt;1925&lt;/year&gt;</span>
<span class="s2">  &lt;/book&gt;</span>
<span class="s2">  &lt;book category=&quot;non-fiction&quot;&gt;</span>
<span class="s2">    &lt;title&gt;Sapiens: A Brief History of Humankind&lt;/title&gt;</span>
<span class="s2">    &lt;author&gt;Yuval Noah Harari&lt;/author&gt;</span>
<span class="s2">    &lt;year&gt;2011&lt;/year&gt;</span>
<span class="s2">  &lt;/book&gt;</span>
<span class="s2">&lt;/bookstore&gt;</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&quot;&gt;.*&lt;&quot;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;&gt;The Great Gatsby&lt;&#39;,
 &#39;&gt;F. Scott Fitzgerald&lt;&#39;,
 &#39;&gt;1925&lt;&#39;,
 &#39;&gt;Sapiens: A Brief History of Humankind&lt;&#39;,
 &#39;&gt;Yuval Noah Harari&lt;&#39;,
 &#39;&gt;2011&lt;&#39;]
</pre></div>
</div>
</div>
</div>
<p>Finally, let’s hand it over to LLM and let it wrap up the demo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">signal</span>

<span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIGUSR1</span><span class="p">)</span>  <span class="c1"># Terminate the current process~</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://conala-corpus.github.io/">CoNaLa: The Code/Natural Language Challenge</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/deepspeed#deepspeed-integration">HuggingFace: DeepSpeed Integration</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/accelerate/main/usage_guides/big_modeling">HuggingFace: Handling big models for inference</a></p></li>
<li><p><a class="reference external" href="https://lightning-transformers.readthedocs.io/en/latest/">Lightning Transformers: DeepSpeed Training with Big Transformer Models</a></p></li>
<li><p><a class="reference external" href="https://www.anyscale.com/blog/announcing-aviary-open-source-multi-llm-serving-solution">Aviary: Open Source Multi-LLM Serving</a></p></li>
<li><p>Rajbhandari, S., Rasley, J., et al. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. <a class="reference external" href="https://arxiv.org/abs/1910.02054">arXiv:1910.02054</a></p></li>
<li><p>Zheng, L., Chiang, W-L., Sheng, Y., et al. (2023). Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. <a class="reference external" href="https://arxiv.org/abs/2306.05685">arXiv:2306.05685</a></p></li>
</ul>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>