
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>数据加载和预处理 &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../_static/js/csat.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="配置 Scale 和 GPU" href="using-gpus.html" />
    <link rel="prev" title="Ray Train 用户指南" href="../user-guides.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   入门「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data/data.html">
   Ray 数据「85%」
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../train.html">
   Ray 训练「95%」
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../overview.html">
     概述
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch.html">
     PyTorch 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch-lightning.html">
     PyTorch Lightning 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-transformers.html">
     Hugging Face Transformers 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../more-frameworks.html">
     更多框架
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="../user-guides.html">
     用户指南
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       数据加载和预处理
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="using-gpus.html">
       配置 Scale 和 GPU
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="persistent-storage.html">
       配置持久存储
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="monitoring-logging.html">
       监控和记录指标
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="checkpoints.html">
       保存加载检查点
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="experiment-tracking.html">
       实现追踪
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="results.html">
       检查训练结果
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="fault-tolerance.html">
       处理故障和节点抢占
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="reproducibility.html">
       可重复性
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hyperparameter-optimization.html">
       Hyperparameter Optimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../examples.html">
     示例
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../benchmarks.html">
     基准
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/api.html">
     Ray 训练 API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/user-guides/data-loading-preprocessing.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/user-guides/data-loading-preprocessing.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/train/user-guides/data-loading-preprocessing.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   快速开始
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-load">
     加载数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-preprocess">
     预处理数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-input">
     输入及分割数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-consume">
     消费数据
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytorch">
   从 PyTorch 数据开始
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-datasets-split">
   分割数据集
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     完全自定义（高级）
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   随机洗牌
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   启用可重现性
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-structured-data">
   预处理结构化数据
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   性能技巧
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     预取批次
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-cache-performance">
     缓存预处理数据集
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cpu-only">
     向集群添加 CPU-only 节点
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>数据加载和预处理</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   快速开始
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-load">
     加载数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-preprocess">
     预处理数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-input">
     输入及分割数据
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-datasets-consume">
     消费数据
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytorch">
   从 PyTorch 数据开始
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-datasets-split">
   分割数据集
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     完全自定义（高级）
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   随机洗牌
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   启用可重现性
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-structured-data">
   预处理结构化数据
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   性能技巧
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     预取批次
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-cache-performance">
     缓存预处理数据集
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cpu-only">
     向集群添加 CPU-only 节点
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="data-ingest-torch">
<span id="id1"></span><h1>数据加载和预处理<a class="headerlink" href="#data-ingest-torch" title="Permalink to this headline">#</a></h1>
<p>Ray Train 与 <a class="reference internal" href="../../data/data.html#data"><span class="std std-ref">Ray Data</span></a> 集成，为加载和预处理大型数据集提供高效的流式解决方案。
们推荐使用 Ray Data，因为它能够支持大规模分布式训练工作负载，具有高效的性能 - 有关优势和与其他替代方案的比较，请参见 <a class="reference internal" href="../../data/overview.html#data-overview"><span class="std std-ref">Ray Data 概述</span></a>。</p>
<p>本指引中，我们将介绍如何将 Ray Data 集成到 Ray Train 脚本中，以及不同的方式来定制数据摄入管道。</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/train_ingest.png"><img alt="../../_images/train_ingest.png" src="../../_images/train_ingest.png" style="width: 300px;" /></a>
</figure>
<section id="id2">
<h2>快速开始<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>安装 Ray Data 和 Ray Train：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install -U <span class="s2">&quot;ray[data,train]&quot;</span>
</pre></div>
</div>
<p>Data ingestion can be set up with four basic steps:
数据提取可以通过以下四个基本步骤设置：</p>
<ol class="arabic simple">
<li><p>创建一个 Ray 数据集。</p></li>
<li><p>预处理 Ray 数据集。</p></li>
<li><p>将预处理后的数据集输入到 Ray Train Trainer 中。</p></li>
<li><p>在训练函数中使用 Ray 数据集。</p></li>
</ol>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-UHlUb3JjaA==" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-UHlUb3JjaA==" name="UHlUb3JjaA==" role="tab" tabindex="0">PyTorch</button><button aria-controls="panel-0-UHlUb3JjaCBMaWdodG5pbmc=" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-UHlUb3JjaCBMaWdodG5pbmc=" name="UHlUb3JjaCBMaWdodG5pbmc=" role="tab" tabindex="-1">PyTorch Lightning</button><button aria-controls="panel-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" name="SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" role="tab" tabindex="-1">HuggingFace Transformers</button></div><div aria-labelledby="tab-0-UHlUb3JjaA==" class="sphinx-tabs-panel group-tab" id="panel-0-UHlUb3JjaA==" name="UHlUb3JjaA==" role="tabpanel" tabindex="0"><div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="c1"># Set this to True to use GPU.</span>
<span class="c1"># If False, do CPU training instead of GPU training.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Step 1: Create a Ray Dataset from in-memory Python lists.</span>
<span class="c1"># You can also create a Ray Dataset from many other sources and file</span>
<span class="c1"># formats.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">([{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">]}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)])</span>

<span class="c1"># Step 2: Preprocess your Ray Dataset.</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">increment</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="c1"># Step 4: Access the dataset shard for the training worker via</span>
    <span class="c1"># ``get_dataset_shard``.</span>
    <span class="n">train_data_shard</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">train_data_shard</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="k">assert</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span>
            <span class="k">assert</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span>
            <span class="k">break</span> <span class="c1"># Only check one batch. Last batch can be partial.</span>

<span class="c1"># Step 3: Create a TorchTrainer. Specify the number of training workers and</span>
<span class="c1"># pass in your Ray Dataset.</span>
<span class="c1"># The Ray Dataset is automatically split across all training workers.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-UHlUb3JjaCBMaWdodG5pbmc=" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-UHlUb3JjaCBMaWdodG5pbmc=" name="UHlUb3JjaCBMaWdodG5pbmc=" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./train.csv&quot;</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./validation.csv&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_func_per_worker</span><span class="p">():</span>
    <span class="c1"># Access Ray datsets in your train_func via ``get_dataset_shard``.</span>
    <span class="c1"># The &quot;train&quot; dataset gets sharded across workers by default</span>
<span class="hll">    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</span><span class="hll">    <span class="n">val_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
</span>
    <span class="c1"># Create Ray dataset iterables via ``iter_torch_batches``.</span>
<span class="hll">    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span><span class="hll">    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span>
    <span class="o">...</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="c1"># ...</span>
    <span class="p">)</span>

    <span class="c1"># Feed the Ray dataset iterables to ``pl.Trainer.fit``.</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
<span class="hll">        <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
</span><span class="hll">        <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">val_dataloader</span>
</span>    <span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span> <span class="n">val_data</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" name="SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">ray.train</span>

<span class="o">...</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">hf_train_ds</span><span class="p">)</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">hf_eval_ds</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Access Ray datsets in your train_func via ``get_dataset_shard``.</span>
    <span class="c1"># The &quot;train&quot; dataset gets sharded across workers by default</span>
<span class="hll">    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</span><span class="hll">    <span class="n">eval_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;evaluation&quot;</span><span class="p">)</span>
</span>
    <span class="c1"># Create Ray dataset iterables via ``iter_torch_batches``.</span>
<span class="hll">    <span class="n">train_iterable_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span><span class="hll">    <span class="n">eval_iterable_ds</span> <span class="o">=</span> <span class="n">eval_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span>
    <span class="o">...</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span> <span class="c1"># Required for iterable datasets</span>
<span class="hll">    <span class="p">)</span>
</span><span class="hll">
</span>    <span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_iterable_ds</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_iterable_ds</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Prepare your Transformers Trainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">huggingface</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">prepare_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">val_data</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div></div>
<section id="train-datasets-load">
<span id="id3"></span><h3>加载数据<a class="headerlink" href="#train-datasets-load" title="Permalink to this headline">#</a></h3>
<p>Ray 数据集可以从许多不同的数据源和格式中创建。有关更多详细信息，请参见 <a class="reference internal" href="../../data/loading-data.html#loading-data"><span class="std std-ref">Loading Data</span></a>。</p>
</section>
<section id="train-datasets-preprocess">
<span id="id4"></span><h3>预处理数据<a class="headerlink" href="#train-datasets-preprocess" title="Permalink to this headline">#</a></h3>
<p>Ray 数据支持广泛的预处理操作，可用于在训练之前转换数据。</p>
<ul class="simple">
<li><p>对于常规预处理，请参阅 <a class="reference internal" href="../../data/transforming-data.html#transforming-data"><span class="std std-ref">转换数据</span></a>.</p></li>
<li><p>对于表格数据，请参见 <a class="reference internal" href="#preprocessing-structured-data"><span class="std std-ref">结构化数据预处理</span></a>。</p></li>
<li><p>对于 PyTorch 张量，请参见 <a class="reference internal" href="../../data/working-with-pytorch.html#transform-pytorch"><span class="std std-ref">使用 torch 张量进行转换</span></a>。</p></li>
<li><p>对于优化昂贵的预处理操作，请参见 <a class="reference internal" href="#dataset-cache-performance"><span class="std std-ref">缓存预处理数据集</span></a>。</p></li>
</ul>
</section>
<section id="train-datasets-input">
<span id="id5"></span><h3>输入及分割数据<a class="headerlink" href="#train-datasets-input" title="Permalink to this headline">#</a></h3>
<p>你的预处理数据集可以通过 <code class="docutils literal notranslate"><span class="pre">datasets</span></code> 参数传递给 Ray Train Trainer（例如：<a class="reference internal" href="../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a>）。</p>
<p>传入 Trainer <code class="docutils literal notranslate"><span class="pre">datasets</span></code> 的数据集可以通过在每个分布式训练工作节点上运行的 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中调用 <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a> 来访问。</p>
<p>所有数据集默认都会被分割（即分片）到训练工作节点上。 <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_dataset_shard()</span></code></a> 将返回数据集的 <code class="docutils literal notranslate"><span class="pre">1/n</span></code> 部分，其中 <code class="docutils literal notranslate"><span class="pre">n</span></code> 是训练工作节点的数量。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>请注意，由于评估数据集被分割，用户必须在工作节点上聚合评估结果。
你可以考虑使用 <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a> (<a class="reference internal" href="../examples/deepspeed/deepspeed_example.html#deepspeed-example"><span class="std std-ref">example</span></a>) 或
其他框架中提供的实用工具。</p>
</div>
<p>可通过传递 <code class="docutils literal notranslate"><span class="pre">dataset_config</span></code> 参数来覆盖此行为。有关配置拆分逻辑的更多信息，请参见 <a class="reference internal" href="#train-datasets-split"><span class="std std-ref">Splitting datasets</span></a>。</p>
</section>
<section id="train-datasets-consume">
<span id="id6"></span><h3>消费数据<a class="headerlink" href="#train-datasets-consume" title="Permalink to this headline">#</a></h3>
<p>在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中，每个工作节点都可以通过 <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a> 访问其数据集的分片。</p>
<p>这些数据可以通过多种方式使用：</p>
<ul class="simple">
<li><p>要创建批量的 Iterable，可以调用 <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches()</span></code></a>。</p></li>
<li><p>要创建一个 PyTorch DataLoader 的替代品，可以调用 <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches()</span></code></a>。</p></li>
</ul>
<p>更多如何迭代数据的详细信息，请参见 <a class="reference internal" href="../../data/iterating-over-data.html#iterating-over-data"><span class="std std-ref">Iterating over data</span></a>。</p>
</section>
</section>
<section id="pytorch">
<span id="train-datasets-pytorch"></span><h2>从 PyTorch 数据开始<a class="headerlink" href="#pytorch" title="Permalink to this headline">#</a></h2>
<p>一些框架提供了自己的数据集和数据加载工具。例如：</p>
<ul class="simple">
<li><p><strong>PyTorch:</strong> <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Dataset &amp; DataLoader</a></p></li>
<li><p><strong>Hugging Face:</strong> <a class="reference external" href="https://huggingface.co/docs/datasets/index">Dataset</a></p></li>
<li><p><strong>PyTorch Lightning:</strong> <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/data/datamodule.html">LightningDataModule</a></p></li>
</ul>
<p>这些实用程序仍可直接与 Ray Train 一起使用。特别是，如果您已经设置了数据提取管道，则可能需要这样做。
但是，为了实现更高性能的大规模数据提取，我们建议迁移到 Ray Data。</p>
<p>从高层次来看，您可以按如下方式比较这些概念：</p>
<table class="table">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch API</p></th>
<th class="head"><p>HuggingFace API</p></th>
<th class="head"><p>Ray Data API</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">torch.utils.data.Dataset</a></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset">datasets.Dataset</a></p></td>
<td><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.html#ray.data.Dataset" title="ray.data.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">ray.data.Dataset</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a></p></td>
<td><p>n/a</p></td>
<td><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.iter_torch_batches.html#ray.data.Dataset.iter_torch_batches" title="ray.data.Dataset.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.Dataset.iter_torch_batches()</span></code></a></p></td>
</tr>
</tbody>
</table>
<p>有关更多详细信息，请参阅以下每个框架的部分。</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">PyTorch Dataset 和 DataLoader</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">LightningDataModule</button><button aria-controls="panel-1-1-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-2" name="1-2" role="tab" tabindex="-1">Hugging Face Dataset</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><p><strong>选项 1 (使用 Ray Data):</strong> 将 PyTorch Dataset 转换为 Ray Dataset 并通过 <code class="docutils literal notranslate"><span class="pre">datasets</span></code> 参数传递给 Trainer。在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中，
您可以通过 <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a> 访问数据集。
您可以通过 <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.DataIterator.iter_torch_batches()</span></code></a> 将其转换为替换 PyTorch DataLoader。</p>
<p>更多详细信息，请参见 <a class="reference internal" href="../../data/working-with-pytorch.html#migrate-pytorch"><span class="std std-ref">从 PyTorch 数据集和 DataLoader 迁移</span></a>。</p>
<p><strong>选项 2 (不用 Ray Data):</strong> 直接在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中实例化 Torch Dataset 和 DataLoader。
你可以使用 <a class="reference internal" href="../api/doc/ray.train.torch.prepare_data_loader.html#ray.train.torch.prepare_data_loader" title="ray.train.torch.prepare_data_loader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.torch.prepare_data_loader()</span></code></a> 实用工具来设置 DataLoader 以进行分布式训练。</p>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><p><code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> 是使用 PyTorch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 和 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 创建的。您可以在这里应用相同的逻辑。</p>
</div><div aria-labelledby="tab-1-1-2" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-2" name="1-2" role="tabpanel" tabindex="0"><p><strong>选项 1 (使用 Ray Data):</strong> 转换你的 Hugging Face Dataset 为 Ray Dataset 并通过 <code class="docutils literal notranslate"><span class="pre">datasets</span></code> 参数传递给 Trainer。
在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中，你可以通过 <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a> 访问数据集。</p>
<p>For instructions, see <a class="reference internal" href="../../data/loading-data.html#loading-datasets-from-ml-libraries"><span class="std std-ref">Ray Data for Hugging Face</span></a>.</p>
<p><strong>选项 2 (不用 Ray Data):</strong> 直接在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中实例化 Hugging Face Dataset。</p>
</div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>当直接使用 Torch 或 Hugging Face 数据集而不使用 Ray Data 时，请确保在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 中实例化您的数据集。
在 <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> 之外实例化数据集并通过全局范围传递可能会导致错误，因为数据集不可序列化。</p>
</div>
</div>
</section>
<section id="train-datasets-split">
<span id="id7"></span><h2>分割数据集<a class="headerlink" href="#train-datasets-split" title="Permalink to this headline">#</a></h2>
<p>默认，Ray Train 使用 <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.streaming_split.html#ray.data.Dataset.streaming_split" title="ray.data.Dataset.streaming_split"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.streaming_split</span></code></a> 将所有数据集分割到工作节点上。每个 worker 看到数据的一个不相交子集，而不是迭代整个数据集。除非随机洗牌，否则每次迭代数据集时都会使用相同的拆分。</p>
<p>如果要自定义哪些数据集被拆分，请在 Trainer 构造函数中传递 <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a>。</p>
<p>例如，要仅拆分训练数据集，请执行以下操作：</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>
<span class="n">train_ds</span><span class="p">,</span> <span class="n">val_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get the sharded training dataset</span>
    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Get the unsharded full validation dataset</span>
    <span class="n">val_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some evaluation on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_ds</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">:</span> <span class="n">val_ds</span><span class="p">},</span>
    <span class="n">dataset_config</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">DataConfig</span><span class="p">(</span>
        <span class="n">datasets_to_split</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<section id="id8">
<h3>完全自定义（高级）<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h3>
<p>对于默认配置类未覆盖的用例，您还可以完全自定义输入数据集的拆分方式。定义一个自定义 <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> 类（DeveloperAPI）。<a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> 类负责在节点之间共享设置和拆分数据。</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note that this example class is doing the same thing as the basic DataConfig</span>
<span class="c1"># implementation included with Ray Train.</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">DataConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataIterator</span><span class="p">,</span> <span class="n">NodeIdStr</span>
<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyCustomDataConfig</span><span class="p">(</span><span class="n">DataConfig</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">configure</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">],</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">worker_handles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ActorHandle</span><span class="p">]],</span>
        <span class="n">worker_node_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">NodeIdStr</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataIterator</span><span class="p">]]:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;This example only handles the simple case&quot;</span>

        <span class="c1"># Configure Ray Data for ingest.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">DataConfig</span><span class="o">.</span><span class="n">default_ingest_options</span><span class="p">()</span>

        <span class="c1"># Split the stream into shards.</span>
        <span class="n">iterator_shards</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">streaming_split</span><span class="p">(</span>
            <span class="n">world_size</span><span class="p">,</span> <span class="n">equal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">locality_hints</span><span class="o">=</span><span class="n">worker_node_ids</span>
        <span class="p">)</span>

        <span class="c1"># Return the assigned iterators for each worker.</span>
        <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">it</span><span class="p">}</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">iterator_shards</span><span class="p">]</span>


<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
    <span class="n">dataset_config</span><span class="o">=</span><span class="n">MyCustomDataConfig</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>子类必须是可序列化的，因为 Ray Train 会将其从驱动脚本复制到 Trainer 的驱动 actor。Ray Train 在 Trainer 组的主 actor 上调用其 <a class="reference internal" href="../api/doc/ray.train.DataConfig.configure.html#ray.train.DataConfig.configure" title="ray.train.DataConfig.configure"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure</span></code></a> 方法，以为每个 worker 创建数据迭代器。</p>
<p>通常，你可以使用 <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> 来设置任何必须在 worker 开始迭代数据之前提前发生的共享设置。设置在每次 Trainer 运行开始时运行。</p>
</section>
</section>
<section id="id9">
<h2>随机洗牌<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h2>
<p>根据您正在训练的模型，随机打乱每个时期的数据对于模型质量非常重要</p>
<p>Ray Data 有两种随机改组方法：</p>
<ol class="arabic simple">
<li><p>在每个训练 worker 上对数据块进行随机化。这需要更少的通信，但牺牲了一些随机性（即，出现在同一数据块中的行更有可能在迭代顺序中靠近彼此）。</p></li>
<li><p>全局洗牌，这更昂贵。这将完全使行迭代顺序与原始数据集顺序解耦，但代价是更多的计算、I/O 和通信。</p></li>
</ol>
<p>对于大多数情况来说，选项 1 就足够了。</p>
<p>首先，通过 <code class="xref py py-meth docutils literal notranslate"><span class="pre">randomize_block_order()</span></code> 对数据集的每个 <a class="reference internal" href="../../data/data-internals.html#dataset-concept"><span class="std std-ref">block</span></a> 进行随机化。然后，在训练期间迭代数据集时，通过为 <code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches()</span></code> 或 <code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches()</span></code> 指定 <code class="docutils literal notranslate"><span class="pre">local_shuffle_buffer_size</span></code> 来启用本地洗牌。</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="c1"># Randomize the blocks of this dataset.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">randomize_block_order</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Use a shuffle buffer size of 10k rows.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
            <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>如果您的模型对洗牌质量很敏感，请调 <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle" title="ray.data.Dataset.random_shuffle"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.random_shuffle</span></code></a> 以执行全局洗牌。</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="c1"># Do a global shuffle of all rows in this dataset.</span>
<span class="c1"># The dataset will be shuffled on each iteration, unless `.materialize()`</span>
<span class="c1"># is called after the `.random_shuffle()`</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">random_shuffle</span><span class="p">()</span>
</pre></div>
</div>
<p>有关如何优化 shuffing 以及选择哪种方法的更多信息，请参阅 <a class="reference internal" href="../../data/performance-tips.html#optimizing-shuffles"><span class="std std-ref">shuffling 优化指南</span></a>。</p>
</section>
<section id="id10">
<h2>启用可重现性<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h2>
<p>当开发或超参数调整模型时，数据提取的可重现性很重要，以确保数据提取不会影响模型质量。遵循以下三个步骤以启用可重现性：</p>
<p><strong>步骤 1:</strong> 通过在 <a class="reference internal" href="../../data/api/doc/ray.data.DataContext.html#ray.data.DataContext" title="ray.data.context.DataContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataContext</span></code></a> 中设置 <code class="xref py py-obj docutils literal notranslate"><span class="pre">preserve_order</span></code> 标志来启用 Ray 数据集中的确定性执行。</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="c1"># Preserve ordering in Ray Datasets for reproducibility.</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">execution_options</span><span class="o">.</span><span class="n">preserve_order</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>步骤 2:</strong> 设置任何洗牌操作的种子：</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle" title="ray.data.Dataset.random_shuffle"><code class="xref py py-meth docutils literal notranslate"><span class="pre">random_shuffle</span></code></a> 的 <code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code> 参数</p></li>
<li><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.randomize_block_order.html#ray.data.Dataset.randomize_block_order" title="ray.data.Dataset.randomize_block_order"><code class="xref py py-meth docutils literal notranslate"><span class="pre">randomize_block_order</span></code></a> 的 <code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code> 参数</p></li>
<li><p><a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches</span></code></a> 的 <code class="xref py py-obj docutils literal notranslate"><span class="pre">local_shuffle_seed</span></code> 参数</p></li>
</ul>
<p><strong>步骤 3:</strong> 遵循最佳实践，以确保您的训练框架在可重现性方面的设置正确。更多信息，请参见 <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">Pytorch 可重现性指南</a>。</p>
</section>
<section id="preprocessing-structured-data">
<span id="id12"></span><h2>预处理结构化数据<a class="headerlink" href="#preprocessing-structured-data" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>本节适用于表格/结构化数据。预处理非结构化数据的推荐方式是使用 Ray Data 操作，例如 <code class="xref py py-obj docutils literal notranslate"><span class="pre">map_batches</span></code>。
有关更多详细信息，请参见 <a class="reference internal" href="../../data/working-with-pytorch.html#working-with-pytorch"><span class="std std-ref">Ray Data Working with Pytorch guide</span></a>。</p>
</div>
<p>针对表格数据，我们建议使用 Ray Data <a class="reference internal" href="../../data/preprocessors.html#air-preprocessors"><span class="std std-ref">preprocessors</span></a>，这些预处理器实现了常见的数据预处理操作。
你可使用这些预处理器在将数据集传递到 Trainer 之前对数据集进行处理。例如：</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.data.preprocessors</span> <span class="kn">import</span> <span class="n">Concatenator</span><span class="p">,</span> <span class="n">StandardScaler</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@air-example-data/breast_cancer.csv&quot;</span><span class="p">)</span>

<span class="c1"># Create preprocessors to scale some columns and concatenate the results.</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean radius&quot;</span><span class="p">,</span> <span class="s2">&quot;mean texture&quot;</span><span class="p">])</span>
<span class="n">concatenator</span> <span class="o">=</span> <span class="n">Concatenator</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Compute dataset statistics and get transformed datasets. Note that the</span>
<span class="c1"># fit call is executed immediately, but the transformation is lazy.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">concatenator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">get_metadata</span><span class="p">())</span>  <span class="c1"># prints {&quot;preprocessor_pkl&quot;: ...}</span>

    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Prefetch 10 batches at a time.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Save a checkpoint.</span>
    <span class="k">with</span> <span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">},</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">),</span>
        <span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">dataset</span><span class="p">},</span>
    <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;preprocessor_pkl&quot;</span><span class="p">:</span> <span class="n">scaler</span><span class="o">.</span><span class="n">serialize</span><span class="p">()},</span>
<span class="p">)</span>

<span class="c1"># Get the fitted preprocessor back from the result metadata.</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">get_metadata</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">StandardScaler</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;preprocessor_pkl&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p>在本示例中，我们使用 <code class="docutils literal notranslate"><span class="pre">Trainer(metadata={...})</span></code> 构造函数参数持久化了拟合的预处理器。此参数指定一个字典，将在 <code class="docutils literal notranslate"><span class="pre">TrainContext.get_metadata()</span></code> 和 <code class="docutils literal notranslate"><span class="pre">checkpoint.get_metadata()</span></code> 中可用，用于从 Trainer 保存的检查点中重新创建拟合的预处理器以用于推理。</p>
</section>
<section id="id13">
<h2>性能技巧<a class="headerlink" href="#id13" title="Permalink to this headline">#</a></h2>
<section id="id14">
<h3>预取批次<a class="headerlink" href="#id14" title="Permalink to this headline">#</a></h3>
<p>在迭代数据集进行训练时，您可以增加 <code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> 参数以进一步提高性能。在训练当前批次时，这会启动 N 个后台线程来获取和处理下一个 N 个批次。</p>
<p>如果训练在跨节点数据传输或在最后一英里预处理上受到瓶颈，这种方法可以帮助。例如，将批次转换为张量或执行 <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code>。但是，增加 <code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> 会导致更多数据需要保存在堆内存中。默认情况下，<code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> 设置为 1。</p>
<p>比如，以下代码每次预取 10 个批次给每个训练 worker：</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Prefetch 10 batches at a time.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="dataset-cache-performance">
<span id="id15"></span><h3>缓存预处理数据集<a class="headerlink" href="#dataset-cache-performance" title="Permalink to this headline">#</a></h3>
<p>如果你在 GPU 上训练并且有一个昂贵的 CPU 预处理操作，这种方法可能会成为训练吞吐量的瓶颈。</p>
<p>如果你的预处理数据集小到可以放入 Ray 对象存储内存中（默认情况下这是总集群 RAM 的 30%），通过在预处理数据集上调用 <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize" title="ray.data.Dataset.materialize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">materialize()</span></code></a> 来在 Ray 的内置对象存储中 <em>materialize</em> 预处理数据集。此方法告诉 Ray Data 计算整个预处理数据并将其固定在 Ray 对象存储内存中。结果是，当重复迭代数据集时，不需要重新运行预处理操作。但是，如果预处理数据太大而无法放入 Ray 对象存储内存中，这种方法会大大降低性能，因为数据需要溢出到磁盘并从磁盘读回。</p>
<p>你希望在每个 epoch 运行的转换，例如随机化，应该在 materialize 调用之后。</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="c1"># Load the data.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@ray-example-data/iris.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Define a preprocessing function.</span>
<span class="k">def</span> <span class="nf">normalize_length</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">new_col</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">])</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;normalized.sepal.length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_col</span>
    <span class="k">del</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="c1"># Preprocess the data. Transformations that are made before the materialize call</span>
<span class="c1"># below are only run once.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">normalize_length</span><span class="p">)</span>

<span class="c1"># Materialize the dataset in object store memory.</span>
<span class="c1"># Only do this if train_ds is small enough to fit in object store memory.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">materialize</span><span class="p">()</span>

<span class="c1"># Dummy augmentation transform.</span>
<span class="k">def</span> <span class="nf">augment_data</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="c1"># Add per-epoch preprocessing. Transformations that you want to run per-epoch, such</span>
<span class="c1"># as data augmentation or randomization, should go after the materialize call.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">augment_data</span><span class="p">)</span>

<span class="c1"># Pass train_ds to the Trainer</span>
</pre></div>
</div>
</section>
<section id="cpu-only">
<h3>向集群添加 CPU-only 节点<a class="headerlink" href="#cpu-only" title="Permalink to this headline">#</a></h3>
<p>如果你在昂贵的 CPU 预处理上受到瓶颈，并且预处理数据集太大而无法放入对象存储内存中，那么材料化数据集就行不通。在这种情况下，由于 Ray 支持异构集群，您可以向集群添加更多的 CPU-only 节点。</p>
<p>对于受对象存储内存瓶颈的情况，向集群添加更多的 CPU-only 节点会增加总集群对象存储内存，从而允许在预处理和训练阶段之间缓冲更多数据。</p>
<p>对于预处理计算时间受限的情况，添加更多的 CPU-only 节点会向集群添加更多的 CPU 核心，进一步并行化预处理。如果您的预处理仍然无法快速饱和 GPU，则添加足够的 CPU-only 节点以 <a class="reference internal" href="#dataset-cache-performance"><span class="std std-ref">缓存预处理数据集</span></a>。</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../user-guides.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ray Train 用户指南</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="using-gpus.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">配置 Scale 和 GPU</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>