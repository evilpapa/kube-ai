
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Saving and Loading Checkpoints &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../_static/js/csat.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/user-guides/checkpoints.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Experiment Tracking" href="experiment-tracking.html" />
    <link rel="prev" title="Monitoring and Logging Metrics" href="monitoring-logging.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   入门
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data/data.html">
   Ray 数据「75%」
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../train.html">
   Ray 训练「0%」
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../overview.html">
     概述
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch.html">
     PyTorch 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch-lightning.html">
     PyTorch Lightning 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-transformers.html">
     Hugging Face Transformers 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../more-frameworks.html">
     更多框架
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="../user-guides.html">
     用户指南
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="data-loading-preprocessing.html">
       Data Loading and Preprocessing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="using-gpus.html">
       Configuring Scale and GPUs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="persistent-storage.html">
       Configuring Persistent Storage
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="monitoring-logging.html">
       Monitoring and Logging Metrics
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Saving and Loading Checkpoints
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="experiment-tracking.html">
       Experiment Tracking
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="results.html">
       Inspecting Training Results
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="fault-tolerance.html">
       Handling Failures and Node Preemption
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="reproducibility.html">
       Reproducibility
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hyperparameter-optimization.html">
       Hyperparameter Optimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../examples.html">
     示例
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../benchmarks.html">
     基准
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/api.html">
     Ray 训练 API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/user-guides/checkpoints.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/user-guides/checkpoints.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/train/user-guides/checkpoints.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-checkpoints-during-training">
   Saving checkpoints during training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-checkpoints-from-multiple-workers-distributed-checkpointing">
     Saving checkpoints from multiple workers (distributed checkpointing)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#configure-checkpointing">
   Configure checkpointing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-checkpoints-after-training">
   Using checkpoints after training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#restore-training-state-from-a-checkpoint">
   Restore training state from a checkpoint
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Saving and Loading Checkpoints</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-checkpoints-during-training">
   Saving checkpoints during training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-checkpoints-from-multiple-workers-distributed-checkpointing">
     Saving checkpoints from multiple workers (distributed checkpointing)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#configure-checkpointing">
   Configure checkpointing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-checkpoints-after-training">
   Using checkpoints after training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#restore-training-state-from-a-checkpoint">
   Restore training state from a checkpoint
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="saving-and-loading-checkpoints">
<span id="train-checkpointing"></span><h1>Saving and Loading Checkpoints<a class="headerlink" href="#saving-and-loading-checkpoints" title="Permalink to this headline">#</a></h1>
<p>Ray Train provides a way to snapshot training progress with <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoints</span></code></a>.</p>
<p>This is useful for:</p>
<ol class="arabic simple">
<li><p><strong>Storing the best-performing model weights:</strong> Save your model to persistent storage, and use it for downstream serving/inference.</p></li>
<li><p><strong>Fault tolerance:</strong> Handle node failures in a long-running training job on a cluster of pre-emptible machines/pods.</p></li>
<li><p><strong>Distributed checkpointing:</strong> When doing <em>model-parallel training</em>, Ray Train checkpointing provides an easy way to
<a class="reference internal" href="#train-distributed-checkpointing"><span class="std std-ref">upload model shards from each worker in parallel</span></a>,
without needing to gather the full model to a single node.</p></li>
<li><p><strong>Integration with Ray Tune:</strong> Checkpoint saving and loading is required by certain <a class="reference internal" href="../../tune/api/schedulers.html#tune-schedulers"><span class="std std-ref">Ray Tune schedulers</span></a>.</p></li>
</ol>
<section id="saving-checkpoints-during-training">
<span id="train-dl-saving-checkpoints"></span><h2>Saving checkpoints during training<a class="headerlink" href="#saving-checkpoints-during-training" title="Permalink to this headline">#</a></h2>
<p>The <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> is a lightweight interface provided
by Ray Train that represents a <em>directory</em> that exists at on local or remote storage.</p>
<p>For example, a checkpoint could point to a directory in cloud storage:
<code class="docutils literal notranslate"><span class="pre">s3://my-bucket/my-checkpoint-dir</span></code>.
A locally available checkpoint points to a location on the local filesystem:
<code class="docutils literal notranslate"><span class="pre">/tmp/my-checkpoint-dir</span></code>.</p>
<p>Here’s how you save a checkpoint in the training loop:</p>
<ol class="arabic simple">
<li><p>Write your model checkpoint to a local directory.</p>
<ul class="simple">
<li><p>Since a <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> just points to a directory, the contents are completely up to you.</p></li>
<li><p>This means that you can use any serialization format you want.</p></li>
<li><p>This makes it <strong>easy to use familiar checkpoint utilities provided by training frameworks</strong>, such as
<code class="docutils literal notranslate"><span class="pre">torch.save</span></code>, <code class="docutils literal notranslate"><span class="pre">pl.Trainer.save_checkpoint</span></code>, Accelerate’s <code class="docutils literal notranslate"><span class="pre">accelerator.save_model</span></code>,
Transformers’ <code class="docutils literal notranslate"><span class="pre">save_pretrained</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.keras.Model.save</span></code>, etc.</p></li>
</ul>
</li>
<li><p>Create a <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> from the directory using <a class="reference internal" href="../api/doc/ray.train.Checkpoint.from_directory.html#ray.train.Checkpoint.from_directory" title="ray.train.Checkpoint.from_directory"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Checkpoint.from_directory</span></code></a>.</p></li>
<li><p>Report the checkpoint to Ray Train using <a class="reference internal" href="../api/doc/ray.train.report.html#ray.train.report" title="ray.train.report"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.report(metrics,</span> <span class="pre">checkpoint=...)</span></code></a>.</p>
<ul class="simple">
<li><p>The metrics reported alongside the checkpoint are used to <a class="reference internal" href="#train-dl-configure-checkpoints"><span class="std std-ref">keep track of the best-performing checkpoints</span></a>.</p></li>
<li><p>This will <strong>upload the checkpoint to persistent storage</strong> if configured. See <a class="reference internal" href="persistent-storage.html#persistent-storage-guide"><span class="std std-ref">Configuring Persistent Storage</span></a>.</p></li>
</ul>
</li>
</ol>
<figure class="align-default" id="id1">
<img alt="../../_images/checkpoint_lifecycle.png" src="../../_images/checkpoint_lifecycle.png" />
<figcaption>
<p><span class="caption-text">The lifecycle of a <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a>, from being saved locally
to disk to being uploaded to persistent storage via <code class="docutils literal notranslate"><span class="pre">train.report</span></code>.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As shown in the figure above, the best practice for saving checkpoints is to
first dump the checkpoint to a local temporary directory. Then, the call to <code class="docutils literal notranslate"><span class="pre">train.report</span></code>
uploads the checkpoint to its final persistent storage location.
Then, the local temporary directory can be safely cleaned up to free up disk space
(e.g., from exiting the <code class="docutils literal notranslate"><span class="pre">tempfile.TemporaryDirectory</span></code> context).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In standard DDP training, where each worker has a copy of the full-model, you should
only save and report a checkpoint from a single worker to prevent redundant uploads.</p>
<p>This typically looks like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>


<span class="k">def</span> <span class="nf">train_fn</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="p">}</span>
    <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_checkpoint_dir</span><span class="p">:</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Only the global rank 0 worker saves and reports the checkpoint</span>
        <span class="k">if</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="o">...</span>  <span class="c1"># Save checkpoint to temp_checkpoint_dir</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span>

        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>


</pre></div>
</div>
<p>If using parallel training strategies such as DeepSpeed Zero-3 and FSDP, where
each worker only has a shard of the full-model, you should save and report a checkpoint
from each worker. See <a class="reference internal" href="#train-distributed-checkpointing"><span class="std std-ref">Saving checkpoints from multiple workers (distributed checkpointing)</span></a> for an example.</p>
</div>
<p>Here are a few examples of saving checkpoints with different training frameworks:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-0">
Native PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="kn">import</span> <span class="nn">ray.train.torch</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1"># toy neural network : 1-layer</span>
    <span class="c1"># Wrap the model in DDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_checkpoint_dir</span><span class="p">:</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">should_checkpoint</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;checkpoint_freq&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="c1"># In standard DDP training, where the model is the same across all ranks,</span>
            <span class="c1"># only the global rank 0 worker needs to save and report the checkpoint</span>
            <span class="k">if</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">should_checkpoint</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>  <span class="c1"># NOTE: Unwrap the model.</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">)</span>

            <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You most likely want to unwrap the DDP model before saving it to a checkpoint.
<code class="docutils literal notranslate"><span class="pre">model.module.state_dict()</span></code> is the state dict without each key having a <code class="docutils literal notranslate"><span class="pre">&quot;module.&quot;</span></code> prefix.</p>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-1">
PyTorch Lightning</label><div class="sd-tab-content docutils">
<p>Ray Train leverages PyTorch Lightning’s <code class="docutils literal notranslate"><span class="pre">Callback</span></code> interface to report metrics
and checkpoints. We provide a simple callback implementation that reports
<code class="docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code>.</p>
<p>Specifically, on each train epoch end, it</p>
<ul class="simple">
<li><p>collects all the logged metrics from <code class="docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code></p></li>
<li><p>saves a checkpoint via <code class="docutils literal notranslate"><span class="pre">trainer.save_checkpoint</span></code></p></li>
<li><p>reports to Ray Train via <a class="reference internal" href="../api/doc/ray.train.report.html#ray.train.report" title="ray.train.report"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.report(metrics,</span> <span class="pre">checkpoint)</span></code></a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.lightning</span> <span class="kn">import</span> <span class="n">RayTrainReportCallback</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>


<span class="k">class</span> <span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="n">mean_acc</span> <span class="o">=</span> <span class="n">calculate_accuracy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;mean_accuracy&quot;</span><span class="p">,</span> <span class="n">mean_acc</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_func_per_worker</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">datamodule</span> <span class="o">=</span> <span class="n">MyLightningDataModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="c1"># ...</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">RayTrainReportCallback</span><span class="p">()]</span>
    <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">datamodule</span><span class="p">)</span>


<span class="n">ray_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span>
            <span class="n">num_to_keep</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">checkpoint_score_attribute</span><span class="o">=</span><span class="s2">&quot;mean_accuracy&quot;</span><span class="p">,</span>
            <span class="n">checkpoint_score_order</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can always get the saved checkpoint path from <a class="reference internal" href="../api/doc/ray.train.Result.html#ray.train.Result.checkpoint" title="ray.train.Result.checkpoint"><code class="xref py py-attr docutils literal notranslate"><span class="pre">result.checkpoint</span></code></a> and
<a class="reference internal" href="../api/doc/ray.train.Result.html#ray.train.Result.best_checkpoints" title="ray.train.Result.best_checkpoints"><code class="xref py py-attr docutils literal notranslate"><span class="pre">result.best_checkpoints</span></code></a>.</p>
<p>For more advanced usage (e.g. reporting at different frequency, reporting
customized checkpoint files), you can implement your own customized callback.
Here is a simple example that reports a checkpoint every 3 epochs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">ray.train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span>


<span class="k">class</span> <span class="nc">CustomRayTrainReportCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="n">should_checkpoint</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>
            <span class="c1"># Fetch metrics</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

            <span class="c1"># Add customized metrics</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">current_epoch</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;custom_metric&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">123</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">global_rank</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">should_checkpoint</span><span class="p">:</span>
                <span class="c1"># Save model checkpoint file to tmpdir</span>
                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s2">&quot;ckpt.pt&quot;</span><span class="p">)</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span>

            <span class="c1"># Report to train session</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>


</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-2">
Hugging Face Transformers</label><div class="sd-tab-content docutils">
<p>Ray Train leverages HuggingFace Transformers Trainer’s <code class="docutils literal notranslate"><span class="pre">Callback</span></code> interface
to report metrics and checkpoints.</p>
<p><strong>Option 1: Use Ray Train’s default report callback</strong></p>
<p>We provide a simple callback implementation <a class="reference internal" href="../api/doc/ray.train.huggingface.transformers.RayTrainReportCallback.html#ray.train.huggingface.transformers.RayTrainReportCallback" title="ray.train.huggingface.transformers.RayTrainReportCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayTrainReportCallback</span></code></a> that
reports on checkpoint save. You can change the checkpointing frequency by <code class="docutils literal notranslate"><span class="pre">save_strategy</span></code> and <code class="docutils literal notranslate"><span class="pre">save_steps</span></code>.
It collects the latest logged metrics and report them together with the latest saved checkpoint.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.huggingface.transformers</span> <span class="kn">import</span> <span class="n">RayTrainReportCallback</span><span class="p">,</span> <span class="n">prepare_trainer</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="c1"># Configure logging, saving, evaluation strategies as usual.</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">logging_strategy</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

    <span class="c1"># Add a report callback to transformers Trainer</span>
    <span class="c1"># =============================================</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">RayTrainReportCallback</span><span class="p">())</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">prepare_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>


<span class="n">ray_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
        <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span>
            <span class="n">num_to_keep</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">checkpoint_score_attribute</span><span class="o">=</span><span class="s2">&quot;eval_loss&quot;</span><span class="p">,</span>  <span class="c1"># The monitoring metric</span>
            <span class="n">checkpoint_score_order</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Note that <a class="reference internal" href="../api/doc/ray.train.huggingface.transformers.RayTrainReportCallback.html#ray.train.huggingface.transformers.RayTrainReportCallback" title="ray.train.huggingface.transformers.RayTrainReportCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayTrainReportCallback</span></code></a>
binds the latest metrics and checkpoints together,
so users can properly configure <code class="docutils literal notranslate"><span class="pre">logging_strategy</span></code>, <code class="docutils literal notranslate"><span class="pre">save_strategy</span></code> and <code class="docutils literal notranslate"><span class="pre">evaluation_strategy</span></code>
to ensure the monitoring metric is logged at the same step as checkpoint saving.</p>
<p>For example, the evaluation metrics (<code class="docutils literal notranslate"><span class="pre">eval_loss</span></code> in this case) are logged during
evaluation. If users want to keep the best 3 checkpoints according to <code class="docutils literal notranslate"><span class="pre">eval_loss</span></code>, they
should align the saving and evaluation frequency. Below are two examples of valid configurations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span>
    <span class="n">eval_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># And more ...</span>
</pre></div>
</div>
<p><strong>Option 2: Implement your customized report callback</strong></p>
<p>If you feel that Ray Train’s default <a class="reference internal" href="../api/doc/ray.train.huggingface.transformers.RayTrainReportCallback.html#ray.train.huggingface.transformers.RayTrainReportCallback" title="ray.train.huggingface.transformers.RayTrainReportCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayTrainReportCallback</span></code></a>
is not sufficient for your use case, you can also implement a callback yourself!
Below is a example implementation that collects latest metrics
and reports on checkpoint save.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>

<span class="kn">from</span> <span class="nn">transformers.trainer_callback</span> <span class="kn">import</span> <span class="n">TrainerCallback</span>


<span class="k">class</span> <span class="nc">MyTrainReportCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Log is called on evaluation step and logging step.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Event called after a checkpoint save.&quot;&quot;&quot;</span>

        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Build a Ray Train Checkpoint from the latest checkpoint</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">get_last_checkpoint</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="c1"># Report to Ray Train with up-to-date metrics</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

        <span class="c1"># Clear the metrics buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>


</pre></div>
</div>
<p>You can customize when (<code class="docutils literal notranslate"><span class="pre">on_save</span></code>, <code class="docutils literal notranslate"><span class="pre">on_epoch_end</span></code>, <code class="docutils literal notranslate"><span class="pre">on_evaluate</span></code>) and
what (customized metrics and checkpoint files) to report by implementing your own
Transformers Trainer callback.</p>
</div>
</div>
<section id="saving-checkpoints-from-multiple-workers-distributed-checkpointing">
<span id="train-distributed-checkpointing"></span><h3>Saving checkpoints from multiple workers (distributed checkpointing)<a class="headerlink" href="#saving-checkpoints-from-multiple-workers-distributed-checkpointing" title="Permalink to this headline">#</a></h3>
<p>In model parallel training strategies where each worker only has a shard of the full-model,
you can save and report checkpoint shards in parallel from each worker.</p>
<figure class="align-default" id="id2">
<img alt="../../_images/persistent_storage_checkpoint.png" src="../../_images/persistent_storage_checkpoint.png" />
<figcaption>
<p><span class="caption-text">Distributed checkpointing in Ray Train. Each worker uploads its own checkpoint shard
to persistent storage independently.</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Distributed checkpointing is the best practice for saving checkpoints
when doing model-parallel training (e.g., DeepSpeed, FSDP, Megatron-LM).</p>
<p>There are two major benefits:</p>
<ol class="arabic">
<li><p><strong>It is faster, resulting in less idle time.</strong> Faster checkpointing incentivizes more frequent checkpointing!</p>
<p>Each worker can upload its checkpoint shard in parallel,
maximizing the network bandwidth of the cluster. Instead of a single node
uploading the full model of size <code class="docutils literal notranslate"><span class="pre">M</span></code>, the cluster distributes the load across
<code class="docutils literal notranslate"><span class="pre">N</span></code> nodes, each uploading a shard of size <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">/</span> <span class="pre">N</span></code>.</p>
</li>
<li><p><strong>Distributed checkpointing avoids needing to gather the full model onto a single worker’s CPU memory.</strong></p>
<p>This gather operation puts a large CPU memory requirement on the worker that performs checkpointing
and is a common source of OOM errors.</p>
</li>
</ol>
<p>Here is an example of distributed checkpointing with PyTorch:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_checkpoint_dir</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="o">...</span><span class="p">,</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;model-rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">)</span>

        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://bucket/&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># The checkpoint in cloud storage will contain: model-rank=0.pt, model-rank=1.pt</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Checkpoint files with the same name will collide between workers.
You can get around this by adding a rank-specific suffix to checkpoint files.</p>
<p>Note that having filename collisions does not error, but it will result in the last
uploaded version being the one that is persisted. This is fine if the file
contents are the same across all workers.</p>
<p>Model shard saving utilities provided by frameworks such as DeepSpeed will create
rank-specific filenames already, so you usually do not need to worry about this.</p>
</div>
</section>
</section>
<section id="configure-checkpointing">
<span id="train-dl-configure-checkpoints"></span><h2>Configure checkpointing<a class="headerlink" href="#configure-checkpointing" title="Permalink to this headline">#</a></h2>
<p>Ray Train provides some configuration options for checkpointing via <a class="reference internal" href="../api/doc/ray.train.CheckpointConfig.html#ray.train.CheckpointConfig" title="ray.train.CheckpointConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointConfig</span></code></a>.
The primary configuration is keeping only the top <code class="docutils literal notranslate"><span class="pre">K</span></code> checkpoints with respect to a metric.
Lower-performing checkpoints are deleted to save storage space. By default, all checkpoints are kept.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">RunConfig</span><span class="p">,</span> <span class="n">CheckpointConfig</span>

<span class="c1"># Example 1: Only keep the 2 *most recent* checkpoints and delete the others.</span>
<span class="n">run_config</span> <span class="o">=</span> <span class="n">RunConfig</span><span class="p">(</span><span class="n">checkpoint_config</span><span class="o">=</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">num_to_keep</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>


<span class="c1"># Example 2: Only keep the 2 *best* checkpoints and delete the others.</span>
<span class="n">run_config</span> <span class="o">=</span> <span class="n">RunConfig</span><span class="p">(</span>
    <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">CheckpointConfig</span><span class="p">(</span>
        <span class="n">num_to_keep</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="c1"># *Best* checkpoints are determined by these params:</span>
        <span class="n">checkpoint_score_attribute</span><span class="o">=</span><span class="s2">&quot;mean_accuracy&quot;</span><span class="p">,</span>
        <span class="n">checkpoint_score_order</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># This will store checkpoints on S3.</span>
    <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;s3://remote-bucket/location&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to save the top <code class="docutils literal notranslate"><span class="pre">num_to_keep</span></code> checkpoints with respect to a metric via
<a class="reference internal" href="../api/doc/ray.train.CheckpointConfig.html#ray.train.CheckpointConfig" title="ray.train.CheckpointConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointConfig</span></code></a>,
please ensure that the metric is always reported together with the checkpoints.</p>
</div>
</section>
<section id="using-checkpoints-after-training">
<h2>Using checkpoints after training<a class="headerlink" href="#using-checkpoints-after-training" title="Permalink to this headline">#</a></h2>
<p>The latest saved checkpoint can be accessed with <a class="reference internal" href="../api/doc/ray.train.Result.html#ray.train.Result.checkpoint" title="ray.train.Result.checkpoint"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Result.checkpoint</span></code></a>.</p>
<p>The full list of persisted checkpoints can be accessed with <a class="reference internal" href="../api/doc/ray.train.Result.html#ray.train.Result.best_checkpoints" title="ray.train.Result.best_checkpoints"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Result.best_checkpoints</span></code></a>.
If <a class="reference internal" href="../api/doc/ray.train.CheckpointConfig.html#ray.train.CheckpointConfig" title="ray.train.CheckpointConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CheckpointConfig(num_to_keep)</span></code></a> is set, this list will contain the best <code class="docutils literal notranslate"><span class="pre">num_to_keep</span></code> checkpoints.</p>
<p>See <a class="reference internal" href="results.html#train-inspect-results"><span class="std std-ref">Inspecting Training Results</span></a> for a full guide on inspecting training results.</p>
<p><a class="reference internal" href="../api/doc/ray.train.Checkpoint.as_directory.html#ray.train.Checkpoint.as_directory" title="ray.train.Checkpoint.as_directory"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Checkpoint.as_directory</span></code></a>
and <a class="reference internal" href="../api/doc/ray.train.Checkpoint.to_directory.html#ray.train.Checkpoint.to_directory" title="ray.train.Checkpoint.to_directory"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Checkpoint.to_directory</span></code></a>
are the two main APIs to interact with Train checkpoints:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span>

<span class="c1"># Create a sample locally available checkpoint</span>
<span class="n">example_checkpoint_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/tmp/test-checkpoint&quot;</span><span class="p">)</span>
<span class="n">example_checkpoint_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
<span class="n">example_checkpoint_dir</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">touch</span><span class="p">()</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">example_checkpoint_dir</span><span class="p">)</span>

<span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>

<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_directory</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="restore-training-state-from-a-checkpoint">
<span id="train-dl-loading-checkpoints"></span><h2>Restore training state from a checkpoint<a class="headerlink" href="#restore-training-state-from-a-checkpoint" title="Permalink to this headline">#</a></h2>
<p>In order to enable fault tolerance, you should modify your training loop to restore
training state from a <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a>.</p>
<p>The <a class="reference internal" href="../api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpoint</span></code></a> to restore from can be accessed in the
training function with <a class="reference internal" href="../api/doc/ray.train.get_checkpoint.html#ray.train.get_checkpoint" title="ray.train.get_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.get_checkpoint</span></code></a>.</p>
<p>The checkpoint returned by <a class="reference internal" href="../api/doc/ray.train.get_checkpoint.html#ray.train.get_checkpoint" title="ray.train.get_checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.train.get_checkpoint</span></code></a> is populated in two ways:</p>
<ol class="arabic simple">
<li><p>It can be auto-populated as the latest reported checkpoint, e.g. during <a class="reference internal" href="fault-tolerance.html#train-fault-tolerance"><span class="std std-ref">automatic failure recovery</span></a> or <a class="reference internal" href="fault-tolerance.html#train-restore-guide"><span class="std std-ref">on manual restoration</span></a>.</p></li>
<li><p>It can be manually populated by passing a checkpoint to the <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint</span></code> argument of a Ray <a class="reference internal" href="../api/doc/ray.train.trainer.BaseTrainer.html#ray.train.trainer.BaseTrainer" title="ray.train.trainer.BaseTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.
This is useful for initializing a new training run with a previous run’s checkpoint.</p></li>
</ol>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-3">
Native PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="kn">import</span> <span class="nn">ray.train.torch</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="c1"># toy neural network : 1-layer</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="c1"># ====== Resume training state from the checkpoint. ======</span>
    <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)))</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;optimizer.pt&quot;</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;extra_state.pt&quot;</span><span class="p">))[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
    <span class="c1"># ========================================================</span>

    <span class="c1"># Wrap the model in DDP</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>

        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_checkpoint_dir</span><span class="p">:</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">should_checkpoint</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;checkpoint_freq&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="c1"># In standard DDP training, where the model is the same across all ranks,</span>
            <span class="c1"># only the global rank 0 worker needs to save and report the checkpoint</span>
            <span class="k">if</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_world_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">should_checkpoint</span><span class="p">:</span>
                <span class="c1"># === Make sure to save all state needed for resuming training ===</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>  <span class="c1"># NOTE: Unwrap the model.</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;optimizer.pt&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">},</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;extra_state.pt&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="c1"># ================================================================</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">temp_checkpoint_dir</span><span class="p">)</span>

            <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Intentional error to showcase restoration!&quot;</span><span class="p">)</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">run_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span><span class="n">failure_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">FailureConfig</span><span class="p">(</span><span class="n">max_failures</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Seed a training run with a checkpoint using `resume_from_checkpoint`</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
</input><label class="sd-tab-label" for="sd-tab-item-4">
PyTorch Lightning</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train.lightning</span> <span class="kn">import</span> <span class="n">RayTrainReportCallback</span>


<span class="k">def</span> <span class="nf">train_func_per_worker</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">datamodule</span> <span class="o">=</span> <span class="n">MyLightningDataModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">RayTrainReportCallback</span><span class="p">()])</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">ckpt_dir</span><span class="p">:</span>
            <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ckpt_dir</span><span class="p">,</span> <span class="s2">&quot;checkpoint.ckpt&quot;</span><span class="p">)</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">datamodule</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="o">=</span><span class="n">ckpt_path</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">datamodule</span><span class="p">)</span>


<span class="c1"># Build a Ray Train Checkpoint</span>
<span class="c1"># Suppose we have a Lightning checkpoint at `s3://bucket/ckpt_dir/checkpoint.ckpt`</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="p">(</span><span class="s2">&quot;s3://bucket/ckpt_dir&quot;</span><span class="p">)</span>

<span class="c1"># Resume training from checkpoint file</span>
<span class="n">ray_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In these examples, <a class="reference internal" href="../api/doc/ray.train.Checkpoint.as_directory.html#ray.train.Checkpoint.as_directory" title="ray.train.Checkpoint.as_directory"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Checkpoint.as_directory</span></code></a>
is used to view the checkpoint contents as a local directory.</p>
<p><em>If the checkpoint points to a local directory</em>, this method just returns the
local directory path without making a copy.</p>
<p><em>If the checkpoint points to a remote directory</em>, this method will download the
checkpoint to a local temporary directory and return the path to the temporary directory.</p>
<p><strong>If multiple processes on the same node call this method simultaneously,</strong>
only a single process will perform the download, while the others
wait for the download to finish. Once the download finishes, all processes receive
the same local (temporary) directory to read from.</p>
<p>Once all processes have finished working with the checkpoint, the temporary directory
is cleaned up.</p>
</div>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="monitoring-logging.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Monitoring and Logging Metrics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="experiment-tracking.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Experiment Tracking</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>