
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Data Loading and Preprocessing &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/js/versionwarning.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../_static/js/csat.js"></script>
    <script defer="defer" src="../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../_static/js/custom.js"></script>
    <script defer="defer" src="../../_static/js/top-navigation.js"></script>
    <script src="../../_static/js/tags.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Configuring Scale and GPUs" href="using-gpus.html" />
    <link rel="prev" title="Ray Train 用户指南" href="../user-guides.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/getting-started.html">
   入门
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data/data.html">
   Ray 数据「75%」
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../train.html">
   Ray 训练「0%」
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../overview.html">
     概述
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch.html">
     PyTorch 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-pytorch-lightning.html">
     PyTorch Lightning 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../getting-started-transformers.html">
     Hugging Face Transformers 指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../more-frameworks.html">
     更多框架
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="../user-guides.html">
     用户指南
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Data Loading and Preprocessing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="using-gpus.html">
       Configuring Scale and GPUs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="persistent-storage.html">
       Configuring Persistent Storage
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="monitoring-logging.html">
       Monitoring and Logging Metrics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="checkpoints.html">
       Saving and Loading Checkpoints
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="experiment-tracking.html">
       Experiment Tracking
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="results.html">
       Inspecting Training Results
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="fault-tolerance.html">
       Handling Failures and Node Preemption
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="reproducibility.html">
       Reproducibility
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hyperparameter-optimization.html">
       Hyperparameter Optimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../examples.html">
     示例
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../benchmarks.html">
     基准
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/api.html">
     Ray 训练 API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/user-guides/data-loading-preprocessing.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/user-guides/data-loading-preprocessing.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/train/user-guides/data-loading-preprocessing.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quickstart">
   Quickstart
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-data">
     Loading data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing-data">
     Preprocessing data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inputting-and-splitting-data">
     Inputting and splitting data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consuming-data">
     Consuming data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#starting-with-pytorch-data">
   Starting with PyTorch data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#splitting-datasets">
   Splitting datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#full-customization-advanced">
     Full customization (advanced)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-shuffling">
   Random shuffling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#enabling-reproducibility">
   Enabling reproducibility
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-structured-data">
   Preprocessing structured data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-tips">
   Performance tips
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prefetching-batches">
     Prefetching batches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#caching-the-preprocessed-dataset">
     Caching the preprocessed dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-cpu-only-nodes-to-your-cluster">
     Adding CPU-only nodes to your cluster
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Data Loading and Preprocessing</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quickstart">
   Quickstart
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-data">
     Loading data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing-data">
     Preprocessing data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inputting-and-splitting-data">
     Inputting and splitting data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#consuming-data">
     Consuming data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#starting-with-pytorch-data">
   Starting with PyTorch data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#splitting-datasets">
   Splitting datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#full-customization-advanced">
     Full customization (advanced)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-shuffling">
   Random shuffling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#enabling-reproducibility">
   Enabling reproducibility
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preprocessing-structured-data">
   Preprocessing structured data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-tips">
   Performance tips
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prefetching-batches">
     Prefetching batches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#caching-the-preprocessed-dataset">
     Caching the preprocessed dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-cpu-only-nodes-to-your-cluster">
     Adding CPU-only nodes to your cluster
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="data-loading-and-preprocessing">
<span id="data-ingest-torch"></span><h1>Data Loading and Preprocessing<a class="headerlink" href="#data-loading-and-preprocessing" title="Permalink to this headline">#</a></h1>
<p>Ray Train integrates with <a class="reference internal" href="../../data/data.html#data"><span class="std std-ref">Ray Data</span></a> to offer an efficient, streaming solution for loading and preprocessing large datasets.
We recommend using Ray Data for its ability to performantly support large-scale distributed training workloads - for advantages and comparisons to alternatives, see <a class="reference internal" href="../../data/overview.html#data-overview"><span class="std std-ref">Ray Data Overview</span></a>.</p>
<p>In this guide, we will cover how to incorporate Ray Data into your Ray Train script, and different ways to customize your data ingestion pipeline.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_images/train_ingest.png"><img alt="../../_images/train_ingest.png" src="../../_images/train_ingest.png" style="width: 300px;" /></a>
</figure>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline">#</a></h2>
<p>Install Ray Data and Ray Train:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install -U <span class="s2">&quot;ray[data,train]&quot;</span>
</pre></div>
</div>
<p>Data ingestion can be set up with four basic steps:</p>
<ol class="arabic simple">
<li><p>Create a Ray Dataset.</p></li>
<li><p>Preprocess your Ray Dataset.</p></li>
<li><p>Input the preprocessed Dataset into the Ray Train Trainer.</p></li>
<li><p>Consume the Ray Dataset in your training function.</p></li>
</ol>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-UHlUb3JjaA==" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-UHlUb3JjaA==" name="UHlUb3JjaA==" role="tab" tabindex="0">PyTorch</button><button aria-controls="panel-0-UHlUb3JjaCBMaWdodG5pbmc=" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-UHlUb3JjaCBMaWdodG5pbmc=" name="UHlUb3JjaCBMaWdodG5pbmc=" role="tab" tabindex="-1">PyTorch Lightning</button><button aria-controls="panel-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" name="SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" role="tab" tabindex="-1">HuggingFace Transformers</button></div><div aria-labelledby="tab-0-UHlUb3JjaA==" class="sphinx-tabs-panel group-tab" id="panel-0-UHlUb3JjaA==" name="UHlUb3JjaA==" role="tabpanel" tabindex="0"><div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="c1"># Set this to True to use GPU.</span>
<span class="c1"># If False, do CPU training instead of GPU training.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Step 1: Create a Ray Dataset from in-memory Python lists.</span>
<span class="c1"># You can also create a Ray Dataset from many other sources and file</span>
<span class="c1"># formats.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">([{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">]}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)])</span>

<span class="c1"># Step 2: Preprocess your Ray Dataset.</span>
<span class="k">def</span> <span class="nf">increment</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">increment</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="c1"># Step 4: Access the dataset shard for the training worker via</span>
    <span class="c1"># ``get_dataset_shard``.</span>
    <span class="n">train_data_shard</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">train_data_shard</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="k">assert</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span>
            <span class="k">assert</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span>
            <span class="k">break</span> <span class="c1"># Only check one batch. Last batch can be partial.</span>

<span class="c1"># Step 3: Create a TorchTrainer. Specify the number of training workers and</span>
<span class="c1"># pass in your Ray Dataset.</span>
<span class="c1"># The Ray Dataset is automatically split across all training workers.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-UHlUb3JjaCBMaWdodG5pbmc=" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-UHlUb3JjaCBMaWdodG5pbmc=" name="UHlUb3JjaCBMaWdodG5pbmc=" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./train.csv&quot;</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./validation.csv&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_func_per_worker</span><span class="p">():</span>
    <span class="c1"># Access Ray datsets in your train_func via ``get_dataset_shard``.</span>
    <span class="c1"># The &quot;train&quot; dataset gets sharded across workers by default</span>
<span class="hll">    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</span><span class="hll">    <span class="n">val_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
</span>
    <span class="c1"># Create Ray dataset iterables via ``iter_torch_batches``.</span>
<span class="hll">    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span><span class="hll">    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span>
    <span class="o">...</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="c1"># ...</span>
    <span class="p">)</span>

    <span class="c1"># Feed the Ray dataset iterables to ``pl.Trainer.fit``.</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
<span class="hll">        <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
</span><span class="hll">        <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">val_dataloader</span>
</span>    <span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="s2">&quot;validation&quot;</span><span class="p">:</span> <span class="n">val_data</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" name="SHVnZ2luZ0ZhY2UgVHJhbnNmb3JtZXJz" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">ray.train</span>

<span class="o">...</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">hf_train_ds</span><span class="p">)</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">hf_eval_ds</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Access Ray datsets in your train_func via ``get_dataset_shard``.</span>
    <span class="c1"># The &quot;train&quot; dataset gets sharded across workers by default</span>
<span class="hll">    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</span><span class="hll">    <span class="n">eval_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;evaluation&quot;</span><span class="p">)</span>
</span>
    <span class="c1"># Create Ray dataset iterables via ``iter_torch_batches``.</span>
<span class="hll">    <span class="n">train_iterable_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span><span class="hll">    <span class="n">eval_iterable_ds</span> <span class="o">=</span> <span class="n">eval_ds</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span>
    <span class="o">...</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span> <span class="c1"># Required for iterable datasets</span>
<span class="hll">    <span class="p">)</span>
</span><span class="hll">
</span>    <span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_iterable_ds</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_iterable_ds</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Prepare your Transformers Trainer</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">huggingface</span><span class="o">.</span><span class="n">transformers</span><span class="o">.</span><span class="n">prepare_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">val_data</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div></div>
<section id="loading-data">
<span id="train-datasets-load"></span><h3>Loading data<a class="headerlink" href="#loading-data" title="Permalink to this headline">#</a></h3>
<p>Ray Datasets can be created from many different data sources and formats. For more details, see <a class="reference internal" href="../../data/loading-data.html#loading-data"><span class="std std-ref">Loading Data</span></a>.</p>
</section>
<section id="preprocessing-data">
<span id="train-datasets-preprocess"></span><h3>Preprocessing data<a class="headerlink" href="#preprocessing-data" title="Permalink to this headline">#</a></h3>
<p>Ray Data support a wide range of preprocessing operations that can be used to transform your data prior to training.</p>
<ul class="simple">
<li><p>For general preprocessing, see <a class="reference internal" href="../../data/transforming-data.html#transforming-data"><span class="std std-ref">Transforming Data</span></a>.</p></li>
<li><p>For tabular data, see <a class="reference internal" href="#preprocessing-structured-data"><span class="std std-ref">Preprocessing Structured Data</span></a>.</p></li>
<li><p>For PyTorch tensors, see <a class="reference internal" href="../../data/working-with-pytorch.html#transform-pytorch"><span class="std std-ref">Transformations with torch tensors</span></a>.</p></li>
<li><p>For optimizing expensive preprocessing operations, see <a class="reference internal" href="#dataset-cache-performance"><span class="std std-ref">Caching the preprocessed dataset</span></a>.</p></li>
</ul>
</section>
<section id="inputting-and-splitting-data">
<span id="train-datasets-input"></span><h3>Inputting and splitting data<a class="headerlink" href="#inputting-and-splitting-data" title="Permalink to this headline">#</a></h3>
<p>Your preprocessed datasets can be passed into a Ray Train Trainer (e.g. <a class="reference internal" href="../api/doc/ray.train.torch.TorchTrainer.html#ray.train.torch.TorchTrainer" title="ray.train.torch.TorchTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TorchTrainer</span></code></a>) through the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> argument.</p>
<p>The datasets passed into the Trainer’s <code class="docutils literal notranslate"><span class="pre">datasets</span></code> can be accessed inside of the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> run on each distributed training worker by calling <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a>.</p>
<p>All datasets are split (i.e. sharded) across the training workers by default. <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_dataset_shard()</span></code></a> will return <code class="docutils literal notranslate"><span class="pre">1/n</span></code> of the dataset, where <code class="docutils literal notranslate"><span class="pre">n</span></code> is the number of training workers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please be aware that as the evaluation dataset is split, users have to aggregate the evaluation results across workers.
You might consider using <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a> (<a class="reference internal" href="../examples/deepspeed/deepspeed_example.html#deepspeed-example"><span class="std std-ref">example</span></a>) or
utilities available in other frameworks that you can explore.</p>
</div>
<p>This behavior can be overwritten by passing in the <code class="docutils literal notranslate"><span class="pre">dataset_config</span></code> argument. For more information on configuring splitting logic, see <a class="reference internal" href="#train-datasets-split"><span class="std std-ref">Splitting datasets</span></a>.</p>
</section>
<section id="consuming-data">
<span id="train-datasets-consume"></span><h3>Consuming data<a class="headerlink" href="#consuming-data" title="Permalink to this headline">#</a></h3>
<p>Inside the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>, each worker can access its shard of the dataset via <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a>.</p>
<p>This data can be consumed in a variety of ways:</p>
<ul class="simple">
<li><p>To create a generic Iterable of batches, you can call <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches()</span></code></a>.</p></li>
<li><p>To create a replacement for a PyTorch DataLoader, you can call <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches()</span></code></a>.</p></li>
</ul>
<p>For more details on how to iterate over your data, see <a class="reference internal" href="../../data/iterating-over-data.html#iterating-over-data"><span class="std std-ref">Iterating over data</span></a>.</p>
</section>
</section>
<section id="starting-with-pytorch-data">
<span id="train-datasets-pytorch"></span><h2>Starting with PyTorch data<a class="headerlink" href="#starting-with-pytorch-data" title="Permalink to this headline">#</a></h2>
<p>Some frameworks provide their own dataset and data loading utilities. For example:</p>
<ul class="simple">
<li><p><strong>PyTorch:</strong> <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Dataset &amp; DataLoader</a></p></li>
<li><p><strong>Hugging Face:</strong> <a class="reference external" href="https://huggingface.co/docs/datasets/index">Dataset</a></p></li>
<li><p><strong>PyTorch Lightning:</strong> <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/data/datamodule.html">LightningDataModule</a></p></li>
</ul>
<p>These utilities can still be used directly with Ray Train. In particular, you may want to do this if you already have your data ingestion pipeline set up.
However, for more performant large-scale data ingestion we do recommend migrating to Ray Data.</p>
<p>At a high level, you can compare these concepts as follows:</p>
<table class="table">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch API</p></th>
<th class="head"><p>HuggingFace API</p></th>
<th class="head"><p>Ray Data API</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">torch.utils.data.Dataset</a></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset">datasets.Dataset</a></p></td>
<td><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.html#ray.data.Dataset" title="ray.data.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">ray.data.Dataset</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a></p></td>
<td><p>n/a</p></td>
<td><p><a class="reference internal" href="../../data/api/doc/ray.data.Dataset.iter_torch_batches.html#ray.data.Dataset.iter_torch_batches" title="ray.data.Dataset.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.Dataset.iter_torch_batches()</span></code></a></p></td>
</tr>
</tbody>
</table>
<p>For more details, see the following sections for each framework.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">PyTorch Dataset and DataLoader</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">LightningDataModule</button><button aria-controls="panel-1-1-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-2" name="1-2" role="tab" tabindex="-1">Hugging Face Dataset</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><p><strong>Option 1 (with Ray Data):</strong> Convert your PyTorch Dataset to a Ray Dataset and pass it into the Trainer via  <code class="docutils literal notranslate"><span class="pre">datasets</span></code> argument.
Inside your <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>, you can access the dataset via <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a>.
You can convert this to replace the PyTorch DataLoader via <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.data.DataIterator.iter_torch_batches()</span></code></a>.</p>
<p>For more details, see the <a class="reference internal" href="../../data/working-with-pytorch.html#migrate-pytorch"><span class="std std-ref">Migrating from PyTorch Datasets and DataLoaders</span></a>.</p>
<p><strong>Option 2 (without Ray Data):</strong> Instantiate the Torch Dataset and DataLoader directly in the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>.
You can use the <a class="reference internal" href="../api/doc/ray.train.torch.prepare_data_loader.html#ray.train.torch.prepare_data_loader" title="ray.train.torch.prepare_data_loader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.torch.prepare_data_loader()</span></code></a> utility to set up the DataLoader for distributed training.</p>
</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><p>The <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> is created with PyTorch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>s. You can apply the same logic here.</p>
</div><div aria-labelledby="tab-1-1-2" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-2" name="1-2" role="tabpanel" tabindex="0"><p><strong>Option 1 (with Ray Data):</strong> Convert your Hugging Face Dataset to a Ray Dataset and pass it into the Trainer via the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> argument.
Inside your <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>, you can access the dataset via <a class="reference internal" href="../api/doc/ray.train.get_dataset_shard.html#ray.train.get_dataset_shard" title="ray.train.get_dataset_shard"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ray.train.get_dataset_shard()</span></code></a>.</p>
<p>For instructions, see <a class="reference internal" href="../../data/loading-data.html#loading-datasets-from-ml-libraries"><span class="std std-ref">Ray Data for Hugging Face</span></a>.</p>
<p><strong>Option 2 (without Ray Data):</strong> Instantiate the Hugging Face Dataset directly in the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>.</p>
</div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When using Torch or Hugging Face Datasets directly without Ray Data, make sure to instantiate your Dataset <em>inside</em> the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code>.
Instatiating the Dataset outside of the <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> and passing it in via global scope
can cause errors due to the Dataset not being serializable.</p>
</div>
</div>
</section>
<section id="splitting-datasets">
<span id="train-datasets-split"></span><h2>Splitting datasets<a class="headerlink" href="#splitting-datasets" title="Permalink to this headline">#</a></h2>
<p>By default, Ray Train splits all datasets across workers using <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.streaming_split.html#ray.data.Dataset.streaming_split" title="ray.data.Dataset.streaming_split"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.streaming_split</span></code></a>. Each worker sees a disjoint subset of the data, instead of iterating over the entire dataset. Unless randomly shuffled, the same splits are used for each iteration of the dataset.</p>
<p>If want to customize which datasets are split, pass in a <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> to the Trainer constructor.</p>
<p>For example, to split only the training dataset, do the following:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>
<span class="n">train_ds</span><span class="p">,</span> <span class="n">val_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get the sharded training dataset</span>
    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Get the unsharded full validation dataset</span>
    <span class="n">val_ds</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some evaluation on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_ds</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">:</span> <span class="n">val_ds</span><span class="p">},</span>
    <span class="n">dataset_config</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">DataConfig</span><span class="p">(</span>
        <span class="n">datasets_to_split</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<section id="full-customization-advanced">
<h3>Full customization (advanced)<a class="headerlink" href="#full-customization-advanced" title="Permalink to this headline">#</a></h3>
<p>For use cases not covered by the default config class, you can also fully customize exactly how your input datasets are split. Define a custom <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> class (DeveloperAPI). The <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> class is responsible for that shared setup and splitting of data across nodes.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note that this example class is doing the same thing as the basic DataConfig</span>
<span class="c1"># implementation included with Ray Train.</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">DataConfig</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataIterator</span><span class="p">,</span> <span class="n">NodeIdStr</span>
<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyCustomDataConfig</span><span class="p">(</span><span class="n">DataConfig</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">configure</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">datasets</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">],</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">worker_handles</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ActorHandle</span><span class="p">]],</span>
        <span class="n">worker_node_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">NodeIdStr</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataIterator</span><span class="p">]]:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;This example only handles the simple case&quot;</span>

        <span class="c1"># Configure Ray Data for ingest.</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">DataConfig</span><span class="o">.</span><span class="n">default_ingest_options</span><span class="p">()</span>

        <span class="c1"># Split the stream into shards.</span>
        <span class="n">iterator_shards</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">streaming_split</span><span class="p">(</span>
            <span class="n">world_size</span><span class="p">,</span> <span class="n">equal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">locality_hints</span><span class="o">=</span><span class="n">worker_node_ids</span>
        <span class="p">)</span>

        <span class="c1"># Return the assigned iterators for each worker.</span>
        <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">it</span><span class="p">}</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">iterator_shards</span><span class="p">]</span>


<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
    <span class="n">dataset_config</span><span class="o">=</span><span class="n">MyCustomDataConfig</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>The subclass must be serializable, since Ray Train copies it from the driver script to the driving actor of the Trainer. Ray Train calls its <a class="reference internal" href="../api/doc/ray.train.DataConfig.configure.html#ray.train.DataConfig.configure" title="ray.train.DataConfig.configure"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure</span></code></a> method on the main actor of the Trainer group to create the data iterators for each worker.</p>
<p>In general, you can use <a class="reference internal" href="../api/doc/ray.train.DataConfig.html#ray.train.DataConfig" title="ray.train.DataConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataConfig</span></code></a> for any shared setup that has to occur ahead of time before the workers start iterating over data. The setup runs at the start of each Trainer run.</p>
</section>
</section>
<section id="random-shuffling">
<h2>Random shuffling<a class="headerlink" href="#random-shuffling" title="Permalink to this headline">#</a></h2>
<p>Randomly shuffling data for each epoch can be important for model quality depending on what model you are training.</p>
<p>Ray Data has two approaches to random shuffling:</p>
<ol class="arabic simple">
<li><p>Shuffling data blocks and local shuffling on each training worker. This requires less communication at the cost of less randomness (i.e. rows that appear in the same data block are more likely to appear near each other in the iteration order).</p></li>
<li><p>Full global shuffle, which is more expensive. This will fully decorrelate row iteration order from the original dataset order, at the cost of significantly more computation, I/O, and communication.</p></li>
</ol>
<p>For most cases, option 1 suffices.</p>
<p>First, randomize each <a class="reference internal" href="../../data/data-internals.html#dataset-concept"><span class="std std-ref">block</span></a> of your dataset via <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.randomize_block_order.html#ray.data.Dataset.randomize_block_order" title="ray.data.Dataset.randomize_block_order"><code class="xref py py-meth docutils literal notranslate"><span class="pre">randomize_block_order</span></code></a>. Then, when iterating over your dataset during training, enable local shuffling by specifying a <code class="docutils literal notranslate"><span class="pre">local_shuffle_buffer_size</span></code> to <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches</span></code></a> or <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches</span></code></a>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="c1"># Randomize the blocks of this dataset.</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">randomize_block_order</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Use a shuffle buffer size of 10k rows.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span>
            <span class="n">local_shuffle_buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>If your model is sensitive to shuffle quality, call <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle" title="ray.data.Dataset.random_shuffle"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.random_shuffle</span></code></a> to perform a global shuffle.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="c1"># Do a global shuffle of all rows in this dataset.</span>
<span class="c1"># The dataset will be shuffled on each iteration, unless `.materialize()`</span>
<span class="c1"># is called after the `.random_shuffle()`</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">random_shuffle</span><span class="p">()</span>
</pre></div>
</div>
<p>For more information on how to optimize shuffling, and which approach to choose, see the <a class="reference internal" href="../../data/performance-tips.html#optimizing-shuffles"><span class="std std-ref">Optimize shuffling guide</span></a>.</p>
</section>
<section id="enabling-reproducibility">
<h2>Enabling reproducibility<a class="headerlink" href="#enabling-reproducibility" title="Permalink to this headline">#</a></h2>
<p>When developing or hyperparameter tuning models, reproducibility is important during data ingest so that data ingest does not affect model quality. Follow these three steps to enable reproducibility:</p>
<p><strong>Step 1:</strong> Enable deterministic execution in Ray Datasets by setting the <code class="xref py py-obj docutils literal notranslate"><span class="pre">preserve_order</span></code> flag in the <a class="reference internal" href="../../data/api/doc/ray.data.DataContext.html#ray.data.DataContext" title="ray.data.context.DataContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataContext</span></code></a>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>

<span class="c1"># Preserve ordering in Ray Datasets for reproducibility.</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataContext</span><span class="o">.</span><span class="n">get_current</span><span class="p">()</span>
<span class="n">ctx</span><span class="o">.</span><span class="n">execution_options</span><span class="o">.</span><span class="n">preserve_order</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 2:</strong> Set a seed for any shuffling operations:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code> argument to <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.random_shuffle.html#ray.data.Dataset.random_shuffle" title="ray.data.Dataset.random_shuffle"><code class="xref py py-meth docutils literal notranslate"><span class="pre">random_shuffle</span></code></a></p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code> argument to <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.randomize_block_order.html#ray.data.Dataset.randomize_block_order" title="ray.data.Dataset.randomize_block_order"><code class="xref py py-meth docutils literal notranslate"><span class="pre">randomize_block_order</span></code></a></p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_shuffle_seed</span></code> argument to <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches</span></code></a></p></li>
</ul>
<p><strong>Step 3:</strong> Follow the best practices for enabling reproducibility for your training framework of choice. For example, see the <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">Pytorch reproducibility guide</a>.</p>
</section>
<section id="preprocessing-structured-data">
<span id="id1"></span><h2>Preprocessing structured data<a class="headerlink" href="#preprocessing-structured-data" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section is for tabular/structured data. The recommended way for preprocessing unstructured data is to use
Ray Data operations such as <code class="xref py py-obj docutils literal notranslate"><span class="pre">map_batches</span></code>. See the <a class="reference internal" href="../../data/working-with-pytorch.html#working-with-pytorch"><span class="std std-ref">Ray Data Working with Pytorch guide</span></a> for more details.</p>
</div>
<p>For tabular data, we recommend using Ray Data <a class="reference internal" href="../../data/preprocessors.html#air-preprocessors"><span class="std std-ref">preprocessors</span></a>, which implement common data preprocessing operations.
You can use this with Ray Train Trainers by applying them on the dataset before passing the dataset into a Trainer. For example:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>
<span class="kn">from</span> <span class="nn">ray.data.preprocessors</span> <span class="kn">import</span> <span class="n">Concatenator</span><span class="p">,</span> <span class="n">StandardScaler</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@air-example-data/breast_cancer.csv&quot;</span><span class="p">)</span>

<span class="c1"># Create preprocessors to scale some columns and concatenate the results.</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mean radius&quot;</span><span class="p">,</span> <span class="s2">&quot;mean texture&quot;</span><span class="p">])</span>
<span class="n">concatenator</span> <span class="o">=</span> <span class="n">Concatenator</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Compute dataset statistics and get transformed datasets. Note that the</span>
<span class="c1"># fit call is executed immediately, but the transformation is lazy.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">concatenator</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">get_metadata</span><span class="p">())</span>  <span class="c1"># prints {&quot;preprocessor_pkl&quot;: ...}</span>

    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Prefetch 10 batches at a time.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Save a checkpoint.</span>
    <span class="k">with</span> <span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">},</span>
            <span class="n">checkpoint</span><span class="o">=</span><span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">),</span>
        <span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">dataset</span><span class="p">},</span>
    <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;preprocessor_pkl&quot;</span><span class="p">:</span> <span class="n">scaler</span><span class="o">.</span><span class="n">serialize</span><span class="p">()},</span>
<span class="p">)</span>

<span class="c1"># Get the fitted preprocessor back from the result metadata.</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">get_metadata</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">StandardScaler</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;preprocessor_pkl&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p>In this example, we persist the fitted preprocessor using the <code class="docutils literal notranslate"><span class="pre">Trainer(metadata={...})</span></code> constructor argument. This arg specifies a dict that will available from <code class="docutils literal notranslate"><span class="pre">TrainContext.get_metadata()</span></code> and <code class="docutils literal notranslate"><span class="pre">checkpoint.get_metadata()</span></code> for checkpoints saved from the Trainer. This enables recreation of the fitted preprocessor for use for inference.</p>
</section>
<section id="performance-tips">
<h2>Performance tips<a class="headerlink" href="#performance-tips" title="Permalink to this headline">#</a></h2>
<section id="prefetching-batches">
<h3>Prefetching batches<a class="headerlink" href="#prefetching-batches" title="Permalink to this headline">#</a></h3>
<p>While iterating over your dataset for training, you can increase <code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> in <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_batches.html#ray.data.DataIterator.iter_batches" title="ray.data.DataIterator.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_batches</span></code></a> or <a class="reference internal" href="../../data/api/doc/ray.data.DataIterator.iter_torch_batches.html#ray.data.DataIterator.iter_torch_batches" title="ray.data.DataIterator.iter_torch_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">iter_torch_batches</span></code></a> to further increase performance. While training on the current batch, this launches N background threads to fetch and process the next N batches.</p>
<p>This approach can help if training is bottlenecked on cross-node data transfer or on last-mile preprocessing such as converting batches to tensors or executing <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code>. However, increasing <code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> leads to more data that needs to be held in heap memory. By default, <code class="docutils literal notranslate"><span class="pre">prefetch_batches</span></code> is set to 1.</p>
<p>For example, the following code prefetches 10 batches at a time for each training worker:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.torch</span> <span class="kn">import</span> <span class="n">TorchTrainer</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span>
    <span class="s2">&quot;s3://anonymous@ray-example-data/sms_spam_collection_subset.txt&quot;</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">train_loop_per_worker</span><span class="p">():</span>
    <span class="c1"># Get an iterator to the dataset we passed in below.</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="c1"># Prefetch 10 batches at a time.</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">it</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">prefetch_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Do some training on batch&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

<span class="n">my_trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">ds</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">my_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="caching-the-preprocessed-dataset">
<span id="dataset-cache-performance"></span><h3>Caching the preprocessed dataset<a class="headerlink" href="#caching-the-preprocessed-dataset" title="Permalink to this headline">#</a></h3>
<p>If you’re training on GPUs and have an expensive CPU preprocessing operation, this approach may bottleneck training throughput.</p>
<p>If your preprocessed Dataset is small enough to fit in Ray object store memory (by default this is 30% of total cluster RAM), <em>materialize</em> the preprocessed dataset in Ray’s built-in object store, by calling <a class="reference internal" href="../../data/api/doc/ray.data.Dataset.materialize.html#ray.data.Dataset.materialize" title="ray.data.Dataset.materialize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">materialize()</span></code></a> on the preprocessed dataset. This method tells Ray Data to compute the entire preprocessed and pin it in the Ray object store memory. As a result, when iterating over the dataset repeatedly, the preprocessing operations do not need to be re-run. However, if the preprocessed data is too large to fit into Ray object store memory, this approach will greatly decreases performance as data needs to be spilled to and read back from disk.</p>
<p>Transformations that you want run per-epoch, such as randomization, should go after the materialize call.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="c1"># Load the data.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">&quot;s3://anonymous@ray-example-data/iris.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Define a preprocessing function.</span>
<span class="k">def</span> <span class="nf">normalize_length</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">new_col</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">])</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;normalized.sepal.length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_col</span>
    <span class="k">del</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sepal.length&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="c1"># Preprocess the data. Transformations that are made before the materialize call</span>
<span class="c1"># below are only run once.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">normalize_length</span><span class="p">)</span>

<span class="c1"># Materialize the dataset in object store memory.</span>
<span class="c1"># Only do this if train_ds is small enough to fit in object store memory.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">materialize</span><span class="p">()</span>

<span class="c1"># Dummy augmentation transform.</span>
<span class="k">def</span> <span class="nf">augment_data</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">batch</span>

<span class="c1"># Add per-epoch preprocessing. Transformations that you want to run per-epoch, such</span>
<span class="c1"># as data augmentation or randomization, should go after the materialize call.</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="n">augment_data</span><span class="p">)</span>

<span class="c1"># Pass train_ds to the Trainer</span>
</pre></div>
</div>
</section>
<section id="adding-cpu-only-nodes-to-your-cluster">
<h3>Adding CPU-only nodes to your cluster<a class="headerlink" href="#adding-cpu-only-nodes-to-your-cluster" title="Permalink to this headline">#</a></h3>
<p>If you are bottlenecked on expensive CPU preprocessing and the preprocessed Dataset is too large to fit in object store memory, then materializing the dataset doesn’t work. In this case, since Ray supports heterogeneous clusters, you can add more CPU-only nodes to your cluster.</p>
<p>For cases where you’re bottlenecked by object store memory, adding more CPU-only nodes to your cluster increases total cluster object store memory, allowing more data to be buffered in between preprocessing and training stages.</p>
<p>For cases where you’re bottlenecked by preprocessing compute time, adding more CPU-only nodes adds more CPU cores to your cluster, further parallelizing preprocessing. If your preprocessing is still not fast enough to saturate GPUs, then add enough CPU-only nodes to <a class="reference internal" href="#dataset-cache-performance"><span class="std std-ref">cache the preprocessed dataset</span></a>.</p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../user-guides.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ray Train 用户指南</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="using-gpus.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configuring Scale and GPUs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>