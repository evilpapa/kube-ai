
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Fine-tune of Stable Diffusion with DreamBooth and Ray Train &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/versionwarning.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../../../_static/js/docsearch.js"></script>
    <script defer="defer" src="../../../_static/js/csat.js"></script>
    <script defer="defer" src="../../../_static/js/termynal.js"></script>
    <script defer="defer" src="../../../_static/js/custom.js"></script>
    <script defer="defer" src="../../../_static/js/top-navigation.js"></script>
    <script src="../../../_static/js/tags.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/examples/pytorch/dreambooth_finetuning.html" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/getting-started.html">
   入门
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../data/data.html">
   Ray 数据「75%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../train.html">
   Ray 训练「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/examples/pytorch/dreambooth_finetuning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/examples/pytorch/dreambooth_finetuning.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/train/examples/pytorch/dreambooth_finetuning.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-it-works">
   How it works
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading">
     Data loading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-training">
     Distributed training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configure-the-scale">
     Configure the scale
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-throughput">
       Training throughput
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run-the-example">
   Run the example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-0-preparation">
     Step 0: Preparation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-download-the-pre-trained-model">
     Step 1: Download the pre-trained model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-supply-images-of-your-subject">
     Step 2: Supply images of your subject
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-create-the-regularization-images">
     Step 3: Create the regularization images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-fine-tune-the-model">
     Step 4: Fine-tune the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-5-generate-images-of-the-subject">
     Step 5: Generate images of the subject
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   See also
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fine-tune of Stable Diffusion with DreamBooth and Ray Train</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-it-works">
   How it works
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-loading">
     Data loading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-training">
     Distributed training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configure-the-scale">
     Configure the scale
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-throughput">
       Training throughput
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run-the-example">
   Run the example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-0-preparation">
     Step 0: Preparation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-download-the-pre-trained-model">
     Step 1: Download the pre-trained model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-supply-images-of-your-subject">
     Step 2: Supply images of your subject
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-create-the-regularization-images">
     Step 3: Create the regularization images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-fine-tune-the-model">
     Step 4: Fine-tune the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-5-generate-images-of-the-subject">
     Step 5: Generate images of the subject
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   See also
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="fine-tune-of-stable-diffusion-with-dreambooth-and-ray-train">
<span id="torch-finetune-dreambooth-ex"></span><h1>Fine-tune of Stable Diffusion with DreamBooth and Ray Train<a class="headerlink" href="#fine-tune-of-stable-diffusion-with-dreambooth-and-ray-train" title="Permalink to this headline">#</a></h1>
<p>This is an intermediate example that shows how to do DreamBooth fine-tuning of a Stable Diffusion model using Ray Train.
It demonstrates how to use <a class="reference internal" href="../../../data/data.html#data"><span class="std std-ref">Ray Data</span></a> with PyTorch Lightning in Ray Train.</p>
<p>See the original <a class="reference external" href="https://dreambooth.github.io/">DreamBooth project homepage</a> for more details on what this fine-tuning method achieves.</p>
<a class="reference external image-reference" href="https://dreambooth.github.io"><img alt="DreamBooth fine-tuning overview" src="https://dreambooth.github.io/DreamBooth_files/high_level.png" /></a>
<p>This example builds on <a class="reference external" href="https://huggingface.co/docs/diffusers/training/dreambooth">this Hugging Face 🤗 tutorial</a>.
See the Hugging Face tutorial for useful explanations and suggestions on hyperparameters.
<strong>Adapting this example to Ray Train allows you to easily scale up the fine-tuning to an arbitrary number of distributed training workers.</strong></p>
<p><strong>Compute requirements:</strong></p>
<ul class="simple">
<li><p>Because of the large model sizes, you need a machine with at least 1 A10G GPU.</p></li>
<li><p>Each training worker uses 1 GPU. You can use multiple GPUs or workers to leverage data-parallel training to speed up training time.</p></li>
</ul>
<p>This example fine-tunes both the <code class="docutils literal notranslate"><span class="pre">text_encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">unet</span></code> models used in the stable diffusion process, with respect to a prior preserving loss.</p>
<img alt="DreamBooth overview" src="../../../_images/dreambooth_example.png" />
<p>Find the full code repository at <a class="reference external" href="https://github.com/ray-project/ray/tree/master/doc/source/templates/05_dreambooth_finetuning">https://github.com/ray-project/ray/tree/master/doc/source/templates/05_dreambooth_finetuning</a></p>
<section id="how-it-works">
<h2>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">#</a></h2>
<p>This example uses Ray Data for data loading and Ray Train for distributed training.</p>
<section id="data-loading">
<h3>Data loading<a class="headerlink" href="#data-loading" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Find the latest version of the code at <a class="reference external" href="https://github.com/ray-project/ray/tree/master/doc/source/templates/05_dreambooth_finetuning/dreambooth/dataset.py">dataset.py</a></p>
<p>The latest version might differ slightly from the code presented here.</p>
</div>
<p>Use Ray Data for data loading. The code has three interesting parts.</p>
<p>First, load two datasets using <a class="reference internal" href="../../../data/api/doc/ray.data.read_images.html#ray.data.read_images" title="ray.data.read_images"><code class="xref py py-func docutils literal notranslate"><span class="pre">ray.data.read_images()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">instance_dataset</span> <span class="o">=</span> <span class="n">read_images</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">instance_images_dir</span><span class="p">)</span>
<span class="n">class_dataset</span> <span class="o">=</span> <span class="n">read_images</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">class_images_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, tokenize the prompt that generated these images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">model_dir</span><span class="p">,</span>
    <span class="n">subfolder</span><span class="o">=</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Get the token ids for both prompts.</span>
<span class="n">class_prompt_ids</span> <span class="o">=</span> <span class="n">_tokenize</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">class_prompt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">instance_prompt_ids</span> <span class="o">=</span> <span class="n">_tokenize</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">instance_prompt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>And lastly, apply a <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> preprocessing pipeline to the images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span>
            <span class="n">image_resolution</span><span class="p">,</span>
            <span class="n">interpolation</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
            <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="n">image_resolution</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">transform_image</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">output_column_name</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">transformed_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]]</span>
    <span class="n">batch</span><span class="p">[</span><span class="n">output_column_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">transformed_tensors</span>
    <span class="k">return</span> <span class="n">batch</span>

</pre></div>
</div>
<p>Apply all three parts in a final step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For each dataset:</span>
<span class="c1"># - perform image preprocessing</span>
<span class="c1"># - drop the original image column</span>
<span class="c1"># - add a new column with the tokenized prompts</span>
<span class="n">instance_dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">instance_dataset</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span>
        <span class="n">transform_image</span><span class="p">,</span> <span class="n">fn_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;output_column_name&quot;</span><span class="p">:</span> <span class="s2">&quot;instance_image&quot;</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">drop_columns</span><span class="p">([</span><span class="s2">&quot;image&quot;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">add_column</span><span class="p">(</span><span class="s2">&quot;instance_prompt_ids&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">df</span><span class="p">:</span> <span class="p">[</span><span class="n">instance_prompt_ids</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="distributed-training">
<h3>Distributed training<a class="headerlink" href="#distributed-training" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Find the latest version of the code at <a class="reference external" href="https://github.com/ray-project/ray/tree/master/doc/source/templates/05_dreambooth_finetuning/dreambooth/train.py">train.py</a></p>
<p>The latest version might differ slightly from the code presented here.</p>
</div>
<p>The central part of the training code is the <a class="reference internal" href="../../overview.html#train-overview-training-function"><span class="std std-ref">training function</span></a>. This function accepts a configuration dict that contains the hyperparameters. It then defines a regular PyTorch training loop.</p>
<p>You interact with the Ray Train API in only a few locations, which follow in-line comments in the snippet below.</p>
<p>Remember that you want to do data-parallel training for all the models.</p>
<ol class="arabic simple">
<li><p>Load the data shard for each worker with <code class="xref py py-obj docutils literal notranslate"><span class="pre">session.get_dataset_shard(&quot;train&quot;)`</span></code></p></li>
<li><p>Iterate over the dataset with <code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataset.iter_torch_batches()`</span></code></p></li>
<li><p>Report results to Ray Train with <code class="xref py py-obj docutils literal notranslate"><span class="pre">session.report(results)`</span></code></p></li>
</ol>
<p>The code is compacted for brevity. The <a class="reference external" href="https://github.com/ray-project/ray/tree/master/doc/source/templates/05_dreambooth_finetuning/dreambooth/train.py">full code</a> is more thoroughly annotated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_fn</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>

    <span class="c1"># Load pre-trained models.</span>
    <span class="p">(</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">noise_scheduler</span><span class="p">,</span>
        <span class="n">vae</span><span class="p">,</span>
        <span class="n">unet</span><span class="p">,</span>
        <span class="n">unet_trainable_parameters</span><span class="p">,</span>
        <span class="n">text_trainable_parameters</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">load_models</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">)</span>
    <span class="n">unet</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">unet</span><span class="p">)</span>
    <span class="c1"># manually move to device as `prepare_model` can&#39;t be used on</span>
    <span class="c1"># non-training models.</span>
    <span class="n">vae</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

    <span class="c1"># Use the regular AdamW optimizer to work with bfloat16 weights.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
        <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">unet_trainable_parameters</span><span class="p">,</span> <span class="n">text_trainable_parameters</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="c1"># Train!</span>
    <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">num_train_epochs</span><span class="si">}</span><span class="s2"> epochs.&quot;</span><span class="p">)</span>

    <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train_epochs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_train_steps&quot;</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stopping training after reaching </span><span class="si">{</span><span class="n">global_step</span><span class="si">}</span><span class="s2"> steps...&quot;</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="o">.</span><span class="n">iter_torch_batches</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">],</span>
                <span class="n">device</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">collate</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Convert images to latent space</span>
            <span class="n">latents</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;images&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.18215</span>

            <span class="c1"># Sample noise that we&#39;ll add to the latents</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">latents</span><span class="p">)</span>
            <span class="n">bsz</span> <span class="o">=</span> <span class="n">latents</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Sample a random timestep for each image</span>
            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span>
                <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_timesteps</span><span class="p">,</span>
                <span class="p">(</span><span class="n">bsz</span><span class="p">,),</span>
                <span class="n">device</span><span class="o">=</span><span class="n">latents</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

            <span class="c1"># Add noise to the latents according to the noise magnitude at each timestep</span>
            <span class="c1"># (this is the forward diffusion process)</span>
            <span class="n">noisy_latents</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

            <span class="c1"># Get the text embedding for conditioning</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;prompt_ids&quot;</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Predict the noise residual.</span>
            <span class="n">model_pred</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span>
                <span class="n">noisy_latents</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">()),</span>
                <span class="n">timesteps</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">()),</span>
                <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">()),</span>
            <span class="p">)</span><span class="o">.</span><span class="n">sample</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">get_target</span><span class="p">(</span><span class="n">noise_scheduler</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">latents</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">prior_preserving_loss</span><span class="p">(</span>
                <span class="n">model_pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;prior_loss_weight&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Gradient clipping before optimizer stepping.</span>
            <span class="n">clip_grad_norm_</span><span class="p">(</span>
                <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">unet_trainable_parameters</span><span class="p">,</span> <span class="n">text_trainable_parameters</span><span class="p">),</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_grad_norm&quot;</span><span class="p">],</span>
            <span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Step all optimizers.</span>

            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">global_step</span><span class="p">,</span>
                <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="p">}</span>
            <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_train_steps&quot;</span><span class="p">]:</span>
                <span class="k">break</span>
</pre></div>
</div>
<p>You can then run this training function with Ray Train’s TorchTrainer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="n">train_arguments</span><span class="p">()</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># Build training dataset.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">get_train_dataset</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded training dataset (size: </span><span class="si">{</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Train with Ray Train TorchTrainer.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TorchTrainer</span><span class="p">(</span>
    <span class="n">train_fn</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span>
        <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="configure-the-scale">
<h3>Configure the scale<a class="headerlink" href="#configure-the-scale" title="Permalink to this headline">#</a></h3>
<p>In the TorchTrainer, you can easily configure the scale.
The preceding example uses the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> argument to specify the number
of workers. This argument defaults to 2 workers with 1 GPU each, totalling to 2 GPUs.</p>
<p>To run the example on 4 GPUs, set the number of workers to 4 using <code class="docutils literal notranslate"><span class="pre">--num-workers=4</span></code>.
Or you can change the scaling config directly:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>scaling_config=ScalingConfig(<span class="w"></span>
<span class="w"> </span>    use_gpu=True,<span class="w"></span>
<span class="gd">-    num_workers=args.num_workers,</span><span class="w"></span>
<span class="gi">+    num_workers=4,</span><span class="w"></span>
<span class="w"> </span>)<span class="w"></span>
</pre></div>
</div>
<p>If you’re running multi-node training, make sure that all nodes have access to a shared
storage like NFS or EFS. In the following example script, you can adjust the location with the
<code class="docutils literal notranslate"><span class="pre">DATA_PREFIX</span></code> environment variable.</p>
<section id="training-throughput">
<h4>Training throughput<a class="headerlink" href="#training-throughput" title="Permalink to this headline">#</a></h4>
<p>Compare throughput of the preceding training runs that used 1,  2, and 4 workers or GPUs.</p>
<p>Consider the following setup:</p>
<ul class="simple">
<li><p>1 GCE g2-standard-48-nvidia-l4-4 instance with 4 GPUs</p></li>
<li><p>Model as configured below</p></li>
<li><p>Data from this example</p></li>
<li><p>200 regularization images</p></li>
<li><p>Training for 4 epochs (local batch size = 2)</p></li>
<li><p>3 runs per configuration</p></li>
</ul>
<p>You expect that the training time should benefit from scale and decreases when running with
more workers and GPUs.</p>
<img alt="DreamBooth training times" src="../../../_images/dreambooth_training.png" />
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Number of workers/GPUs</p></th>
<th class="head"><p>Training time (seconds)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>802.14</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>487.82</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>313.25</p></td>
</tr>
</tbody>
</table>
<p>While the training time decreases linearly with the amount of workers/GPUs, you can observe some penalty.
Specifically, with double the amount of workers you don’t get half of the training time.</p>
<p>This penalty is most likely due to additional communication between processes and the transfer of large model
weights. You are also only training with a batch size of one because of the GPU memory limitation. On larger
GPUs with higher batch sizes you would expect a greater benefit from scaling out.</p>
</section>
</section>
</section>
<section id="run-the-example">
<h2>Run the example<a class="headerlink" href="#run-the-example" title="Permalink to this headline">#</a></h2>
<p>First, download the pre-trained Stable Diffusion model as a starting point.</p>
<p>Then train this model with a few images of a subject.</p>
<p>To achieve this, choose a non-word as an identifier, such as <code class="docutils literal notranslate"><span class="pre">unqtkn</span></code>. When fine-tuning the model with this subject, you teach the model that the prompt is <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">photo</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">unqtkn</span> <span class="pre">&lt;class&gt;</span></code>.</p>
<p>After fine-tuning you can run inference with this specific prompt.
For instance: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">photo</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">unqtkn</span> <span class="pre">&lt;class&gt;</span></code> creates an image of the subject.
Similarly, <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">photo</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">unqtkn</span> <span class="pre">&lt;class&gt;</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">beach</span></code> creates an image of the subject at the beach.</p>
<section id="step-0-preparation">
<h3>Step 0: Preparation<a class="headerlink" href="#step-0-preparation" title="Permalink to this headline">#</a></h3>
<p>Clone the Ray repository, go to the example directory, and install dependencies.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/ray-project/ray.git
<span class="nb">cd</span> doc/source/templates/05_dreambooth_finetuning
pip install -Ur dreambooth/requirements.txt
</pre></div>
</div>
<p>Prepare some directories and environment variables.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: If running on multiple nodes, change this path to a shared directory (ex: NFS)</span>
<span class="nb">export</span> <span class="nv">DATA_PREFIX</span><span class="o">=</span><span class="s2">&quot;/tmp&quot;</span>
<span class="nb">export</span> <span class="nv">ORIG_MODEL_NAME</span><span class="o">=</span><span class="s2">&quot;CompVis/stable-diffusion-v1-4&quot;</span>
<span class="nb">export</span> <span class="nv">ORIG_MODEL_HASH</span><span class="o">=</span><span class="s2">&quot;b95be7d6f134c3a9e62ee616f310733567f069ce&quot;</span>
<span class="nb">export</span> <span class="nv">ORIG_MODEL_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DATA_PREFIX</span><span class="s2">/model-orig&quot;</span>
<span class="nb">export</span> <span class="nv">ORIG_MODEL_PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$ORIG_MODEL_DIR</span><span class="s2">/models--</span><span class="si">${</span><span class="nv">ORIG_MODEL_NAME</span><span class="p">/</span><span class="se">\/</span><span class="p">/--</span><span class="si">}</span><span class="s2">/snapshots/</span><span class="nv">$ORIG_MODEL_HASH</span><span class="s2">&quot;</span>
<span class="nb">export</span> <span class="nv">TUNED_MODEL_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DATA_PREFIX</span><span class="s2">/model-tuned&quot;</span>
<span class="nb">export</span> <span class="nv">IMAGES_REG_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DATA_PREFIX</span><span class="s2">/images-reg&quot;</span>
<span class="nb">export</span> <span class="nv">IMAGES_OWN_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DATA_PREFIX</span><span class="s2">/images-own&quot;</span>
<span class="nb">export</span> <span class="nv">IMAGES_NEW_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$DATA_PREFIX</span><span class="s2">/images-new&quot;</span>
<span class="c1"># TODO: Add more worker nodes and increase NUM_WORKERS for more data-parallelism</span>
<span class="nb">export</span> <span class="nv">NUM_WORKERS</span><span class="o">=</span><span class="m">2</span>

mkdir -p <span class="nv">$ORIG_MODEL_DIR</span> <span class="nv">$TUNED_MODEL_DIR</span> <span class="nv">$IMAGES_REG_DIR</span> <span class="nv">$IMAGES_OWN_DIR</span> <span class="nv">$IMAGES_NEW_DIR</span>
</pre></div>
</div>
</section>
<section id="step-1-download-the-pre-trained-model">
<h3>Step 1: Download the pre-trained model<a class="headerlink" href="#step-1-download-the-pre-trained-model" title="Permalink to this headline">#</a></h3>
<p>Download and cache a pre-trained Stable Diffusion model locally.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python cache_model.py --model_dir<span class="o">=</span><span class="nv">$ORIG_MODEL_DIR</span> --model_name<span class="o">=</span><span class="nv">$ORIG_MODEL_NAME</span> --revision<span class="o">=</span><span class="nv">$ORIG_MODEL_HASH</span>
</pre></div>
</div>
<p>You can access the downloaded model checkpoint at the <code class="docutils literal notranslate"><span class="pre">$ORIG_MODEL_PATH</span></code>.</p>
</section>
<section id="step-2-supply-images-of-your-subject">
<h3>Step 2: Supply images of your subject<a class="headerlink" href="#step-2-supply-images-of-your-subject" title="Permalink to this headline">#</a></h3>
<p>Use one of the sample datasets, like <code class="xref py py-obj docutils literal notranslate"><span class="pre">dog</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">lego</span> <span class="pre">car</span></code>, or provide your own directory
of images, and specify the directory with the <code class="docutils literal notranslate"><span class="pre">$INSTANCE_DIR</span></code> environment variable.</p>
<p>Then, copy these images to <code class="docutils literal notranslate"><span class="pre">$IMAGES_OWN_DIR</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  <span class="c1"># Only uncomment one of the following:</span>

  <span class="c1"># Option 1: Use the dog dataset ---------</span>
  <span class="nb">export</span> <span class="nv">CLASS_NAME</span><span class="o">=</span><span class="s2">&quot;dog&quot;</span>
  python download_example_dataset.py ./images/dog
  <span class="nb">export</span> <span class="nv">INSTANCE_DIR</span><span class="o">=</span>./images/dog
  <span class="c1"># ---------------------------------------</span>

  <span class="c1"># Option 2: Use the lego car dataset ----</span>
  <span class="c1"># export CLASS_NAME=&quot;car&quot;</span>
  <span class="c1"># export INSTANCE_DIR=./images/lego-car</span>
  <span class="c1"># ---------------------------------------</span>

  <span class="c1"># Option 3: Use your own images ---------</span>
  <span class="c1"># export CLASS_NAME=&quot;&lt;class-of-your-subject&gt;&quot;</span>
  <span class="c1"># export INSTANCE_DIR=&quot;/path/to/images/of/subject&quot;</span>
  <span class="c1"># ---------------------------------------</span>

  <span class="c1"># Copy own images into IMAGES_OWN_DIR</span>
  cp -rf <span class="nv">$INSTANCE_DIR</span>/* <span class="s2">&quot;</span><span class="nv">$IMAGES_OWN_DIR</span><span class="s2">/&quot;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">$CLASS_NAME</span></code> should be the general category of your subject.
The images produced by the prompt <code class="docutils literal notranslate"><span class="pre">photo</span> <span class="pre">of</span> <span class="pre">a</span> <span class="pre">unqtkn</span> <span class="pre">&lt;class&gt;</span></code> should be diverse images
that are different enough from the subject in order for generated images to clearly
show the effect of fine-tuning.</p>
</section>
<section id="step-3-create-the-regularization-images">
<h3>Step 3: Create the regularization images<a class="headerlink" href="#step-3-create-the-regularization-images" title="Permalink to this headline">#</a></h3>
<p>Create a regularization image set for a class of subjects using the pre-trained
Stable Diffusion model. This regularization set ensures that
the model still produces decent images for random images of the same class,
rather than just optimize for producing good images of the subject.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  python generate.py <span class="se">\</span>
    --model_dir<span class="o">=</span><span class="nv">$ORIG_MODEL_PATH</span> <span class="se">\</span>
    --output_dir<span class="o">=</span><span class="nv">$IMAGES_REG_DIR</span> <span class="se">\</span>
    --prompts<span class="o">=</span><span class="s2">&quot;photo of a </span><span class="nv">$CLASS_NAME</span><span class="s2">&quot;</span> <span class="se">\</span>
    --num_samples_per_prompt<span class="o">=</span><span class="m">200</span> <span class="se">\</span>
    --use_ray_data
</pre></div>
</div>
<p>Use Ray Data to do batch inference with 4 workers, to generate more images in parallel.</p>
</section>
<section id="step-4-fine-tune-the-model">
<h3>Step 4: Fine-tune the model<a class="headerlink" href="#step-4-fine-tune-the-model" title="Permalink to this headline">#</a></h3>
<p>Save a few, like 4 to 5, images of the subject being fine-tuned
in a local directory. Then launch the training job with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  python train.py <span class="se">\</span>
    --model_dir<span class="o">=</span><span class="nv">$ORIG_MODEL_PATH</span> <span class="se">\</span>
    --output_dir<span class="o">=</span><span class="nv">$TUNED_MODEL_DIR</span> <span class="se">\</span>
    --instance_images_dir<span class="o">=</span><span class="nv">$IMAGES_OWN_DIR</span> <span class="se">\</span>
    --instance_prompt<span class="o">=</span><span class="s2">&quot;photo of </span><span class="nv">$UNIQUE_TOKEN</span><span class="s2"> </span><span class="nv">$CLASS_NAME</span><span class="s2">&quot;</span> <span class="se">\</span>
    --class_images_dir<span class="o">=</span><span class="nv">$IMAGES_REG_DIR</span> <span class="se">\</span>
    --class_prompt<span class="o">=</span><span class="s2">&quot;photo of a </span><span class="nv">$CLASS_NAME</span><span class="s2">&quot;</span> <span class="se">\</span>
    --train_batch_size<span class="o">=</span><span class="m">2</span> <span class="se">\</span>
    --lr<span class="o">=</span>5e-6 <span class="se">\</span>
    --num_epochs<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
    --max_train_steps<span class="o">=</span><span class="m">200</span> <span class="se">\</span>
    --num_workers <span class="nv">$NUM_WORKERS</span>
</pre></div>
</div>
</section>
<section id="step-5-generate-images-of-the-subject">
<h3>Step 5: Generate images of the subject<a class="headerlink" href="#step-5-generate-images-of-the-subject" title="Permalink to this headline">#</a></h3>
<p>Try your model with the same command line as Step 2, but point
to your own model this time.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>  python generate.py <span class="se">\</span>
    --model_dir<span class="o">=</span><span class="nv">$TUNED_MODEL_DIR</span> <span class="se">\</span>
    --output_dir<span class="o">=</span><span class="nv">$IMAGES_NEW_DIR</span> <span class="se">\</span>
    --prompts<span class="o">=</span><span class="s2">&quot;photo of a </span><span class="nv">$UNIQUE_TOKEN</span><span class="s2"> </span><span class="nv">$CLASS_NAME</span><span class="s2"> in a bucket&quot;</span> <span class="se">\</span>
    --num_samples_per_prompt<span class="o">=</span><span class="m">5</span>
</pre></div>
</div>
<p>Next, try replacing the prompt with something more interesting.</p>
<p>For example, for the dog subject, you can try:</p>
<ul class="simple">
<li><p>“photo of a unqtkn dog in a bucket”</p></li>
<li><p>“photo of a unqtkn dog sleeping”</p></li>
<li><p>“photo of a unqtkn dog in a doghouse”</p></li>
</ul>
</section>
</section>
<section id="see-also">
<h2>See also<a class="headerlink" href="#see-also" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../../examples.html#train-examples"><span class="std std-ref">Ray Train Examples</span></a> for more use cases</p></li>
<li><p><a class="reference internal" href="../../user-guides.html#train-user-guides"><span class="std std-ref">Ray Train User Guides</span></a> for how-to guides</p></li>
</ul>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>