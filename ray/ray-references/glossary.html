
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Ray 术语表 &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script defer="defer" src="../_static/js/csat.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/ray-references/glossary.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="开发者指引" href="../ray-contribute/index.html" />
    <link rel="prev" title="使用统计收集" href="../cluster/usage-stats.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   入门
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray 数据「75%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray 训练「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="api.html">
   参考「20%」
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../data/api/api.html">
     Ray 数据 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../train/api/api.html">
     Ray 训练 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../tune/api/api.html">
     Ray Tune API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../serve/api/index.html">
     Ray Serve API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../rllib/package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../workflows/api/api.html">
     Ray Workflows API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cluster/package-overview.html">
     Ray Cluster 管理 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ray-core/api/index.html">
     Ray Core API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cluster/usage-stats.html">
     使用统计收集
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Ray 术语表
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Fray-references/glossary.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/ray-references/glossary.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/ray-references/glossary.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Ray 术语表</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="ray">
<span id="ray-glossary"></span><h1>Ray 术语表<a class="headerlink" href="#ray" title="Permalink to this headline">#</a></h1>
<p>在此页面上，您可以找到整个 Ray 文档中使用的重要术语列表，按字母顺序排序。</p>
<dl class="glossary">
<dt id="term-Action">Action 空间<a class="headerlink" href="#term-Action" title="Permalink to this term">#</a></dt><dd><p>Property of an RL environment. The shape(s) and datatype(s) that actions within
an RL environment are allowed to have.
Examples: An RL environment, in which an agent can move up, down, left,
or right might have an action space of <code class="docutils literal notranslate"><span class="pre">Discrete(4)</span></code> (integer values
of 0, 1, 2, or 3).
An RL environment, in which an agent can apply a torque between
-1.0 and 1.0 to a joint, the action space might be
<code class="docutils literal notranslate"><span class="pre">Box(-1.0,</span> <span class="pre">1.0,</span> <span class="pre">(1,),</span> <span class="pre">float32)</span></code> (single float values between -1.0 and 1.0).</p>
</dd>
<dt id="term-Actor">Actor<a class="headerlink" href="#term-Actor" title="Permalink to this term">#</a></dt><dd><p>是一个类的远程实例，本质上是一个有状态服务。 <a class="reference internal" href="../ray-core/actors.html#actor-guide"><span class="std std-ref">了解有关 Ray actor 的更多信息</span></a>.</p>
</dd>
<dt id="term-Actor-task">Actor task<a class="headerlink" href="#term-Actor-task" title="Permalink to this term">#</a></dt><dd><p>Ray actor 方法的调用。有时我们只是称其为任务。</p>
</dd>
<dt id="term-Ray-Agent">Ray Agent<a class="headerlink" href="#term-Ray-Agent" title="Permalink to this term">#</a></dt><dd><p>每个Ray节点上运行的守护进程。它具有多种功能，
例如收集本地节点上的指标和安装运行时环境。</p>
</dd>
<dt id="term-Agent">Agent<a class="headerlink" href="#term-Agent" title="Permalink to this term">#</a></dt><dd><p>强化学习环境中的行为实体。一个强化学习环境可能包含一个（单智能体强化学习）或多个（多智能体强化学习）行为实体。
不同环境中的智能体可能具有不同的观察空间和动作空间、不同的奖励函数，并在不同的时间步骤中行动。</p>
</dd>
<dt id="term-Algorithm">Algorithm 算法<a class="headerlink" href="#term-Algorithm" title="Permalink to this term">#</a></dt><dd><p>一个类，它包含训练一个或多个 RL 代理的 who/when/where/how 信息。
用户直接与算法实例交互，以训练他们的代理（它是最顶层的用户接口或 RLlib）。</p>
</dd>
<dt id="term-Asynchronous-execution">Asynchronous execution 异步执行<a class="headerlink" href="#term-Asynchronous-execution" title="Permalink to this term">#</a></dt><dd><p>An execution model where a later task can begin executing in parallel,
without waiting for an earlier task to finish.
Ray tasks and actor tasks are all executed asynchronously.</p>
</dd>
<dt id="term-Asynchronous-sampling">Asynchronous sampling<a class="headerlink" href="#term-Asynchronous-sampling" title="Permalink to this term">#</a></dt><dd><p>Sampling is the process of rolling out (playing) episodes within an RL
environment and thereby collecting the training data (observations, actions
and rewards). In an asynchronous sampling setup, Ray actors run sampling in the
background and send collected samples back to a main driver script. The driver
checks for such “ready” data frequently and then triggers central model
learning updates. Hence, sampling and learning happen at the same time.
Note that because of this, the policy/ies used for creating the samples
(action computations) might be slightly behind the centrally learned policy
model(s), even in an on-policy Algorithm.</p>
</dd>
<dt id="term-Autoscaler">Autoscaler<a class="headerlink" href="#term-Autoscaler" title="Permalink to this term">#</a></dt><dd><p>A Ray component that scales up and down the Ray cluster by adding and removing
Ray nodes according to the resources requested by applications running on
the cluster.</p>
</dd>
<dt id="term-Autoscaling">Autoscaling<a class="headerlink" href="#term-Autoscaling" title="Permalink to this term">#</a></dt><dd><p>The process of scaling up and down the Ray cluster automatically.</p>
</dd>
<dt id="term-Backend">Backend<a class="headerlink" href="#term-Backend" title="Permalink to this term">#</a></dt><dd><p>A class containing the initialization and teardown logic for a specific deep
learning framework (eg. Torch, TensorFlow), used to set up distributed
data-parallel training for <a class="reference internal" href="../train/api/api.html#train-api"><span class="std std-ref">Ray Train’s built-in trainers</span></a>.</p>
</dd>
<dt id="term-Batch-format">Batch format<a class="headerlink" href="#term-Batch-format" title="Permalink to this term">#</a></dt><dd><p>The way Ray Data represents batches of data.</p>
<p>Set <code class="docutils literal notranslate"><span class="pre">batch_format</span></code> in methods like
<a class="reference internal" href="../data/api/doc/ray.data.Dataset.iter_batches.html#ray.data.Dataset.iter_batches" title="ray.data.Dataset.iter_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.iter_batches()</span></code></a> and
<a class="reference internal" href="../data/api/doc/ray.data.Dataset.map_batches.html#ray.data.Dataset.map_batches" title="ray.data.Dataset.map_batches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Dataset.map_batches()</span></code></a> to specify the
batch type.</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ray</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;numpy&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)))</span>
<span class="go">{&#39;id&#39;: array([0, 1, 2, 3, 4])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">iter_batches</span><span class="p">(</span><span class="n">batch_format</span><span class="o">=</span><span class="s2">&quot;pandas&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)))</span>
<span class="go">   id</span>
<span class="go">0   0</span>
<span class="go">1   1</span>
<span class="go">2   2</span>
<span class="go">3   3</span>
<span class="go">4   4</span>
</pre></div>
</div>
<p>To learn more about batch formats, read
<a class="reference internal" href="../data/transforming-data.html#configure-batch-format"><span class="std std-ref">Configuring batch formats</span></a>.</p>
</dd>
<dt id="term-Batch-size">Batch size<a class="headerlink" href="#term-Batch-size" title="Permalink to this term">#</a></dt><dd><p>A batch size in the context of model training is the number of data points used
to compute and apply one gradient update to the model weights.</p>
</dd>
<dt id="term-Block">Block<a class="headerlink" href="#term-Block" title="Permalink to this term">#</a></dt><dd><p>A processing unit of data. A <a class="reference internal" href="../data/api/doc/ray.data.Dataset.html#ray.data.Dataset" title="ray.data.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> consists of a
collection of blocks.</p>
<p>Under the hood, Ray Data partitions rows into a set of distributed data blocks.
This allows it to perform operations in parallel.</p>
<p>Unlike a batch, which is a user-facing object, a block is an internal abstraction.</p>
</dd>
<dt id="term-Placement-Group-Bundle">Placement Group Bundle<a class="headerlink" href="#term-Placement-Group-Bundle" title="Permalink to this term">#</a></dt><dd><p>A collection of resources that must be reserved on a single Ray node.
<a class="reference internal" href="../ray-core/scheduling/placement-group.html#ray-placement-group-doc-ref"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Checkpoint">Checkpoint<a class="headerlink" href="#term-Checkpoint" title="Permalink to this term">#</a></dt><dd><p>A Ray Train Checkpoint is a common interface for accessing data and models across
different Ray components and libraries. A Checkpoint can have its data
represented as a directory on local (on-disk) storage, as a directory on an
external storage (e.g., cloud storage), and as an in-memory dictionary.
<a class="reference internal" href="../train/api/doc/ray.train.Checkpoint.html#ray.train.Checkpoint" title="ray.train.Checkpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learn</span> <span class="pre">more</span></code></a>,</p>
</dd>
<dt id="term-Ray-Client">Ray Client<a class="headerlink" href="#term-Ray-Client" title="Permalink to this term">#</a></dt><dd><p>The Ray Client is an API that connects a Python script to a remote Ray cluster.
Effectively, it allows you to leverage a remote Ray cluster just like you would
with Ray running on your local machine.
<a class="reference internal" href="../cluster/running-applications/job-submission/ray-client.html#ray-client-ref"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Ray-Cluster">Ray Cluster<a class="headerlink" href="#term-Ray-Cluster" title="Permalink to this term">#</a></dt><dd><p>A Ray cluster is a set of worker nodes connected to a common Ray head node.
Ray clusters can be fixed-size, or they can autoscale up and down according to
the resources requested by applications running on the cluster.</p>
</dd>
<dt id="term-Connector">Connector<a class="headerlink" href="#term-Connector" title="Permalink to this term">#</a></dt><dd><p>A connector performs transformations on data that comes out of a dataset or an
RL environment and is about to be passed to a model. Connectors are flexible
components and can be swapped out such that models are easily reusable and do
not have to be retrained for different data transformations.</p>
</dd>
<dt id="term-Tune-Config">Tune Config<a class="headerlink" href="#term-Tune-Config" title="Permalink to this term">#</a></dt><dd><p>This is the set of hyperparameters corresponding to a Tune trial.
Sampling from a hyperparameter search space will produce a config.</p>
</dd>
<dt id="term-Ray-Dashboard">Ray Dashboard<a class="headerlink" href="#term-Ray-Dashboard" title="Permalink to this term">#</a></dt><dd><p>Ray’s built-in dashboard is a web interface that provides metrics, charts,
and other features that help Ray users to understand and debug Ray applications.</p>
</dd>
<dt id="term-Dataset-object">Dataset (object)<a class="headerlink" href="#term-Dataset-object" title="Permalink to this term">#</a></dt><dd><p>A class that produces a sequence of distributed data blocks.</p>
<p><a class="reference internal" href="../data/api/doc/ray.data.Dataset.html#ray.data.Dataset" title="ray.data.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> exposes methods to read, transform, and consume data at scale.</p>
<p>To learn more about Datasets and the operations they support, read the <a class="reference internal" href="../data/api/api.html#data-api"><span class="std std-ref">Datasets API Reference</span></a>.</p>
</dd>
<dt id="term-Deployment">Deployment<a class="headerlink" href="#term-Deployment" title="Permalink to this term">#</a></dt><dd><p>A deployment is a group of actors that can handle traffic in Ray Serve.
Deployments are defined as a single class with a number of options, including
the number of “replicas” of the deployment, each of which will map to a Ray
actor at runtime. Requests to a deployment are load balanced across its replicas.</p>
</dd>
<dt id="term-Deployment-graph">Deployment graph<a class="headerlink" href="#term-Deployment-graph" title="Permalink to this term">#</a></dt><dd><p>A deployment graph is a group of Ray Serve deployments that are bound together
into a directed acyclic graph (DAG) to handle requests. This enables model
composition. Each request will be passed through the graph, allowing multiple
stages of processing. For example, there might be a different deployment for
preprocessing, inference, and postprocessing.</p>
</dd>
<dt id="term-Ingress-Deployment">Ingress Deployment<a class="headerlink" href="#term-Ingress-Deployment" title="Permalink to this term">#</a></dt><dd><p>The “ingress” deployment is the one that receives and responds to inbound user
traffic. It handles HTTP parsing and response formatting. In the case of a
deployment graph, it would also fan out requests to other deployments to do
things like a forward pass of an ML model.</p>
</dd>
<dt id="term-Driver">Driver<a class="headerlink" href="#term-Driver" title="Permalink to this term">#</a></dt><dd><p>“Driver” is the name of the process running the main script that starts all
other processes. For Python, this is usually the script you start with
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">...</span></code>.</p>
</dd>
<dt id="term-Tune-Driver">Tune Driver<a class="headerlink" href="#term-Tune-Driver" title="Permalink to this term">#</a></dt><dd><p>The Tune driver is the main event loop that’s happening on the node that
launched the Tune experiment. This event loop schedules trials given the
cluster resources, executes training on remote Trainable actors, and processes
results and checkpoints from those actors.</p>
</dd>
<dt id="term-Distributed-Data-Parallel">Distributed Data-Parallel<a class="headerlink" href="#term-Distributed-Data-Parallel" title="Permalink to this term">#</a></dt><dd><p>A distributed data-parallel (DDP) training job scales machine learning training
to happen on multiple nodes, where each node processes one shard of the full
dataset. Every worker holds a copy of the model weights, and a common strategy
for updating weights is a “mirrored strategy”, where each worker will hold the
exact same weights at all times, and computed gradients are averaged then
applied across all workers.</p>
<p>With N worker nodes and a dataset of size D, each worker is responsible for
only <code class="docutils literal notranslate"><span class="pre">D</span> <span class="pre">/</span> <span class="pre">N</span></code> datapoints. If each worker node computes the gradient on a batch
of size <code class="docutils literal notranslate"><span class="pre">B</span></code>, then the effective batch size of the DDP training is <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">*</span> <span class="pre">B</span></code>.</p>
</dd>
<dt id="term-Environment">Environment<a class="headerlink" href="#term-Environment" title="Permalink to this term">#</a></dt><dd><p>The world or simulation, in which one or more reinforcement learning agents
have to learn to behave optimally in wrt. a given reward function. An
environment consists of an observation space, a reward function, an action
space, a state transition function, and a distribution over initial states
(after a reset).</p>
<p>Episodes consisting of one or more time-steps are played through an
environment in order to generate and collect samples for learning.
These samples contain one 4-tuple of
<code class="docutils literal notranslate"><span class="pre">[observation,</span> <span class="pre">action,</span> <span class="pre">reward,</span> <span class="pre">next</span> <span class="pre">observation]</span></code> per timestep.</p>
</dd>
<dt id="term-Episode">Episode<a class="headerlink" href="#term-Episode" title="Permalink to this term">#</a></dt><dd><p>A series of subsequent RL environment timesteps, each of which is a
4-tuple: <code class="docutils literal notranslate"><span class="pre">[observation,</span> <span class="pre">action,</span> <span class="pre">reward,</span> <span class="pre">next</span> <span class="pre">observation]</span></code>.
Episodes can end with the terminated- or truncated-flags being True.
An episode generally spans multiple time-steps for one or more agents.
The Episode is an important concept in RL as “optimal agent behavior” is
defined as choosing actions that maximize the sum of individual rewards
over the course of an episode.</p>
</dd>
<dt id="term-Trial-Executor">Trial Executor<a class="headerlink" href="#term-Trial-Executor" title="Permalink to this term">#</a></dt><dd><p>An internal <a class="reference internal" href="../tune/api/internals.html#raytrialexecutor-docstring"><span class="std std-ref">Ray Tune component</span></a> that manages
the resource management and execution of each trial’s corresponding remote
Trainable actor. The trial  executor’s responsibilities include launching
training, checkpointing, and restoring remote tasks.</p>
</dd>
<dt id="term-Experiment">Experiment<a class="headerlink" href="#term-Experiment" title="Permalink to this term">#</a></dt><dd><p>A Ray Tune or Ray Train experiment is a collection of one or more training jobs
that may correspond to different hyperparameter configurations. These
experiments are launched via the
<a class="reference internal" href="../tune/api/execution.html#tune-run-ref"><span class="std std-ref">Tuner API</span></a> and the <a class="reference internal" href="../train/api/api.html#train-api"><span class="std std-ref">Trainer API</span></a>.</p>
</dd>
<dt id="term-Fault-tolerance">Fault tolerance<a class="headerlink" href="#term-Fault-tolerance" title="Permalink to this term">#</a></dt><dd><p>Fault tolerance in Ray Train and Tune consists of experiment-level and trial-level
restoration. Experiment-level restoration refers to resuming all trials,
in the event that an experiment is interrupted in the middle of training due
to a cluster-level failure. Trial-level restoration refers to resuming
individual trials, in the event that a trial encounters a runtime
error such as OOM.</p>
</dd>
<dt id="term-Framework">Framework<a class="headerlink" href="#term-Framework" title="Permalink to this term">#</a></dt><dd><p>The deep-learning framework used for the model(s), loss(es), and optimizer(s)
inside an RLlib Algorithm. RLlib currently supports PyTorch and TensorFlow.</p>
</dd>
<dt id="term-GCS-Global-Control-Service">GCS / Global Control Service<a class="headerlink" href="#term-GCS-Global-Control-Service" title="Permalink to this term">#</a></dt><dd><p>Centralized metadata server for a Ray cluster. It runs on the Ray head node
and has functions like managing node membership and actor directory.
It’s also known as the Global Control Store.</p>
</dd>
<dt id="term-Head-node">Head node<a class="headerlink" href="#term-Head-node" title="Permalink to this term">#</a></dt><dd><p>A node that runs extra cluster-level processes like GCS and API server in
addition to those processes running on a worker node. A Ray cluster only has
one head node.</p>
</dd>
<dt id="term-HPO">HPO<a class="headerlink" href="#term-HPO" title="Permalink to this term">#</a></dt><dd><p>Hyperparameter optimization (HPO) is the process of choosing a set of optimal
hyperparameters for a learning algorithm. A hyperparameter can be a parameter
whose value is used to control the learning process (e.g., learning rate),
define the model architecture (e.g, number of hidden layers), or influence data
pre-processing. In the case of Ray Train, hyperparameters can also include
compute processing scale-out parameters such as the number of distributed
training workers.</p>
</dd>
<dt id="term-Job">Job<a class="headerlink" href="#term-Job" title="Permalink to this term">#</a></dt><dd><p>A ray job is a packaged ray application that can be executed on a
(remote) Ray cluster. <a class="reference internal" href="../cluster/running-applications/job-submission/index.html#jobs-overview"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Lineage">Lineage<a class="headerlink" href="#term-Lineage" title="Permalink to this term">#</a></dt><dd><p>For Ray objects, this is the set of tasks that was originally executed to
produce the object. If an object’s value is lost due to node failure,
Ray may attempt to recover the value by re-executing the object’s lineage.</p>
</dd>
<dt id="term-Model">Model<a class="headerlink" href="#term-Model" title="Permalink to this term">#</a></dt><dd><p>A function approximator with trainable parameters (e.g. a neural network) that
can be trained by an algorithm on available data or collected data from an RL
environment. The parameters are usually initialized at random (unlearned state).
During the training process, checkpoints of the model can be created such that -
after the learning process is shut down or crashes - training can resume from
the latest weights rather than having to re-learn from scratch.
After the training process is completed, models can be deployed into production
for inference using Ray Serve.</p>
</dd>
<dt id="term-Multi-agent">Multi-agent<a class="headerlink" href="#term-Multi-agent" title="Permalink to this term">#</a></dt><dd><p>Denotes an RL environment setup, in which several (more than one) agents act
in the same environment and learn either the same or different optimal
behaviors. The relationship between the different agents in a multi-agent setup
might be adversarial (playing against each other), cooperative (trying to reach
a common goal) or neutral (the agents don’t really care about other agents’
actions). The NN model architectures that can be used for multi-agent training
range from “independent” (each agent trains its own separate model), over
“partially shared” (i.e. some agents might share their value function, because
they have a common goal), to “identical” (all agents train on the same model).</p>
</dd>
<dt id="term-Namespace">Namespace<a class="headerlink" href="#term-Namespace" title="Permalink to this term">#</a></dt><dd><p>A namespace is a logical grouping of jobs and named actors. When an actor is
named, its name must be unique within the namespace.
When a namespace is not specified, Ray will place your job in an anonymous
namespace.</p>
</dd>
<dt id="term-Node">Node<a class="headerlink" href="#term-Node" title="Permalink to this term">#</a></dt><dd><p>A Ray node is a physical or virtual machine that is part of a Ray cluster.
See also <a class="reference internal" href="#term-Head-node"><span class="xref std std-term">Head node</span></a>.</p>
</dd>
<dt id="term-Object">Object<a class="headerlink" href="#term-Object" title="Permalink to this term">#</a></dt><dd><p>An application value. These are values that are returned by a task or
created through <code class="docutils literal notranslate"><span class="pre">ray.put</span></code>.</p>
</dd>
<dt id="term-Object-ownership">Object ownership<a class="headerlink" href="#term-Object-ownership" title="Permalink to this term">#</a></dt><dd><p>Ownership is the concept used to decide where metadata for a certain
<code class="docutils literal notranslate"><span class="pre">ObjectRef</span></code> (and the task that creates the value) should be stored.
If a worker calls <code class="docutils literal notranslate"><span class="pre">foo.remote()</span></code> or <code class="docutils literal notranslate"><span class="pre">ray.put()</span></code>, it owns the metadata for
the returned <code class="docutils literal notranslate"><span class="pre">ObjectRef</span></code>, e.g., ref count and location information. If an
object’s owner dies and another worker tries to get the value,
it will receive an <code class="docutils literal notranslate"><span class="pre">OwnerDiedError</span></code> exception.</p>
</dd>
<dt id="term-Object-reference">Object reference<a class="headerlink" href="#term-Object-reference" title="Permalink to this term">#</a></dt><dd><p>A pointer to an application value, which can be stored anywhere in the cluster.
Can be created by calling <code class="docutils literal notranslate"><span class="pre">foo.remote()</span></code> or <code class="docutils literal notranslate"><span class="pre">ray.put()</span></code>.
If using <code class="docutils literal notranslate"><span class="pre">foo.remote()</span></code>, then the returned <code class="docutils literal notranslate"><span class="pre">ObjectRef</span></code> is also a future.</p>
</dd>
<dt id="term-Object-store">Object store<a class="headerlink" href="#term-Object-store" title="Permalink to this term">#</a></dt><dd><p>A distributed in-memory data store for storing Ray objects.</p>
</dd>
<dt id="term-Object-spilling">Object spilling<a class="headerlink" href="#term-Object-spilling" title="Permalink to this term">#</a></dt><dd><p>Objects in the object store are spilled to external storage once the capacity
of the object store is used up. This enables out-of-core data processing for
memory-intensive distributed applications. It comes with a performance penalty
since data needs to be written to disk.</p>
</dd>
<dt id="term-Observation">Observation<a class="headerlink" href="#term-Observation" title="Permalink to this term">#</a></dt><dd><p>The full or partial state of an RL environment, which an agent sees
(has access to) at each timestep. A fully observable environment produces
observations that contain all the information to sufficiently infer the current
underlying state of the environment. Such states are also called “Markovian”.
Examples for environments with Markovian observations are chess or 2D games,
in which the player can see with each frame the entirety of the game’s state.
A partially observable (or non-Markovian) environment produces observations
that do not contain sufficient information to infer the exact underlying state.
An example here would be a robot with a camera on its head facing forward.
The robot walks around in a maze, but from a single camera frame might not know
what’s currently behind it.</p>
</dd>
<dt id="term-Offline-data">Offline data<a class="headerlink" href="#term-Offline-data" title="Permalink to this term">#</a></dt><dd><p>Data collected in an RL environment up-front and stored in some data format
(e.g. JSON). Offline data can be used to train an RL agent. The data might have
been generated by a non-RL/ML system, such as a simple decision making script.
Also, when training from offline data, the RL algorithm will not be able to
explore new actions in new situations as all interactions with the environment
already happened in the past (were recorded prior to training).</p>
</dd>
<dt id="term-Offline-RL">Offline RL<a class="headerlink" href="#term-Offline-RL" title="Permalink to this term">#</a></dt><dd><p>A sub-field of reinforcement learning (RL), in which specialized offline
RL Algorithms learn how to compute optimal actions for an agent inside an
environment without the ability to interact live with that environment.
Instead, the data used for training has already been collected up-front
(maybe even by a non-RL/ML system). This is very similar to a supervised
learning setup. Examples for offline RL algorithms are MARWIL, CQL, and CRR.</p>
</dd>
<dt id="term-Off-Policy">Off-Policy<a class="headerlink" href="#term-Off-Policy" title="Permalink to this term">#</a></dt><dd><p>A type of RL Algorithm. In an off-policy algorithm, the policy used to compute
the actions inside an RL environment (to generate the training data) might be
different from the one that is being optimized. Examples for off-policy
Algorithms are DQN, SAC, and DDPG.</p>
</dd>
<dt id="term-On-Policy">On-Policy<a class="headerlink" href="#term-On-Policy" title="Permalink to this term">#</a></dt><dd><p>A type of RL Algorithm. In an on-policy algorithm, the policy used to compute
the actions inside an RL environment (to generate the training data) must be the
exact same (matching NN weights at all times) than the one that is being
optimized. Examples for on-policy Algorithms are PPO, APPO, and IMPALA.</p>
</dd>
<dt id="term-OOM-Out-of-Memory">OOM (Out of Memory)<a class="headerlink" href="#term-OOM-Out-of-Memory" title="Permalink to this term">#</a></dt><dd><p>Ray may run out of memory if the application is using too much memory on a
single node. In this case the <a class="reference internal" href="../ray-core/scheduling/ray-oom-prevention.html#oom-questions"><span class="std std-ref">Ray OOM killer</span></a> will kick
in and kill worker processes to free up memory.</p>
</dd>
<dt id="term-Placement-group">Placement group<a class="headerlink" href="#term-Placement-group" title="Permalink to this term">#</a></dt><dd><p>Placement groups allow users to atomically reserve groups of resources across
multiple nodes (i.e., gang scheduling). They can be then used to schedule Ray
tasks and actors packed as close as possible for locality (PACK), or spread
apart (SPREAD). Placement groups are generally used for gang-scheduling actors,
but also support tasks.
<a class="reference internal" href="../ray-core/scheduling/placement-group.html#ray-placement-group-doc-ref"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Policy">Policy<a class="headerlink" href="#term-Policy" title="Permalink to this term">#</a></dt><dd><p>A (neural network) model that maps an RL environment observation of some agent
to its next action inside an RL environment.</p>
</dd>
<dt id="term-Predictor">Predictor<a class="headerlink" href="#term-Predictor" title="Permalink to this term">#</a></dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">An</span> <span class="pre">interface</span> <span class="pre">for</span> <span class="pre">performing</span> <span class="pre">inference</span></code> (prediction)
on input data with a trained model.</p>
</dd>
<dt id="term-Preprocessor">Preprocessor<a class="headerlink" href="#term-Preprocessor" title="Permalink to this term">#</a></dt><dd><p><a class="reference internal" href="../data/api/preprocessor.html#preprocessor-ref"><span class="std std-ref">An interface used to preprocess a Dataset</span></a> for
training and inference (prediction). Preprocessors
can be stateful, as they can be fitted on the training dataset before being
used to transform the training and evaluation datasets.</p>
</dd>
<dt id="term-Ray-application">Ray application<a class="headerlink" href="#term-Ray-application" title="Permalink to this term">#</a></dt><dd><p>A collection of Ray tasks, actors, and objects that originate from the
same script.</p>
</dd>
<dt id="term-Raylet">Raylet<a class="headerlink" href="#term-Raylet" title="Permalink to this term">#</a></dt><dd><p>A system process that runs on each Ray node. It’s responsible for scheduling
and object management.</p>
</dd>
<dt id="term-Replica">Replica<a class="headerlink" href="#term-Replica" title="Permalink to this term">#</a></dt><dd><p>A replica is a single actor that handles requests to a given Serve deployment.
A deployment may consist of many replicas, either statically-configured via
<code class="docutils literal notranslate"><span class="pre">num_replicas</span></code> or dynamically configured using auto-scaling.</p>
</dd>
<dt id="term-Resource-logical-and-physical">Resource (logical and physical)<a class="headerlink" href="#term-Resource-logical-and-physical" title="Permalink to this term">#</a></dt><dd><p>Ray resources are logical resources (e.g. CPU, GPU) used by tasks and actors.
It doesn’t necessarily map 1-to-1 to physical resources of machines on which
Ray cluster runs. <a class="reference internal" href="../ray-core/scheduling/resources.html#core-resources"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Reward">Reward<a class="headerlink" href="#term-Reward" title="Permalink to this term">#</a></dt><dd><p>A single floating point value that each agent within an RL environment receives
after each action taken. An agent is defined to be acting optimally inside the
RL environment when the sum over all received rewards within an episode is
maximized.</p>
<p>Note that rewards might be delayed (not immediately telling the agent, whether
an action was good or bad) or sparse (often have a value of zero) making it
harder for the agent to learn.</p>
</dd>
<dt id="term-Rollout">Rollout<a class="headerlink" href="#term-Rollout" title="Permalink to this term">#</a></dt><dd><p>The process of advancing through an episode in an RL environment (with one or
more RL agents) by taking sequential actions. During rollouts, the algorithm
should collect the environment produced 4-tuples [observations, actions,
rewards, next observations] in order to (later or simultaneously) learn how to
behave more optimally from this data.</p>
</dd>
<dt id="term-Rollout-Worker">Rollout Worker<a class="headerlink" href="#term-Rollout-Worker" title="Permalink to this term">#</a></dt><dd><p>Component within a RLlib Algorithm responsible for advancing and collecting
observations and rewards in an RL environment. Actions for the different
agent(s) within the environment are computed by the Algorithms’ policy models.
A distributed algorithm might have several replicas of Rollout Workers running
as Ray actors in order to scale the data collection process for faster RL
training.</p>
<p>RolloutWorkers are used as <code class="docutils literal notranslate"><span class="pre">&#64;ray.remote</span></code> actors to collect and return samples
from environments or offline files in parallel. An RLlib
<code class="xref py py-class docutils literal notranslate"><span class="pre">Algorithm</span></code> usually has
<code class="docutils literal notranslate"><span class="pre">num_workers</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">RolloutWorker</span></code> (not <code class="docutils literal notranslate"><span class="pre">&#64;ray.remote</span></code>) in
its <code class="xref py py-class docutils literal notranslate"><span class="pre">WorkerSet</span></code> under <code class="docutils literal notranslate"><span class="pre">self.workers</span></code>.</p>
<p>Depending on its evaluation config settings, an additional
<code class="xref py py-class docutils literal notranslate"><span class="pre">WorkerSet</span></code> with
<code class="xref py py-class docutils literal notranslate"><span class="pre">Algorithm</span></code>
under <code class="docutils literal notranslate"><span class="pre">self.evaluation_workers</span></code>.</p>
</dd>
<dt id="term-Runtime-environment">Runtime environment<a class="headerlink" href="#term-Runtime-environment" title="Permalink to this term">#</a></dt><dd><p>A runtime environment defines dependencies such as files, packages, environment
variables needed for a Python script to run. It is installed dynamically on the
cluster at runtime, and can be specified for a Ray job, or for specific actors
and tasks. <a class="reference internal" href="../ray-core/handling-dependencies.html#handling-dependencies"><span class="std std-ref">Learn more</span></a>.</p>
</dd>
<dt id="term-Remote-Function">Remote Function<a class="headerlink" href="#term-Remote-Function" title="Permalink to this term">#</a></dt><dd><p>See <a class="reference internal" href="#term-Task"><span class="xref std std-term">Task</span></a>.</p>
</dd>
<dt id="term-Remote-Class">Remote Class<a class="headerlink" href="#term-Remote-Class" title="Permalink to this term">#</a></dt><dd><p>See <a class="reference internal" href="#term-Actor"><span class="xref std std-term">Actor</span></a>.</p>
</dd>
<dt id="term-Ray-Scheduler">(Ray) Scheduler<a class="headerlink" href="#term-Ray-Scheduler" title="Permalink to this term">#</a></dt><dd><p>A Ray component that assigns execution units (Task/Actor) to Ray nodes.</p>
</dd>
<dt id="term-Search-Space">Search Space<a class="headerlink" href="#term-Search-Space" title="Permalink to this term">#</a></dt><dd><p>The definition of the possible values for hyperparameters. Can be composed out
of constants, discrete values, distributions of functions. This is also
referred to as the “parameter space” (<code class="docutils literal notranslate"><span class="pre">param_space</span></code> in the <code class="docutils literal notranslate"><span class="pre">Tuner</span></code>).</p>
</dd>
<dt id="term-Search-algorithm">Search algorithm<a class="headerlink" href="#term-Search-algorithm" title="Permalink to this term">#</a></dt><dd><p>Search algorithms suggest new hyperparameter configurations to be evaluated
by Tune. The default search algorithm is random search, where each new
configuration is independent from the previous one. More sophisticated search
algorithms such as ones using Bayesian optimization will fit a model to predict
the hyperparameter configuration that will produce the best model, while also
exploring the space of possible hyperparameters. Many popular search algorithms
are built into Tune, most of which are integrations with other libraries.</p>
</dd>
<dt id="term-Serve-application">Serve application<a class="headerlink" href="#term-Serve-application" title="Permalink to this term">#</a></dt><dd><p>An application is the unit of upgrade in a Serve cluster.</p>
<p>An application consists of one or more deployments. One of these deployments
is considered the “ingress” deployment, which is where all inbound
traffic is handled.</p>
<p>Applications can be called via HTTP at their configured <code class="docutils literal notranslate"><span class="pre">route_prefix</span></code>.</p>
</dd>
<dt id="term-ServeHandle">ServeHandle<a class="headerlink" href="#term-ServeHandle" title="Permalink to this term">#</a></dt><dd><p>ServeHandle is the Python API for making requests to Serve deployments. A
handle is defined by passing one bound Serve deployment to the constructor of
another. Then at runtime that reference can be used to make requests. This is
used to combine multiple deployments into “deployment graphs.”</p>
</dd>
<dt id="term-Session">Session<a class="headerlink" href="#term-Session" title="Permalink to this term">#</a></dt><dd><ul>
<li><p>A Ray Train/Tune session: Tune session at the experiment execution layer
and Train session at the Data Parallel training layer
if running data-parallel distributed training with Ray Train.</p>
<p>The session allows access to metadata, such as which trial is being run,
information about the total number of workers, as well as the rank of the
current worker. The session is also the interface through which an individual
Trainable can interact with the Tune experiment as a whole. This includes uses
such as reporting an individual trial’s metrics, saving/loading checkpoints,
and retrieving the corresponding dataset shards for each Train worker.</p>
</li>
<li><p>A Ray cluster: in some cases the session also means a <a class="reference internal" href="#term-Ray-Cluster"><span class="xref std std-term">Ray Cluster</span></a>.
For example, logs of a Ray cluster are stored under <code class="docutils literal notranslate"><span class="pre">session_xxx/logs/</span></code>.</p></li>
</ul>
</dd>
<dt id="term-Spillback">Spillback<a class="headerlink" href="#term-Spillback" title="Permalink to this term">#</a></dt><dd><p>A task caller schedules a task by first sending a resource request to the
preferred raylet for that request. If the preferred raylet chooses not to grant
the resources locally, it may also “Spillback” and respond to the caller with
the address of a remote raylet at which the caller should retry the resource
request.</p>
</dd>
<dt id="term-State">State<a class="headerlink" href="#term-State" title="Permalink to this term">#</a></dt><dd><p>State of the environment an RL agent interacts with.</p>
</dd>
<dt id="term-Synchronous-execution">Synchronous execution<a class="headerlink" href="#term-Synchronous-execution" title="Permalink to this term">#</a></dt><dd><p>Two tasks A and B are executed synchronously if A must finish before B can
start. For example, if you call <code class="docutils literal notranslate"><span class="pre">ray.get</span></code> immediately after launching a remote
task with <code class="docutils literal notranslate"><span class="pre">task.remote()</span></code>, you’ll be running with synchronous execution,
since this will wait for the task to finish before the program continues.</p>
</dd>
<dt id="term-Synchronous-sampling">Synchronous sampling<a class="headerlink" href="#term-Synchronous-sampling" title="Permalink to this term">#</a></dt><dd><p>Sampling workers work in synchronous steps. All of them must finish collecting
a new batch of samples before training can proceed to the next iteration.</p>
</dd>
<dt id="term-Task">Task<a class="headerlink" href="#term-Task" title="Permalink to this term">#</a></dt><dd><p>A remote function invocation. This is a single function invocation that
executes on a process different from the caller, and potentially on a different
machine. A task can be stateless (a <code class="docutils literal notranslate"><span class="pre">&#64;ray.remote</span></code> function) or stateful (a
method of a <code class="docutils literal notranslate"><span class="pre">&#64;ray.remote</span></code> class - see Actor below). A task is executed
asynchronously with the caller: the <code class="docutils literal notranslate"><span class="pre">.remote()</span></code> call immediately returns
one or more <code class="docutils literal notranslate"><span class="pre">ObjectRefs</span></code> (futures) that can be used to retrieve the
return value(s). See <a class="reference internal" href="#term-Actor-task"><span class="xref std std-term">Actor task</span></a>.</p>
</dd>
<dt id="term-Trainable">Trainable<a class="headerlink" href="#term-Trainable" title="Permalink to this term">#</a></dt><dd><p>A <a class="reference internal" href="../tune/api/trainable.html#trainable-docs"><span class="std std-ref">Trainable</span></a> is the interface that Ray Tune uses to
perform custom training
logic. User-defined Trainables take in a configuration as an input and can
run user-defined training code as well as custom metric reporting and
checkpointing.</p>
<p>There are many types of trainables. Most commonly used is the function
trainable API, which is simply a Python function that contains model training
logic and metric reporting. Tune also exposes a class trainable API, which
allows you to implement training, checkpointing, and restoring as different
methods.</p>
<p>Ray Tune associates each trial with its own Trainable – the Trainable is the
one actually doing training. The Trainable is a remote actor that can be placed
on any node in a Ray cluster.</p>
</dd>
<dt id="term-Trainer">Trainer<a class="headerlink" href="#term-Trainer" title="Permalink to this term">#</a></dt><dd><p>A Trainer is the top-level API to configure a single distributed training job.
<a class="reference internal" href="../train/api/api.html#air-trainer-ref"><span class="std std-ref">There are built-in Trainers for different frameworks</span></a>,
like PyTorch, Tensorflow, and XGBoost. Each trainer shares a common interface
and otherwise defines framework-specific configurations and entrypoints. The
main job of a trainer is to coordinate N distributed training workers and set
up the communication backends necessary for these workers to communicate
(e.g., for sharing computed gradients).</p>
</dd>
<dt id="term-Trainer-configuration">Trainer configuration<a class="headerlink" href="#term-Trainer-configuration" title="Permalink to this term">#</a></dt><dd><p>A Trainer can be configured in various ways. Some
configurations are shared across all trainers, like the RunConfig, which
configures things like the experiment storage, and ScalingConfig, which
configures the number of training workers as well as resources needed per
worker. Other configurations are specific to the trainer framework.</p>
</dd>
<dt id="term-Training-iteration">Training iteration<a class="headerlink" href="#term-Training-iteration" title="Permalink to this term">#</a></dt><dd><p>A partial training pass of input data up to pre-defined yield point
(e.g., time or data consumed) for checkpointing of long running training jobs.
A full training epoch can consist of multiple training iterations.
.. TODO: RLlib</p>
</dd>
<dt id="term-Training-epoch">Training epoch<a class="headerlink" href="#term-Training-epoch" title="Permalink to this term">#</a></dt><dd><p>A full training pass of the input dataset. Typically, model training iterates
through the full dataset in batches of size B, where gradients are calculated
on each batch and then applied as an update to the model weights. Training
jobs can consist of multiple epochs by training through the same dataset
multiple times.</p>
</dd>
<dt id="term-Training-step">Training step<a class="headerlink" href="#term-Training-step" title="Permalink to this term">#</a></dt><dd><p>An RLlib-specific method of the Algorithm class which includes the core logic
of an RL algorithm. Commonly includes gathering of experiences (either through
sampling or from offline data), optimization steps, redistribution of learnt
model weights. The particularities of this method are specific to algorithms
and configurations.</p>
</dd>
<dt id="term-Transition">Transition<a class="headerlink" href="#term-Transition" title="Permalink to this term">#</a></dt><dd><p>A tuple of (observation, action, reward, next observation). A transition
represents one step of an agent in an environment.</p>
</dd>
<dt id="term-Trial">Trial<a class="headerlink" href="#term-Trial" title="Permalink to this term">#</a></dt><dd><p>One training run within a Ray Tune experiment. If you run multiple trials,
each trial usually corresponds to a different config (a set of hyperparameters).</p>
</dd>
<dt id="term-Trial-scheduler">Trial scheduler<a class="headerlink" href="#term-Trial-scheduler" title="Permalink to this term">#</a></dt><dd><p>When running a Ray Tune job, the scheduler will decide how to allocate
resources to trials. In the most common case, this resource is time - the trial
scheduler decides which trials to run at what time. Certain built-in schedulers
like Asynchronous Hyperband (ASHA) perform early stopping of under-performing
trials, while others like Population Based Training (PBT) will make
under-performing trials copy the hyperparameter config and model weights of
top performing trials and continue training.</p>
</dd>
<dt id="term-Tuner">Tuner<a class="headerlink" href="#term-Tuner" title="Permalink to this term">#</a></dt><dd><p>The Tuner is the top level Ray Tune API used to configure and run an
experiment with many trials.</p>
</dd>
<dt id="term-Worker-process-worker">Worker process / worker<a class="headerlink" href="#term-Worker-process-worker" title="Permalink to this term">#</a></dt><dd><p>The process that runs user defined tasks and actors.</p>
</dd>
</dl>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../cluster/usage-stats.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">使用统计收集</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../ray-contribute/index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">开发者指引</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>