
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>算法 &#8212; Ray 2.7.2</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/versionwarning.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="../_static/js/docsearch.js"></script>
    <script defer="defer" src="../_static/js/csat.js"></script>
    <script defer="defer" src="../_static/js/termynal.js"></script>
    <script defer="defer" src="../_static/js/custom.js"></script>
    <script defer="defer" src="../_static/js/top-navigation.js"></script>
    <script src="../_static/js/tags.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/rllib/rllib-algorithms.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="用户指南" href="user-guides.html" />
    <link rel="prev" title="环境" href="rllib-env.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
  data-modal-disclaimer = "Results are automated and may be incorrect or contain inappropriate information. Do not include any personal data or confidential information."
  data-modal-title = "Ray Docs AI - Ask a Question"
  data-button-position-bottom = "60px"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 2.7.2</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    欢迎来到 Ray ！
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   概述「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   入门
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/installation.html">
   安装「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   用例「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   示例库「1%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   生态「3%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray 核心「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray 数据「75%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../train/train.html">
   Ray 训练「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune/index.html">
   Ray 调参「0%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   Ray RLlib
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-training.html">
     RLlib 入门
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     关键概念
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-env.html">
     环境
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="user-guides.html">
     用户指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="rllib-examples.html">
     示例
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="package_ref/index.html">
     Ray RLlib API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   更多类库「40%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cluster/getting-started.html">
   Ray 集群「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   监控调试「100%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   参考「20%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/index.html">
   开发者指引「30%」
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-security/index.html">
   安全「100%」
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Frllib/rllib-algorithms.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/rllib/rllib-algorithms.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/rllib/rllib-algorithms.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#available-algorithms-overview">
   Available Algorithms - Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#offline">
   Offline
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#behavior-cloning-bc-derived-from-marwil-implementation">
     Behavior Cloning (BC; derived from MARWIL implementation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#critic-regularized-regression-crr">
     Critic Regularized Regression (CRR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-q-learning-cql">
     Conservative Q-Learning (CQL)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monotonic-advantage-re-weighted-imitation-learning-marwil">
     Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-on-policy-rl">
   Model-free On-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asynchronous-proximal-policy-optimization-appo">
     Asynchronous Proximal Policy Optimization (APPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decentralized-distributed-proximal-policy-optimization-dd-ppo">
     Decentralized Distributed Proximal Policy Optimization (DD-PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proximal-policy-optimization-ppo">
     Proximal Policy Optimization (PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importance-weighted-actor-learner-architecture-impala">
     Importance Weighted Actor-Learner Architecture (IMPALA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advantage-actor-critic-a2c">
     Advantage Actor-Critic (A2C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asynchronous-advantage-actor-critic-a3c">
     Asynchronous Advantage Actor-Critic (A3C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-gradients-pg">
     Policy Gradients (PG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-agnostic-meta-learning-maml">
     Model-Agnostic Meta-Learning (MAML)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-off-policy-rl">
   Model-free Off-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-prioritized-experience-replay-ape-x">
     Distributed Prioritized Experience Replay (Ape-X)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-replay-distributed-dqn-r2d2">
     Recurrent Replay Distributed DQN (R2D2)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-q-networks-dqn-rainbow-parametric-dqn">
     Deep Q Networks (DQN, Rainbow, Parametric DQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-deterministic-policy-gradients-ddpg">
     Deep Deterministic Policy Gradients (DDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#twin-delayed-ddpg-td3">
     Twin Delayed DDPG (TD3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-actor-critic-sac">
     Soft Actor Critic (SAC)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-based-rl">
   Model-based RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dreamerv3">
     DreamerV3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dreamer">
     Dreamer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-based-meta-policy-optimization-mb-mpo">
     Model-Based Meta-Policy-Optimization (MB-MPO)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivative-free">
   Derivative-free
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#augmented-random-search-ars">
     Augmented Random Search (ARS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-strategies-es">
     Evolution Strategies (ES)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rl-for-recommender-systems">
   RL for recommender systems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slateq">
     SlateQ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contextual-bandits">
   Contextual Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-upper-confidence-bound-banditlinucb">
     Linear Upper Confidence Bound (BanditLinUCB)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-thompson-sampling-banditlints">
     Linear Thompson Sampling (BanditLinTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-agent">
   Multi-agent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-sharing">
     Parameter Sharing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qmix-monotonic-value-factorisation-qmix-vdn-iqn">
     QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-agent-deep-deterministic-policy-gradient-maddpg">
     Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shared-critic-methods">
     Shared Critic Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#others">
   Others
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-player-alpha-zero-alphazero">
     Single-Player Alpha Zero (AlphaZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiagent-leelachesszero-leelachesszero">
     MultiAgent LeelaChessZero (LeelaChessZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#curiosity-icm-intrinsic-curiosity-module">
     Curiosity (ICM: Intrinsic Curiosity Module)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#re3-random-encoders-for-efficient-exploration">
     RE3 (Random Encoders for Efficient Exploration)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fully-independent-learning">
     Fully Independent Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>算法</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#available-algorithms-overview">
   Available Algorithms - Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#offline">
   Offline
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#behavior-cloning-bc-derived-from-marwil-implementation">
     Behavior Cloning (BC; derived from MARWIL implementation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#critic-regularized-regression-crr">
     Critic Regularized Regression (CRR)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-q-learning-cql">
     Conservative Q-Learning (CQL)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monotonic-advantage-re-weighted-imitation-learning-marwil">
     Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-on-policy-rl">
   Model-free On-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asynchronous-proximal-policy-optimization-appo">
     Asynchronous Proximal Policy Optimization (APPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decentralized-distributed-proximal-policy-optimization-dd-ppo">
     Decentralized Distributed Proximal Policy Optimization (DD-PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proximal-policy-optimization-ppo">
     Proximal Policy Optimization (PPO)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importance-weighted-actor-learner-architecture-impala">
     Importance Weighted Actor-Learner Architecture (IMPALA)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advantage-actor-critic-a2c">
     Advantage Actor-Critic (A2C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asynchronous-advantage-actor-critic-a3c">
     Asynchronous Advantage Actor-Critic (A3C)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-gradients-pg">
     Policy Gradients (PG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-agnostic-meta-learning-maml">
     Model-Agnostic Meta-Learning (MAML)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-off-policy-rl">
   Model-free Off-policy RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-prioritized-experience-replay-ape-x">
     Distributed Prioritized Experience Replay (Ape-X)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recurrent-replay-distributed-dqn-r2d2">
     Recurrent Replay Distributed DQN (R2D2)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-q-networks-dqn-rainbow-parametric-dqn">
     Deep Q Networks (DQN, Rainbow, Parametric DQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-deterministic-policy-gradients-ddpg">
     Deep Deterministic Policy Gradients (DDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#twin-delayed-ddpg-td3">
     Twin Delayed DDPG (TD3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#soft-actor-critic-sac">
     Soft Actor Critic (SAC)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-based-rl">
   Model-based RL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dreamerv3">
     DreamerV3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dreamer">
     Dreamer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-based-meta-policy-optimization-mb-mpo">
     Model-Based Meta-Policy-Optimization (MB-MPO)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivative-free">
   Derivative-free
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#augmented-random-search-ars">
     Augmented Random Search (ARS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evolution-strategies-es">
     Evolution Strategies (ES)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rl-for-recommender-systems">
   RL for recommender systems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slateq">
     SlateQ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contextual-bandits">
   Contextual Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-upper-confidence-bound-banditlinucb">
     Linear Upper Confidence Bound (BanditLinUCB)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-thompson-sampling-banditlints">
     Linear Thompson Sampling (BanditLinTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-agent">
   Multi-agent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-sharing">
     Parameter Sharing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qmix-monotonic-value-factorisation-qmix-vdn-iqn">
     QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-agent-deep-deterministic-policy-gradient-maddpg">
     Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shared-critic-methods">
     Shared Critic Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#others">
   Others
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-player-alpha-zero-alphazero">
     Single-Player Alpha Zero (AlphaZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiagent-leelachesszero-leelachesszero">
     MultiAgent LeelaChessZero (LeelaChessZero)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#curiosity-icm-intrinsic-curiosity-module">
     Curiosity (ICM: Intrinsic Curiosity Module)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#re3-random-encoders-for-efficient-exploration">
     RE3 (Random Encoders for Efficient Exploration)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fully-independent-learning">
     Fully Independent Learning
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="admonition note">
<p class="admonition-title">Note</p>
<p>From Ray 2.6.0 onwards, RLlib is adopting a new stack for training and model customization,
gradually replacing the ModelV2 API and some convoluted parts of Policy API with the RLModule API.
Click <a class="reference external" href="rllib-rlmodule.html">here</a> for details.</p>
</div>
<section id="rllib-algorithms-doc">
<span id="id1"></span><h1>算法<a class="headerlink" href="#rllib-algorithms-doc" title="Permalink to this headline">#</a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out the <a class="reference external" href="rllib-env.html">environments</a> page to learn more about different environment types.</p>
</div>
<section id="available-algorithms-overview">
<h2>Available Algorithms - Overview<a class="headerlink" href="#available-algorithms-overview" title="Permalink to this headline">#</a></h2>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 6%" />
<col style="width: 17%" />
<col style="width: 10%" />
<col style="width: 6%" />
<col style="width: 35%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
<th class="head"><p>Multi-GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#a2c">A2C</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>A2C: tf + torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#a3c">A3C</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#alphazero">AlphaZero</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#appo">APPO</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ars">ARS</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bandits">Bandits</a> (<a class="reference external" href="rllib-algorithms.html#lints">TS</a> &amp; <a class="reference external" href="rllib-algorithms.html#lin-ucb">LinUCB</a>)</p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bc">BC</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cql">CQL</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#crr">CRR</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ddpg">DDPG</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="rllib-algorithms.html#apex">APEX-DDPG</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dreamerv3">DreamerV3</a></p></td>
<td><p>tf</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a> (GRU-based by default)</p></td>
<td><p>tf</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dreamer">Dreamer</a></p></td>
<td><p>torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dqn">DQN</a>, <a class="reference external" href="rllib-algorithms.html#dqn">Rainbow</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="rllib-algorithms.html#apex">APEX-DQN</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#es">ES</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#impala">IMPALA</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#leelachesszero">LeelaChessZero</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#maml">MAML</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#marwil">MARWIL</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mbmpo">MBMPO</a></p></td>
<td><p>torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pg">PG</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ppo">PPO</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#attention">+Attention</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>tf + torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#r2d2">R2D2</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a>, <a class="reference external" href="rllib-models.html#built-in-models">+LSTM auto-wrapping</a>, <a class="reference external" href="rllib-models.html#autoregressive-action-distributions">+autoreg</a></p></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sac">SAC</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#slateq">SlateQ</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> (multi-discr. slates)</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td></td>
<td><p>torch</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#td3">TD3</a></p></td>
<td><p>tf + torch</p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
<td><p>torch</p></td>
</tr>
</tbody>
</table>
<p>Multi-Agent only Methods</p>
<table class="table">
<colgroup>
<col style="width: 28%" />
<col style="width: 9%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 10%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#qmix">QMIX</a></p></td>
<td><p>torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#maddpg">MADDPG</a></p></td>
<td><p>tf</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p>Partial</p></td>
<td><p><strong>Yes</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#parameter-sharing">Parameter Sharing</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#fully-independent-learning">Fully Independent Learning</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#shared-critic-methods">Shared Critic Methods</a></p></td>
<td colspan="5"><p>Depends on bootstrapped algorithm</p></td>
</tr>
</tbody>
</table>
<p>Exploration-based plug-ins (can be combined with any algo)</p>
<table class="table">
<colgroup>
<col style="width: 28%" />
<col style="width: 9%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 10%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Frameworks</p></th>
<th class="head"><p>Discrete Actions</p></th>
<th class="head"><p>Continuous Actions</p></th>
<th class="head"><p>Multi-Agent</p></th>
<th class="head"><p>Model Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#curiosity">Curiosity</a></p></td>
<td><p>tf + torch</p></td>
<td><p><strong>Yes</strong> <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">+parametric</a></p></td>
<td><p>No</p></td>
<td><p><strong>Yes</strong></p></td>
<td><p><a class="reference external" href="rllib-models.html#rnns">+RNN</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="offline">
<h2>Offline<a class="headerlink" href="#offline" title="Permalink to this headline">#</a></h2>
<section id="behavior-cloning-bc-derived-from-marwil-implementation">
<span id="bc"></span><h3>Behavior Cloning (BC; derived from MARWIL implementation)<a class="headerlink" href="#behavior-cloning-bc-derived-from-marwil-implementation" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="http://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bc/bc.py">[implementation]</a></p>
<p>Our behavioral cloning implementation is directly derived from our <a class="reference internal" href="#marwil">MARWIL</a> implementation,
with the only difference being the <code class="docutils literal notranslate"><span class="pre">beta</span></code> parameter force-set to 0.0. This makes
BC try to match the behavior policy, which generated the offline data, disregarding any resulting rewards.
BC requires the <a class="reference external" href="rllib-offline.html">offline datasets API</a> to be used.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/bc/cartpole-bc.yaml">CartPole-v1</a></p>
<p><strong>BC-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="critic-regularized-regression-crr">
<span id="crr"></span><h3>Critic Regularized Regression (CRR)<a class="headerlink" href="#critic-regularized-regression-crr" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/2006.15134">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/crr/crr.py">[implementation]</a></p>
<p>CRR is another offline RL algorithm based on Q-learning that can learn from an offline experience replay.
The challenge in applying existing Q-learning algorithms to offline RL lies in the overestimation of the Q-function, as well as, the lack of exploration beyond the observed data.
The latter becomes increasingly important during bootstrapping in the bellman equation, where the Q-function queried for the next state’s Q-value(s) does not have support in the observed data.
To mitigate these issues, CRR implements a simple and yet powerful idea of “value-filtered regression”.
The key idea is to use a learned critic to filter-out the non-promising transitions from the replay dataset. For more details, please refer to the paper (see link above).</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/crr/cartpole-v1-crr.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/crr/pendulum-v1-crr.yaml">Pendulum-v1</a></p>
</section>
<section id="conservative-q-learning-cql">
<span id="cql"></span><h3>Conservative Q-Learning (CQL)<a class="headerlink" href="#conservative-q-learning-cql" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/2006.04779">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/cql/cql.py">[implementation]</a></p>
<p>In offline RL, the algorithm has no access to an environment, but can only sample from a fixed dataset of pre-collected state-action-reward tuples.
In particular, CQL (Conservative Q-Learning) is an offline RL algorithm that mitigates the overestimation of Q-values outside the dataset distribution via
conservative critic estimates. It does so by adding a simple Q regularizer loss to the standard Bellman update loss.
This ensures that the critic does not output overly-optimistic Q-values. This conservative
correction term can be added on top of any off-policy Q-learning algorithm (here, we provide this for SAC).</p>
<p>RLlib’s CQL is evaluated against the Behavior Cloning (BC) benchmark at 500K gradient steps over the dataset. The only difference between the BC- and CQL configs is the <code class="docutils literal notranslate"><span class="pre">bc_iters</span></code> parameter in CQL, indicating how many gradient steps we perform over the BC loss. CQL is evaluated on the <a class="reference external" href="https://github.com/rail-berkeley/d4rl">D4RL</a> benchmark, which has pre-collected offline datasets for many types of environments.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/cql/halfcheetah-cql.yaml">HalfCheetah Random</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/cql/hopper-cql.yaml">Hopper Random</a></p>
<p><strong>CQL-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="monotonic-advantage-re-weighted-imitation-learning-marwil">
<span id="marwil"></span><h3>Monotonic Advantage Re-Weighted Imitation Learning (MARWIL)<a class="headerlink" href="#monotonic-advantage-re-weighted-imitation-learning-marwil" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="http://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/marwil/marwil.py">[implementation]</a></p>
<p>MARWIL is a hybrid imitation learning and policy gradient algorithm suitable for training on batched historical data.
When the <code class="docutils literal notranslate"><span class="pre">beta</span></code> hyperparameter is set to zero, the MARWIL objective reduces to vanilla imitation learning (see <a class="reference internal" href="#bc">BC</a>).
MARWIL requires the <a class="reference external" href="rllib-offline.html">offline datasets API</a> to be used.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/marwil/cartpole-marwil.yaml">CartPole-v1</a></p>
<p><strong>MARWIL-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="model-free-on-policy-rl">
<h2>Model-free On-policy RL<a class="headerlink" href="#model-free-on-policy-rl" title="Permalink to this headline">#</a></h2>
<section id="asynchronous-proximal-policy-optimization-appo">
<span id="appo"></span><h3>Asynchronous Proximal Policy Optimization (APPO)<a class="headerlink" href="#asynchronous-proximal-policy-optimization-appo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1707.06347">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/appo/appo.py">[implementation]</a>
We include an asynchronous variant of Proximal Policy Optimization (PPO) based on the IMPALA architecture. This is similar to IMPALA but using a surrogate policy loss with clipping. Compared to synchronous PPO, APPO is more efficient in wall-clock time due to its use of asynchronous sampling. Using a clipped loss also allows for multiple SGD passes, and therefore the potential for better sample efficiency compared to IMPALA. V-trace can also be enabled to correct for off-policy samples.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>APPO is not always more efficient; it is often better to use <a class="reference internal" href="#ppo"><span class="std std-ref">standard PPO</span></a> or <a class="reference internal" href="#impala"><span class="std std-ref">IMPALA</span></a>.</p>
</div>
<figure class="align-default" id="id5">
<img alt="../_images/impala-arch.svg" src="../_images/impala-arch.svg" /><figcaption>
<p><span class="caption-text">APPO architecture (same as IMPALA)</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/appo/pong-appo.yaml">PongNoFrameskip-v4</a></p>
<p><strong>APPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="decentralized-distributed-proximal-policy-optimization-dd-ppo">
<span id="ddppo"></span><h3>Decentralized Distributed Proximal Policy Optimization (DD-PPO)<a class="headerlink" href="#decentralized-distributed-proximal-policy-optimization-dd-ppo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1911.00357">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ddppo/ddppo.py">[implementation]</a>
Unlike APPO or PPO, with DD-PPO policy improvement is no longer done centralized in the algorithm process. Instead, gradients are computed remotely on each rollout worker and all-reduced at each mini-batch using <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">torch distributed</a>. This allows each worker’s GPU to be used both for sampling and for training.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>DD-PPO is best for envs that require GPUs to function, or if you need to scale out SGD to multiple nodes. If you don’t meet these requirements, <a class="reference external" href="#proximal-policy-optimization-ppo">standard PPO</a> will be more efficient.</p>
</div>
<figure class="align-default" id="id6">
<img alt="../_images/ddppo-arch.svg" src="../_images/ddppo-arch.svg" /><figcaption>
<p><span class="caption-text">DD-PPO architecture (both sampling and learning are done on worker GPUs)</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddppo/cartpole-ddppo.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddppo/atari-ddppo.yaml">BreakoutNoFrameskip-v4</a></p>
<p><strong>DDPPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="proximal-policy-optimization-ppo">
<span id="ppo"></span><h3>Proximal Policy Optimization (PPO)<a class="headerlink" href="#proximal-policy-optimization-ppo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1707.06347">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py">[implementation]</a>
PPO’s clipped objective supports multiple SGD passes over the same batch of experiences. RLlib’s multi-GPU optimizer pins that data in GPU memory to avoid unnecessary transfers from host memory, substantially improving performance over a naive implementation. PPO scales out using multiple workers for experience collection, and also to multiple GPUs for SGD.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you need to scale out with GPUs on multiple nodes, consider using <a class="reference external" href="#decentralized-distributed-proximal-policy-optimization-dd-ppo">decentralized PPO</a>.</p>
</div>
<figure class="align-default" id="id7">
<img alt="../_images/ppo-arch.svg" src="../_images/ppo-arch.svg" /><figcaption>
<p><span class="caption-text">PPO architecture</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/unity3d-soccer-strikers-vs-goalie-ppo.yaml">Unity3D Soccer (multi-agent: Strikers vs Goalie)</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/humanoid-ppo-gae.yaml">Humanoid-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/hopper-ppo.yaml">Hopper-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pendulum-ppo.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pong-ppo.yaml">PongDeterministic-v4</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/walker2d-ppo.yaml">Walker2d-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/halfcheetah-ppo.yaml">HalfCheetah-v2</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a></p>
<p><strong>Atari results</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 22%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib PPO &#64;10M</p></th>
<th class="head"><p>RLlib PPO &#64;25M</p></th>
<th class="head"><p>Baselines PPO &#64;10M</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2807</p></td>
<td><p>4480</p></td>
<td><p>~1800</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>104</p></td>
<td><p>201</p></td>
<td><p>~250</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>11085</p></td>
<td><p>14247</p></td>
<td><p>~14000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>671</p></td>
<td><p>944</p></td>
<td><p>~800</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 19%" />
<col style="width: 37%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib PPO 16-workers &#64; 1h</p></th>
<th class="head"><p>Fan et al PPO 16-workers &#64; 1h</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>9664</p></td>
<td><p>~7700</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="../_images/ppo.png"><img alt="../_images/ppo.png" src="../_images/ppo.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-text">RLlib’s multi-GPU PPO scales to multiple GPUs and hundreds of CPUs on solving the Humanoid-v1 task. Here we compare against a reference MPI-based implementation.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>PPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="importance-weighted-actor-learner-architecture-impala">
<span id="impala"></span><h3>Importance Weighted Actor-Learner Architecture (IMPALA)<a class="headerlink" href="#importance-weighted-actor-learner-architecture-impala" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1802.01561">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/impala/impala.py">[implementation]</a>
In IMPALA, a central learner runs SGD in a tight loop while asynchronously pulling sample batches from many actor processes. RLlib’s IMPALA implementation uses DeepMind’s reference <a class="reference external" href="https://github.com/deepmind/scalable_agent/blob/master/vtrace.py">V-trace code</a>. Note that we do not provide a deep residual network out of the box, but one can be plugged in as a <a class="reference external" href="rllib-models.html#custom-models-tensorflow">custom model</a>. Multiple learner GPUs and experience replay are also supported.</p>
<figure class="align-default" id="id9">
<img alt="../_images/impala-arch.svg" src="../_images/impala-arch.svg" /><figcaption>
<p><span class="caption-text">IMPALA architecture</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala.yaml">PongNoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala-vectorized.yaml">vectorized configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/pong-impala-fast.yaml">multi-gpu configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/impala/atari-impala.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a></p>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 16%" />
<col style="width: 41%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib IMPALA 32-workers</p></th>
<th class="head"><p>Mnih et al A3C 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2071</p></td>
<td><p>~3000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>385</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>4068</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>719</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability:</strong></p>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 40%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib IMPALA 32-workers &#64;1 hour</p></th>
<th class="head"><p>Mnih et al A3C 16-workers &#64;1 hour</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>3181</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>538</p></td>
<td><p>~10</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>10850</p></td>
<td><p>~500</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>843</p></td>
<td><p>~300</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id10">
<img alt="../_images/impala.png" src="../_images/impala.png" />
<figcaption>
<p><span class="caption-text">Multi-GPU IMPALA scales up to solve PongNoFrameskip-v4 in ~3 minutes using a pair of V100 GPUs and 128 CPU workers.
The maximum training throughput reached is ~30k transitions per second (~120k environment frames per second).</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>IMPALA-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="advantage-actor-critic-a2c">
<span id="a2c"></span><h3>Advantage Actor-Critic (A2C)<a class="headerlink" href="#advantage-actor-critic-a2c" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1602.01783">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a2c/a2c.py">[implementation]</a>
A2C scales to 16-32+ worker processes depending on the environment and supports microbatching
(i.e., gradient accumulation), which can be enabled by setting the <code class="docutils literal notranslate"><span class="pre">microbatch_size</span></code> config.
Microbatching allows for training with a <code class="docutils literal notranslate"><span class="pre">train_batch_size</span></code> much larger than GPU memory.</p>
<figure class="align-default" id="id11">
<img alt="../_images/a2c-arch.svg" src="../_images/a2c-arch.svg" /><figcaption>
<p><span class="caption-text">A2C architecture</span><a class="headerlink" href="#id11" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/a2c/atari-a2c.yaml">Atari environments</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="#importance-weighted-actor-learner-architecture-impala">IMPALA</a> for faster training with similar timestep efficiency.</p>
</div>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 19%" />
<col style="width: 36%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib A2C 5-workers</p></th>
<th class="head"><p>Mnih et al A3C 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>1401</p></td>
<td><p>~3000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>374</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>3620</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>692</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>A2C-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="asynchronous-advantage-actor-critic-a3c">
<span id="a3c"></span><h3>Asynchronous Advantage Actor-Critic (A3C)<a class="headerlink" href="#asynchronous-advantage-actor-critic-a3c" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1602.01783">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/a3c/a3c.py">[implementation]</a>
A3C is the asynchronous version of A2C, where gradients are computed on the workers directly after trajectory rollouts,
and only then shipped to a central learner to accumulate these gradients on the central model. After the central model update, parameters are broadcast back to
all workers.
Similar to A2C, A3C scales to 16-32+ worker processes depending on the environment.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/a3c/pong-a3c.yaml">PongDeterministic-v4</a></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="#importance-weighted-actor-learner-architecture-impala">IMPALA</a> for faster training with similar timestep efficiency.</p>
</div>
<p><strong>A3C-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="policy-gradients-pg">
<span id="pg"></span><h3>Policy Gradients (PG)<a class="headerlink" href="#policy-gradients-pg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/pg/pg.py">[implementation]</a>
We include a vanilla policy gradients implementation as an example algorithm.</p>
<figure class="align-default" id="id12">
<img alt="../_images/a2c-arch.svg" src="../_images/a2c-arch.svg" /><figcaption>
<p><span class="caption-text">Policy gradients architecture (same as A2C)</span><a class="headerlink" href="#id12" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/pg/cartpole-pg.yaml">CartPole-v1</a></p>
<p><strong>PG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="model-agnostic-meta-learning-maml">
<span id="maml"></span><h3>Model-Agnostic Meta-Learning (MAML)<a class="headerlink" href="#model-agnostic-meta-learning-maml" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1703.03400">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/maml/maml.py">[implementation]</a></p>
<p>RLlib’s MAML implementation is a meta-learning method for learning and quick adaptation across different tasks for continuous control. Code here is adapted from <a class="reference external" href="https://github.com/jonasrothfuss">https://github.com/jonasrothfuss</a>, which outperforms vanilla MAML and avoids computation of the higher order gradients during the meta-update step. MAML is evaluated on custom environments that are described in greater detail <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/apis/task_settable_env.py">here</a>.</p>
<p>MAML uses additional metrics to measure performance; <code class="docutils literal notranslate"><span class="pre">episode_reward_mean</span></code> measures the agent’s returns before adaptation, <code class="docutils literal notranslate"><span class="pre">episode_reward_mean_adapt_N</span></code> measures the agent’s returns after N gradient steps of inner adaptation, and <code class="docutils literal notranslate"><span class="pre">adaptation_delta</span></code> measures the difference in performance before and after adaptation. Examples can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments/tree/master/maml">here</a>.</p>
<p>Tuned examples: HalfCheetahRandDirecEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/halfcheetah_rand_direc.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/halfcheetah-rand-direc-maml.yaml">Config</a>), AntRandGoalEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/ant_rand_goal.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/ant-rand-goal-maml.yaml">Config</a>), PendulumMassEnv (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/pendulum_mass.py">Env</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/maml/pendulum-mass-maml.yaml">Config</a>)</p>
<p><strong>MAML-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="model-free-off-policy-rl">
<h2>Model-free Off-policy RL<a class="headerlink" href="#model-free-off-policy-rl" title="Permalink to this headline">#</a></h2>
<section id="distributed-prioritized-experience-replay-ape-x">
<span id="apex"></span><h3>Distributed Prioritized Experience Replay (Ape-X)<a class="headerlink" href="#distributed-prioritized-experience-replay-ape-x" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.00933">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_dqn/apex_dqn.py">[implementation]</a>
Ape-X variations of DQN and DDPG (<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_dqn/apex_dqn.py">APEX_DQN</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/apex_ddpg/apex_ddpg.py">APEX_DDPG</a>) use a single GPU learner and many CPU workers for experience collection. Experience collection can scale to hundreds of CPU workers due to the distributed prioritization of experience prior to storage in replay buffers.</p>
<figure class="align-default" id="id13">
<img alt="../_images/apex-arch.svg" src="../_images/apex-arch.svg" /><figcaption>
<p><span class="caption-text">Ape-X architecture</span><a class="headerlink" href="#id13" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_dqn/pong-apex-dqn.yaml">PongNoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_ddpg/pendulum-apex-ddpg.yaml">Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_ddpg/mountaincarcontinuous-apex-ddpg.yaml">MountainCarContinuous-v0</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/apex_dqn/atari-apex-dqn.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a>.</p>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib Ape-X 8-workers</p></th>
<th class="head"><p>Mnih et al Async DQN 16-workers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>6134</p></td>
<td><p>~6000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>123</p></td>
<td><p>~50</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>15302</p></td>
<td><p>~1200</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>686</p></td>
<td><p>~600</p></td>
</tr>
</tbody>
</table>
<p><strong>Scalability</strong>:</p>
<table class="table">
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib Ape-X 8-workers &#64;1 hour</p></th>
<th class="head"><p>Mnih et al Async DQN 16-workers &#64;1 hour</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>4873</p></td>
<td><p>~1000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>77</p></td>
<td><p>~10</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>4083</p></td>
<td><p>~500</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>646</p></td>
<td><p>~300</p></td>
</tr>
</tbody>
</table>
<figure class="align-default" id="id14">
<img alt="../_images/apex.png" src="../_images/apex.png" />
<figcaption>
<p><span class="caption-text">Ape-X using 32 workers in RLlib vs vanilla DQN (orange) and A3C (blue) on PongNoFrameskip-v4.</span><a class="headerlink" href="#id14" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Ape-X specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="recurrent-replay-distributed-dqn-r2d2">
<span id="r2d2"></span><h3>Recurrent Replay Distributed DQN (R2D2)<a class="headerlink" href="#recurrent-replay-distributed-dqn-r2d2" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://openreview.net/pdf?id=r1lyTjAqYX">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/r2d2/r2d2.py">[implementation]</a>
R2D2 can be scaled by increasing the number of workers. All of the DQN improvements evaluated in <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow</a> are available, though not all are enabled by default.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/r2d2/stateless-cartpole-r2d2.yaml">Stateless CartPole-v1</a></p>
</section>
<section id="deep-q-networks-dqn-rainbow-parametric-dqn">
<span id="dqn"></span><h3>Deep Q Networks (DQN, Rainbow, Parametric DQN)<a class="headerlink" href="#deep-q-networks-dqn-rainbow-parametric-dqn" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1312.5602">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dqn/dqn.py">[implementation]</a>
DQN can be scaled by increasing the number of workers or using Ape-X. Memory usage is reduced by compressing samples in the replay buffer with LZ4. All of the DQN improvements evaluated in <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow</a> are available, though not all are enabled by default. See also how to use <a class="reference external" href="rllib-models.html#variable-length-parametric-action-spaces">parametric-actions in DQN</a>.</p>
<figure class="align-default" id="id15">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">DQN architecture</span><a class="headerlink" href="#id15" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/pong-dqn.yaml">PongDeterministic-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/pong-rainbow.yaml">Rainbow configuration</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-dqn.yaml">{BeamRider,Breakout,Qbert,SpaceInvaders}NoFrameskip-v4</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-duel-ddqn.yaml">with Dueling and Double-Q</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dqn/atari-dist-dqn.yaml">with Distributional DQN</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider using <a class="reference external" href="#distributed-prioritized-experience-replay-ape-x">Ape-X</a> for faster training with similar timestep efficiency.</p>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>For a complete <a class="reference external" href="https://arxiv.org/pdf/1710.02298.pdf">rainbow</a> setup,
make the following changes to the default DQN config:
<code class="docutils literal notranslate"><span class="pre">&quot;n_step&quot;:</span> <span class="pre">[between</span> <span class="pre">1</span> <span class="pre">and</span> <span class="pre">10],</span>
<span class="pre">&quot;noisy&quot;:</span> <span class="pre">True,</span>
<span class="pre">&quot;num_atoms&quot;:</span> <span class="pre">[more</span> <span class="pre">than</span> <span class="pre">1],</span>
<span class="pre">&quot;v_min&quot;:</span> <span class="pre">-10.0,</span>
<span class="pre">&quot;v_max&quot;:</span> <span class="pre">10.0</span></code>
(set <code class="docutils literal notranslate"><span class="pre">v_min</span></code> and <code class="docutils literal notranslate"><span class="pre">v_max</span></code> according to your expected range of returns).</p>
</div>
<p><strong>Atari results &#64;10M steps</strong>: <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 10%" />
<col style="width: 19%" />
<col style="width: 23%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Atari env</p></th>
<th class="head"><p>RLlib DQN</p></th>
<th class="head"><p>RLlib Dueling DDQN</p></th>
<th class="head"><p>RLlib Dist. DQN</p></th>
<th class="head"><p>Hessel et al. DQN</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BeamRider</p></td>
<td><p>2869</p></td>
<td><p>1910</p></td>
<td><p>4447</p></td>
<td><p>~2000</p></td>
</tr>
<tr class="row-odd"><td><p>Breakout</p></td>
<td><p>287</p></td>
<td><p>312</p></td>
<td><p>410</p></td>
<td><p>~150</p></td>
</tr>
<tr class="row-even"><td><p>Qbert</p></td>
<td><p>3921</p></td>
<td><p>7968</p></td>
<td><p>15780</p></td>
<td><p>~4000</p></td>
</tr>
<tr class="row-odd"><td><p>SpaceInvaders</p></td>
<td><p>650</p></td>
<td><p>1001</p></td>
<td><p>1025</p></td>
<td><p>~500</p></td>
</tr>
</tbody>
</table>
<p><strong>DQN-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="deep-deterministic-policy-gradients-ddpg">
<span id="ddpg"></span><h3>Deep Deterministic Policy Gradients (DDPG)<a class="headerlink" href="#deep-deterministic-policy-gradients-ddpg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1509.02971">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ddpg/ddpg.py">[implementation]</a>
DDPG is implemented similarly to DQN (below). The algorithm can be scaled by increasing the number of workers or using Ape-X.
The improvements from <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">TD3</a> are available as <code class="docutils literal notranslate"><span class="pre">TD3</span></code>.</p>
<figure class="align-default" id="id16">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">DDPG architecture (same as DQN)</span><a class="headerlink" href="#id16" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/pendulum-ddpg.yaml">Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/mountaincarcontinuous-ddpg.yaml">MountainCarContinuous-v0</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/halfcheetah-ddpg.yaml">HalfCheetah-v2</a>.</p>
<p><strong>DDPG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="twin-delayed-ddpg-td3">
<span id="td3"></span><h3>Twin Delayed DDPG (TD3)<a class="headerlink" href="#twin-delayed-ddpg-td3" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1509.02971">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/td3/td3.py">[implementation]</a>
TD3 represents an improvement over DDPG. Its implementation is available in RLlib as <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/td3.html">TD3</a>.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/pendulum-td3.yaml">TD3 Pendulum-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/invertedpendulum-td3.yaml">TD3 InvertedPendulum-v2</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/td3/mujoco-td3.yaml">TD3 Mujoco suite (Ant-v2, HalfCheetah-v2, Hopper-v2, Walker2d-v2)</a>.</p>
<p><strong>TD3-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="soft-actor-critic-sac">
<span id="sac"></span><h3>Soft Actor Critic (SAC)<a class="headerlink" href="#soft-actor-critic-sac" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1801.01290">[original paper]</a>, <a class="reference external" href="https://arxiv.org/pdf/1812.05905.pdf">[follow up paper]</a>, <a class="reference external" href="https://arxiv.org/pdf/1910.07207v2.pdf">[discrete actions paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/sac/sac.py">[implementation]</a></p>
<figure class="align-default" id="id17">
<img alt="../_images/dqn-arch.svg" src="../_images/dqn-arch.svg" /><figcaption>
<p><span class="caption-text">SAC architecture (same as DQN)</span><a class="headerlink" href="#id17" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>RLlib’s soft-actor critic implementation is ported from the <a class="reference external" href="https://github.com/rail-berkeley/softlearning">official SAC repo</a> to better integrate with RLlib APIs.
Note that SAC has two fields to configure for custom models: <code class="docutils literal notranslate"><span class="pre">policy_model_config</span></code> and <code class="docutils literal notranslate"><span class="pre">q_model_config</span></code>, the <code class="docutils literal notranslate"><span class="pre">model</span></code> field of the config will be ignored.</p>
<p>Tuned examples (continuous actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/pendulum-sac.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/halfcheetah-sac.yaml">HalfCheetah-v3</a>,
Tuned examples (discrete actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/sac/cartpole-sac.yaml">CartPole-v1</a></p>
<p><strong>MuJoCo results &#64;3M steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 31%" />
<col style="width: 24%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib SAC</p></th>
<th class="head"><p>Haarnoja et al SAC</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>13000</p></td>
<td><p>~15000</p></td>
</tr>
</tbody>
</table>
<p><strong>SAC-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="model-based-rl">
<h2>Model-based RL<a class="headerlink" href="#model-based-rl" title="Permalink to this headline">#</a></h2>
<section id="dreamerv3">
<span id="id2"></span><h3>DreamerV3<a class="headerlink" href="#dreamerv3" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/2301.04104v1.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamerv3/dreamerv3.py">[implementation]</a></p>
<p>DreamerV3 trains a world model in supervised fashion using real environment
interactions. The world model’s objective is to correctly predict all aspects
of the transition dynamics of the RL environment, which includes (besides predicting the
correct next observations) predicting the received rewards as well as a boolean episode
continuation flag.
A “recurrent state space model” or RSSM is used to alternatingly train the world model
(from actual env data) as well as the critic and actor networks, both of which are trained
on “dreamed” trajectories produced by the world model.</p>
<p>DreamerV3 can be used in all types of environments, including those with image- or vector based
observations, continuous- or discrete actions, as well as sparse or dense reward functions.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dreamerv3/atari_100k.py">Atari 100k</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dreamerv3/atari_200M.py">Atari 200M</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dreamerv3/dm_control_suite_vision.py">DeepMind Control Suite</a></p>
<p><strong>Pong-v5 results (1, 2, and 4 GPUs)</strong>:</p>
<figure class="align-default" id="id18">
<img alt="../_images/pong_1_2_and_4gpus.svg" src="../_images/pong_1_2_and_4gpus.svg" /><figcaption>
<p><span class="caption-text">Episode mean rewards for the Pong-v5 environment (with the “100k” setting, in which only 100k environment steps are allowed):
Note that despite the stable sample efficiency - shown by the constant learning
performance per env step - the wall time improves almost linearly as we go from 1 to 4 GPUs.
<strong>Left</strong>: Episode reward over environment timesteps sampled. <strong>Right</strong>: Episode reward over wall-time.</span><a class="headerlink" href="#id18" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Atari 100k results (1 vs 4 GPUs)</strong>:</p>
<figure class="align-default" id="id19">
<img alt="../_images/atari100k_1_vs_4gpus.svg" src="../_images/atari100k_1_vs_4gpus.svg" /><figcaption>
<p><span class="caption-text">Episode mean rewards for various Atari 100k tasks on 1 vs 4 GPUs.
<strong>Left</strong>: Episode reward over environment timesteps sampled.
<strong>Right</strong>: Episode reward over wall-time.</span><a class="headerlink" href="#id19" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>DeepMind Control Suite (vision) results (1 vs 4 GPUs)</strong>:</p>
<figure class="align-default" id="id20">
<img alt="../_images/dmc_1_vs_4gpus.svg" src="../_images/dmc_1_vs_4gpus.svg" /><figcaption>
<p><span class="caption-text">Episode mean rewards for various Atari 100k tasks on 1 vs 4 GPUs.
<strong>Left</strong>: Episode reward over environment timesteps sampled.
<strong>Right</strong>: Episode reward over wall-time.</span><a class="headerlink" href="#id20" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="dreamer">
<span id="id3"></span><h3>Dreamer<a class="headerlink" href="#dreamer" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1912.01603">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/dreamer/dreamer.py">[implementation]</a></p>
<p>Dreamer is an image-only model-based RL method that learns by imagining trajectories in the future and is evaluated on the DeepMind Control Suite <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/env/dm_control_suite.py">environments</a>. RLlib’s Dreamer is adapted from the <a class="reference external" href="https://github.com/google-research/dreamer">official Google research repo</a>.</p>
<p>To visualize learning, RLlib Dreamer’s imagined trajectories are logged as gifs in TensorBoard. Examples of such can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments">here</a>.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/dreamer/dreamer-deepmind-control.yaml">Deepmind Control Environments</a></p>
<p><strong>Deepmind Control results &#64;1M steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 27%" />
<col style="width: 29%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>DMC env</p></th>
<th class="head"><p>RLlib Dreamer</p></th>
<th class="head"><p>Danijar et al Dreamer</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Walker-Walk</p></td>
<td><p>920</p></td>
<td><p>~930</p></td>
</tr>
<tr class="row-odd"><td><p>Cheetah-Run</p></td>
<td><p>640</p></td>
<td><p>~800</p></td>
</tr>
</tbody>
</table>
<p><strong>Dreamer-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="model-based-meta-policy-optimization-mb-mpo">
<span id="mbmpo"></span><h3>Model-Based Meta-Policy-Optimization (MB-MPO)<a class="headerlink" href="#model-based-meta-policy-optimization-mb-mpo" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1809.05214.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/mbmpo/mbmpo.py">[implementation]</a></p>
<p>RLlib’s MBMPO implementation is a Dyna-styled model-based RL method that learns based on the predictions of an ensemble of transition-dynamics models. Similar to MAML, MBMPO metalearns an optimal policy by treating each dynamics model as a different task. Code here is adapted from <a class="reference external" href="https://github.com/jonasrothfuss/model_ensemble_meta_learning">https://github.com/jonasrothfuss/model_ensemble_meta_learning</a>. Similar to the original paper, MBMPO is evaluated on MuJoCo, with the horizon set to 200 instead of the default 1000.</p>
<p>Additional statistics are logged in MBMPO. Each MBMPO iteration corresponds to multiple MAML iterations, and <code class="docutils literal notranslate"><span class="pre">MAMLIter$i$_DynaTrajInner_$j$_episode_reward_mean</span></code> measures the agent’s returns across the dynamics models at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> of MAML and step <code class="docutils literal notranslate"><span class="pre">j</span></code> of inner adaptation. Examples can be seen <a class="reference external" href="https://github.com/ray-project/rl-experiments/tree/master/mbmpo">here</a>.</p>
<p>Tuned examples (continuous actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/pendulum-mbmpo.yaml">Pendulum-v1</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/halfcheetah-mbmpo.yaml">HalfCheetah</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/hopper-mbmpo.yaml">Hopper</a>,
Tuned examples (discrete actions):
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/mbmpo/cartpole-mbmpo.yaml">CartPole-v1</a></p>
<p><strong>MuJoCo results &#64;100K steps:</strong> <a class="reference external" href="https://github.com/ray-project/rl-experiments">more details</a></p>
<table class="table">
<colgroup>
<col style="width: 29%" />
<col style="width: 27%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>MuJoCo env</p></th>
<th class="head"><p>RLlib MBMPO</p></th>
<th class="head"><p>Clavera et al MBMPO</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>520</p></td>
<td><p>~550</p></td>
</tr>
<tr class="row-odd"><td><p>Hopper</p></td>
<td><p>620</p></td>
<td><p>~650</p></td>
</tr>
</tbody>
</table>
<p><strong>MBMPO-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="derivative-free">
<h2>Derivative-free<a class="headerlink" href="#derivative-free" title="Permalink to this headline">#</a></h2>
<section id="augmented-random-search-ars">
<span id="ars"></span><h3>Augmented Random Search (ARS)<a class="headerlink" href="#augmented-random-search-ars" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.07055">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/ars/ars.py">[implementation]</a>
ARS is a random search method for training linear policies for continuous control problems. Code here is adapted from <a class="reference external" href="https://github.com/modestyachts/ARS">https://github.com/modestyachts/ARS</a> to integrate with RLlib APIs.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ars/cartpole-ars.yaml">CartPole-v1</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ars/swimmer-ars.yaml">Swimmer-v2</a></p>
<p><strong>ARS-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="evolution-strategies-es">
<span id="es"></span><h3>Evolution Strategies (ES)<a class="headerlink" href="#evolution-strategies-es" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a> <a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1703.03864">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/es/es.py">[implementation]</a>
Code here is adapted from <a class="reference external" href="https://github.com/openai/evolution-strategies-starter">https://github.com/openai/evolution-strategies-starter</a> to execute in the distributed setting with Ray.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/es/humanoid-es.yaml">Humanoid-v1</a></p>
<p><strong>Scalability:</strong></p>
<figure class="align-default" id="id21">
<a class="reference internal image-reference" href="../_images/es.png"><img alt="../_images/es.png" src="../_images/es.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-text">RLlib’s ES implementation scales further and is faster than a reference Redis implementation on solving the Humanoid-v1 task.</span><a class="headerlink" href="#id21" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>ES-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="rl-for-recommender-systems">
<h2>RL for recommender systems<a class="headerlink" href="#rl-for-recommender-systems" title="Permalink to this headline">#</a></h2>
<section id="slateq">
<span id="id4"></span><h3>SlateQ<a class="headerlink" href="#slateq" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9f91de1fa0ac351ecb12e4062a37afb896aa1463.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/slateq/slateq.py">[implementation]</a></p>
<p>SlateQ is a model-free RL method that builds on top of DQN and generates recommendation slates for recommender system environments. Since these types of environments come with large combinatorial action spaces, SlateQ mitigates this by decomposing the Q-value into single-item Q-values and solves the decomposed objective via mixing integer programming and deep learning optimization. SlateQ can be evaluated on Google’s RecSim <a class="reference external" href="https://github.com/google-research/recsim">environment</a>. <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/recsim_wrapper.py">An RLlib wrapper for RecSim can be found here &lt;</a>.</p>
<p>RecSim environment wrapper: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/env/wrappers/recsim_wrapper.py">Google RecSim</a></p>
<p><strong>SlateQ-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="contextual-bandits">
<h2>Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permalink to this headline">#</a></h2>
<p id="bandits">The Multi-armed bandit (MAB) problem provides a simplified RL setting that
involves learning to act under one situation only, i.e. the context (observation/state) and arms (actions/items-to-select) are both fixed.
Contextual bandit is an extension of the MAB problem, where at each
round the agent has access not only to a set of bandit arms/actions but also
to a context (state) associated with this iteration. The context changes
with each iteration, but, is not affected by the action that the agent takes.
The objective of the agent is to maximize the cumulative rewards, by
collecting  enough information about how the context and the rewards of the
arms are related to each other. The agent does this by balancing the
trade-off between exploration and exploitation.</p>
<p>Contextual bandit algorithms typically consist of an action-value model (Q
model) and an exploration strategy (epsilon-greedy, LinUCB, Thompson Sampling etc.)</p>
<p>RLlib supports the following online contextual bandit algorithms,
named after the exploration strategies that they employ:</p>
<section id="linear-upper-confidence-bound-banditlinucb">
<span id="lin-ucb"></span><h3>Linear Upper Confidence Bound (BanditLinUCB)<a class="headerlink" href="#linear-upper-confidence-bound-banditlinucb" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="http://rob.schapire.net/papers/www10.pdf">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py">[implementation]</a>
LinUCB assumes a linear dependency between the expected reward of an action and
its context. It estimates the Q value of each action using ridge regression.
It constructs a confidence region around the weights of the linear
regression model and uses this confidence ellipsoid to estimate the
uncertainty of action values.</p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/tests/test_bandits.py">SimpleContextualBandit</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ucb_train_recsim_env.py">UCB Bandit on RecSim</a>.
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ucb_train_recommendation.py">ParametricItemRecoEnv</a>.</p>
<p><strong>LinUCB-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="linear-thompson-sampling-banditlints">
<span id="lints"></span><h3>Linear Thompson Sampling (BanditLinTS)<a class="headerlink" href="#linear-thompson-sampling-banditlints" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="http://proceedings.mlr.press/v28/agrawal13.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/bandit.py">[implementation]</a>
Like LinUCB, LinTS also assumes a linear dependency between the expected
reward of an action and its context and uses online ridge regression to
estimate the Q values of actions given the context. It assumes a Gaussian
prior on the weights and a Gaussian likelihood function. For deciding which
action to take, the agent samples weights for each arm, using
the posterior distributions, and plays the arm that produces the highest reward.</p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/bandit/tests/test_bandits.py">SimpleContextualBandit</a>,
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/bandit/tune_lin_ts_train_wheel_env.py">WheelBandit</a>.</p>
<p><strong>LinTS-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
</section>
<section id="multi-agent">
<h2>Multi-agent<a class="headerlink" href="#multi-agent" title="Permalink to this headline">#</a></h2>
<section id="parameter-sharing">
<span id="parameter"></span><h3>Parameter Sharing<a class="headerlink" href="#parameter-sharing" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="http://ala2017.it.nuigalway.ie/papers/ALA2017_Gupta.pdf">[paper]</a>, <a class="reference external" href="https://arxiv.org/abs/2005.13625">[paper]</a> and <a class="reference external" href="rllib-env.html#multi-agent-and-hierarchical">[instructions]</a>. Parameter sharing refers to a class of methods that take a base single agent method, and use it to learn a single policy for all agents. This simple approach has been shown to achieve state of the art performance in cooperative games, and is usually how you should start trying to learn a multi-agent problem.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/PettingZoo-Team/PettingZoo">PettingZoo</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_parameter_sharing.py">waterworld</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/rock_paper_scissors_multiagent.py">rock-paper-scissors</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">multi-agent cartpole</a></p>
</section>
<section id="qmix-monotonic-value-factorisation-qmix-vdn-iqn">
<span id="qmix"></span><h3>QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)<a class="headerlink" href="#qmix-monotonic-value-factorisation-qmix-vdn-iqn" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1803.11485">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/qmix/qmix.py">[implementation]</a> Q-Mix is a specialized multi-agent algorithm. Code here is adapted from <a class="reference external" href="https://github.com/oxwhirl/pymarl_alpha">https://github.com/oxwhirl/pymarl_alpha</a>  to integrate with RLlib multi-agent APIs. To use Q-Mix, you must specify an agent <a class="reference external" href="rllib-env.html#grouping-agents">grouping</a> in the environment (see the <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">two-step game example</a>). Currently, all agents in the group must be homogeneous. The algorithm can be scaled by increasing the number of workers or using Ape-X.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">Two-step game</a></p>
<p><strong>QMIX-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="multi-agent-deep-deterministic-policy-gradient-maddpg">
<span id="maddpg"></span><h3>Multi-Agent Deep Deterministic Policy Gradient (MADDPG)<a class="headerlink" href="#multi-agent-deep-deterministic-policy-gradient-maddpg" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1706.02275">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/maddpg/maddpg.py">[implementation]</a> MADDPG is a DDPG centralized/shared critic algorithm. Code here is adapted from <a class="reference external" href="https://github.com/openai/maddpg">https://github.com/openai/maddpg</a> to integrate with RLlib multi-agent APIs. Please check <a class="reference external" href="https://github.com/jkterry1/maddpg-rllib">justinkterry/maddpg-rllib</a> for examples and more information. Note that the implementation here is based on OpenAI’s, and is intended for use with the discrete MPE environments. Please also note that people typically find this method difficult to get to work, even with all applicable optimizations for their environment applied. This method should be viewed as for research purposes, and for reproducing the results of the paper introducing it.</p>
<p><strong>MADDPG-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/wsjeon/maddpg-rllib/tree/master/plots">Multi-Agent Particle Environment</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/two_step_game.py">Two-step game</a></p>
</section>
<section id="shared-critic-methods">
<span id="sc"></span><h3>Shared Critic Methods<a class="headerlink" href="#shared-critic-methods" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://docs.ray.io/en/master/rllib-env.html#implementing-a-centralized-critic">[instructions]</a> Shared critic methods are when all agents use a single parameter shared critic network (in some cases with access to more of the observation space than agents can see). Note that many specialized multi-agent algorithms such as MADDPG are mostly shared critic forms of their single-agent algorithm (DDPG in the case of MADDPG).</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/centralized_critic_2.py">TwoStepGame</a></p>
</section>
</section>
<section id="others">
<h2>Others<a class="headerlink" href="#others" title="Permalink to this headline">#</a></h2>
<section id="single-player-alpha-zero-alphazero">
<span id="alphazero"></span><h3>Single-Player Alpha Zero (AlphaZero)<a class="headerlink" href="#single-player-alpha-zero-alphazero" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/abs/1712.01815">[paper]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/alpha_zero">[implementation]</a> AlphaZero is an RL agent originally designed for two-player games. This version adapts it to handle single player games. The code can be scaled to any number of workers. It also implements the ranked rewards <a class="reference external" href="https://arxiv.org/abs/1807.01672">(R2)</a> strategy to enable self-play even in the one-player setting. The code is mainly purposed to be used for combinatorial optimization.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/alpha_zero/cartpole-sparse-rewards-alpha-zero.yaml">Sparse reward CartPole</a></p>
<p><strong>AlphaZero-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="multiagent-leelachesszero-leelachesszero">
<span id="leelachesszero"></span><h3>MultiAgent LeelaChessZero (LeelaChessZero)<a class="headerlink" href="#multiagent-leelachesszero-leelachesszero" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://github.com/LeelaChessZero/lc0/">[source]</a> <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/algorithms/leela_chess_zero">[implementation]</a> LeelaChessZero is an RL agent originally inspired by AlphaZero for playing chess. This version adapts it to handle a MultiAgent competitive environment of chess. The code can be scaled to any number of workers.</p>
<p>Tuned examples: tbd</p>
<p><strong>LeelaChessZero-specific configs</strong> (see also <a class="reference external" href="rllib-training.html#common-parameters">common configs</a>):</p>
</section>
<section id="curiosity-icm-intrinsic-curiosity-module">
<span id="curiosity"></span><h3>Curiosity (ICM: Intrinsic Curiosity Module)<a class="headerlink" href="#curiosity-icm-intrinsic-curiosity-module" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/pytorch.png"><img alt="pytorch" class="inline-figure" src="../_images/pytorch.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/curiosity.py">[implementation]</a></p>
<p>Tuned examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/unity3d_env_local.py">Pyramids (Unity3D)</a> (use <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">Pyramids</span></code> command line option)
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/tests/test_curiosity.py#L184">Test case with MiniGrid example</a> (UnitTest case: <code class="docutils literal notranslate"><span class="pre">test_curiosity_on_partially_observable_domain</span></code>)</p>
<p><strong>Activating Curiosity</strong>
The curiosity plugin can be easily activated by specifying it as the Exploration class to-be-used
in the main Algorithm config. Most of its parameters usually do not have to be specified
as the module uses the values from the paper by default. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">ppo</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Curiosity&quot;</span><span class="p">,</span>  <span class="c1"># &lt;- Use the Curiosity module for exploring.</span>
    <span class="s2">&quot;eta&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Weight for intrinsic rewards before being added to extrinsic ones.</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>  <span class="c1"># Learning rate of the curiosity (ICM) module.</span>
    <span class="s2">&quot;feature_dim&quot;</span><span class="p">:</span> <span class="mi">288</span><span class="p">,</span>  <span class="c1"># Dimensionality of the generated feature vectors.</span>
    <span class="c1"># Setup of the feature net (used to encode observations into feature (latent) vectors).</span>
    <span class="s2">&quot;feature_net_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;fcnet_hiddens&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;fcnet_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;inverse_net_hiddens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">],</span>  <span class="c1"># Hidden layers of the &quot;inverse&quot; model.</span>
    <span class="s2">&quot;inverse_net_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>  <span class="c1"># Activation of the &quot;inverse&quot; model.</span>
    <span class="s2">&quot;forward_net_hiddens&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">],</span>  <span class="c1"># Hidden layers of the &quot;forward&quot; model.</span>
    <span class="s2">&quot;forward_net_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>  <span class="c1"># Activation of the &quot;forward&quot; model.</span>
    <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Weight for the &quot;forward&quot; loss (beta) over the &quot;inverse&quot; loss (1.0 - beta).</span>
    <span class="c1"># Specify, which exploration sub-type to use (usually, the algo&#39;s &quot;default&quot;</span>
    <span class="c1"># exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).</span>
    <span class="s2">&quot;sub_exploration&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Functionality</strong>
RLlib’s Curiosity is based on <a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">“ICM” (intrinsic curiosity module) described in this paper here</a>.
It allows agents to learn in sparse-reward- or even no-reward environments by
calculating so-called “intrinsic rewards”, purely based on the information content that is incoming via the observation channel.
Sparse-reward environments are envs where almost all reward signals are 0.0, such as these <a class="reference external" href="https://github.com/maximecb/gym-minigrid">[MiniGrid env examples here]</a>.
In such environments, agents have to navigate (and change the underlying state of the environment) over long periods of time, without receiving much (or any) feedback.
For example, the task could be to find a key in some room, pick it up, find a matching door (matching the color of the key), and eventually unlock this door with the key to reach a goal state,
all the while not seeing any rewards.
Such problems are impossible to solve with standard RL exploration methods like epsilon-greedy or stochastic sampling.
The Curiosity module - when configured as the Exploration class to use via the Algorithm’s config (see above on how to do this) - automatically adds three simple models to the Policy’s <code class="docutils literal notranslate"><span class="pre">self.model</span></code>:
a) a latent space learning (“feature”) model, taking an environment observation and outputting a latent vector, which represents this observation and
b) a “forward” model, predicting the next latent vector, given the current observation vector and an action to take next.
c) a so-called “inverse” net, only used to train the “feature” net. The inverse net tries to predict the action taken between two latent vectors (obs and next obs).</p>
<p>All the above extra Models are trained inside the modified <code class="docutils literal notranslate"><span class="pre">Exploration.postprocess_trajectory()</span></code> call.</p>
<p>Using the (ever changing) “forward” model, our Curiosity module calculates an artificial (intrinsic) reward signal, weights it via the <code class="docutils literal notranslate"><span class="pre">eta</span></code> parameter, and then adds it to the environment’s (extrinsic) reward.
Intrinsic rewards for each env-step are calculated by taking the euclidian distance between the latent-space encoded next observation (“feature” model) and the <strong>predicted</strong> latent-space encoding for the next observation
(“forward” model).
This allows the agent to explore areas of the environment, where the “forward” model still performs poorly (are not “understood” yet), whereas exploration to these areas will taper down after the agent has visited them
often: The “forward” model will eventually get better at predicting these next latent vectors, which in turn will diminish the intrinsic rewards (decrease the euclidian distance between predicted and actual vectors).</p>
</section>
<section id="re3-random-encoders-for-efficient-exploration">
<span id="re3"></span><h3>RE3 (Random Encoders for Efficient Exploration)<a class="headerlink" href="#re3-random-encoders-for-efficient-exploration" title="Permalink to this headline">#</a></h3>
<p><a class="inline-figure reference internal" href="../_images/tensorflow.png"><img alt="tensorflow" class="inline-figure" src="../_images/tensorflow.png" style="width: 24px;" /></a>
<a class="reference external" href="https://arxiv.org/pdf/2102.09430.pdf">[paper]</a>
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/random_encoder.py">[implementation]</a></p>
<p>Examples:
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/re3_exploration.py">LunarLanderContinuous-v2</a> (use <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">LunarLanderContinuous-v2</span></code> command line option)
<a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/utils/exploration/tests/test_random_encoder.py">Test case with Pendulum-v1 example</a></p>
<p><strong>Activating RE3</strong>
The RE3 plugin can be easily activated by specifying it as the Exploration class to-be-used
in the main Algorithm config and inheriting the <code class="xref py py-obj docutils literal notranslate"><span class="pre">RE3UpdateCallbacks</span></code> as shown in this <a class="reference external" href="https://github.com/ray-project/ray/blob/c9c3f0745a9291a4de0872bdfa69e4ffdfac3657/rllib/utils/exploration/tests/test_random_encoder.py#L35">example</a>. Most of its parameters usually do not have to be specified as the module uses the values from the paper by default. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">sac</span><span class="o">.</span><span class="n">DEFAULT_CONFIG</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;Pendulum-v1&quot;</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12345</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">RE3Callbacks</span>
<span class="n">config</span><span class="p">[</span><span class="s2">&quot;exploration_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;RE3&quot;</span><span class="p">,</span>
     <span class="c1"># the dimensionality of the observation embedding vectors in latent space.</span>
     <span class="s2">&quot;embeds_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
     <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Beta decay factor, used for on-policy algorithm.</span>
     <span class="s2">&quot;k_nn&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="c1"># Number of neighbours to set for K-NN entropy estimation.</span>
     <span class="c1"># Configuration for the encoder network, producing embedding vectors from observations.</span>
     <span class="c1"># This can be used to configure fcnet- or conv_net setups to properly process any</span>
     <span class="c1"># observation space. By default uses the Policy model configuration.</span>
     <span class="s2">&quot;encoder_net_config&quot;</span><span class="p">:</span> <span class="p">{</span>
         <span class="s2">&quot;fcnet_hiddens&quot;</span><span class="p">:</span> <span class="p">[],</span>
         <span class="s2">&quot;fcnet_activation&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
     <span class="p">},</span>
     <span class="c1"># Hyperparameter to choose between exploration and exploitation. A higher value of beta adds</span>
     <span class="c1"># more importance to the intrinsic reward, as per the following equation</span>
     <span class="c1"># `reward = r + beta * intrinsic_reward`</span>
     <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
     <span class="c1"># Schedule to use for beta decay, one of constant&quot; or &quot;linear_decay&quot;.</span>
     <span class="s2">&quot;beta_schedule&quot;</span><span class="p">:</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span>
     <span class="c1"># Specify, which exploration sub-type to use (usually, the algo&#39;s &quot;default&quot;</span>
     <span class="c1"># exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).</span>
     <span class="s2">&quot;sub_exploration&quot;</span><span class="p">:</span> <span class="p">{</span>
         <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;StochasticSampling&quot;</span><span class="p">,</span>
     <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Functionality</strong>
RLlib’s RE3 is based on <a class="reference external" href="https://arxiv.org/pdf/2102.09430.pdf">“Random Encoders for Efficient Exploration” described in this paper here</a>.
RE3 quantifies exploration based on state entropy. The entropy of a state is calculated based on its distance from K nearest neighbor states present in the replay buffer in the latent space (With this implementation, KNN is implemented using training samples from the same batch).
The state entropy is considered as an intrinsic reward and for policy optimization added to the extrinsic reward when available.  If the extrinsic reward is not available then the state entropy is used as “intrinsic reward” for unsupervised pre-training of the RL agent.
RE3 further allows agents to learn in sparse-reward or even no-reward environments by
using the state entropy as “intrinsic rewards”.</p>
<p>This exploration objective can be used with both model-free and model-based RL algorithms.
RE3 uses a randomly initialized encoder to get the state’s latent representation, thus taking away the complexity of training the representation learning method. The encoder weights are fixed during the entire duration of the training process.</p>
</section>
<section id="fully-independent-learning">
<span id="fil"></span><h3>Fully Independent Learning<a class="headerlink" href="#fully-independent-learning" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="rllib-env.html#multi-agent-and-hierarchical">[instructions]</a> Fully independent learning involves a collection of agents learning independently of each other via single agent methods. This typically works, but can be less effective than dedicated multi-agent RL methods, since they do not account for the non-stationarity of the multi-agent environment.</p>
<p>Tuned examples: <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_independent_learning.py">waterworld</a>, <a class="reference external" href="https://github.com/ray-project/ray/blob/master/rllib/examples/multi_agent_cartpole.py">multiagent-cartpole</a></p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="rllib-env.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">环境</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="user-guides.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">用户指南</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>谢谢你的反馈！</span>
  </div>
  <div id="csat-inputs">
    <span>是否能帮助到你？</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>是<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>否<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">反馈</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">提交</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2024, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>