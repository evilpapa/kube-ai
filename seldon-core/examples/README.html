<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../_static/javascripts/modernizr.js"></script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-54780881-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'UA-54780881-2');
</script>
  
  
    <title>Custom pre-processors with the V2 protocol &#8212; seldon-core  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=83e35b93" />
    <link rel="stylesheet" type="text/css" href="../_static/material.css?v=79c92029" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/rtd_sphinx_search.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/theme_overrides.css?v=15a8f09d" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/rtd_search_config.js"></script>
    <script src="../_static/js/rtd_sphinx_search.min.js"></script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=indigo data-md-color-accent=teal>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#examples/README" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../index.html" title="seldon-core  documentation"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Seldon Core Documentation</span>
          <span class="md-header-nav__topic"> Custom pre-processors with the V2 protocol </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/SeldonIO/seldon-core/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Seldon Core
  </div>
</a>
          </div>
        </div>
      
      
    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
            
            <li class="md-tabs__item"><a href="/" class="md-tabs__link">🚀 Our Other Projects & Products:</a></li>
            
            <li class="md-tabs__item"><a href="https://docs.seldon.io/projects/alibi/en/stable/" class="md-tabs__link">Alibi Explain</a></li>
            
            <li class="md-tabs__item"><a href="https://docs.seldon.io/projects/alibi-detect/en/stable/" class="md-tabs__link">Alibi Detect</a></li>
            
            <li class="md-tabs__item"><a href="https://mlserver.readthedocs.io/en/latest/" class="md-tabs__link">MLServer</a></li>
            
            <li class="md-tabs__item"><a href="https://tempo.readthedocs.io/en/latest/" class="md-tabs__link">Tempo SDK</a></li>
            
            <li class="md-tabs__item"><a href="https://deploy.seldon.io" class="md-tabs__link">Seldon Enterprise Platform</a></li>
            
            <li class="md-tabs__item"><a href="https://github.com/SeldonIO/seldon-deploy-sdk#seldon-deploy-sdk" class="md-tabs__link">Seldon Enterprise Platform SDK</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../index.html" title="seldon-core documentation" class="md-nav__button md-logo">
      
        <img src="../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../index.html"
       title="seldon-core documentation">Seldon Core Documentation</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/SeldonIO/seldon-core/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Seldon Core
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
    
      <a href="../nav/getting-started.html" class="md-nav__link">开始</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../nav/concepts.html" class="md-nav__link">概念</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../nav/configuration.html" class="md-nav__link">配置</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../nav/tutorials.html" class="md-nav__link">教程</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../nav/reference.html" class="md-nav__link">参考</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../nav/contributing.html" class="md-nav__link">贡献</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#examples-readme--page-root" class="md-nav__link">Custom pre-processors with the V2 protocol</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#creating-a-tokeniser" class="md-nav__link">Creating a Tokeniser</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#requirements-and-default-model-settings" class="md-nav__link">Requirements and default model settings</a>
        </li>
        <li class="md-nav__item"><a href="#testing-our-tokeniser" class="md-nav__link">Testing our tokeniser</a>
        </li>
        <li class="md-nav__item"><a href="#building-the-image" class="md-nav__link">Building the image</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#deploying-our-inference-graph" class="md-nav__link">Deploying our inference graph</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#testing-our-deployed-inference-graph" class="md-nav__link">Testing our deployed inference graph</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="../_sources/examples/README.md.txt">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <section id="custom-pre-processors-with-the-v2-protocol">
<h1 id="examples-readme--page-root">Custom pre-processors with the V2 protocol<a class="headerlink" href="#examples-readme--page-root" title="Permalink to this heading">¶</a></h1>
<p>Most of the time, the requests that we send to our model need some kind of processing.
For example, extra information may need to be fetched (e.g. from a feature store), or processed, in order to obtain the actual tensors required by the model. One example for this use case are NLP models, where natural language needs first to be tokenised according to a vocabulary, or embedded by a 2nd model.</p>
<p>In this tutorial, we will focus on this latter scenario.
In particular, we will explore how to deploy a <em>tokeniser</em> pre-transformer that converts our natural language text to tokens.
This tokeniser will then be part of an inference graph, so that its output gets routed to a <a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/triton_gpt2_example.html">GPT-2 model deployed using Triton</a>.</p>
<blockquote>
<div><p><strong>NOTE</strong>: The tokeniser logic and the Triton artifacts are taken from the <a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/triton_gpt2_example.html">GPT-2 Model example</a>. To learn more about these, feel free to check that tutorial.</p>
</div></blockquote>
<a class="reference external image-reference" href="./gpt2-graph.svg"><img alt="Inference graph with tokeniser and GPT-2 model" src="../_images/gpt2-graph.svg"/></a>
<section id="creating-a-tokeniser">
<h2 id="creating-a-tokeniser">Creating a Tokeniser<a class="headerlink" href="#creating-a-tokeniser" title="Permalink to this heading">¶</a></h2>
<p>In order to create a custom pre-processing step, the first step will be to <a class="reference external" href="https://mlserver.readthedocs.io/en/latest/runtimes/custom.html">write a **custom runtime**</a> using <a class="reference external" href="https://mlserver.readthedocs.io/en/latest/">MLServer</a>.
MLServer is a production-grade inference server, whose main goal is to ease up the serving of models through a REST and gRPC interface compatible with the <a class="reference external" href="https://kserve.github.io/website/modelserving/inference_api/">V2 Inference Protocol</a>.</p>
<p>As well as an inference server, MLServer also exposes a <em>framework</em> which can be leveraged to easily <strong>write your custom inference runtimes</strong>.
These custom runtimes can be used to write any custom logic, including (you guessed it!) our tokeniser pre-processor.
Therefore, we will start by extending the base <code class="docutils literal notranslate"><span class="pre">mlserver.MLModel</span></code> class, adding our custom logic.
Note that this logic is taken (almost) verbatim from the <a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/triton_gpt2_example.html">GPT-2 Model example</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">runtime</span><span class="o">.</span><span class="n">py</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mlserver</span> <span class="kn">import</span> <span class="n">MLModel</span>
<span class="kn">from</span> <span class="nn">mlserver.types</span> <span class="kn">import</span> <span class="n">InferenceRequest</span><span class="p">,</span> <span class="n">InferenceResponse</span>
<span class="kn">from</span> <span class="nn">mlserver.codecs</span> <span class="kn">import</span> <span class="n">NumpyCodec</span>
<span class="kn">from</span> <span class="nn">mlserver.codecs.string</span> <span class="kn">import</span> <span class="n">StringRequestCodec</span><span class="p">,</span> <span class="n">StringCodec</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>

<span class="n">TOKENIZER_TYPE_ENV_NAME</span> <span class="o">=</span> <span class="s2">"SELDON_TOKENIZER_TYPE"</span>
<span class="n">TOKENIZER_TYPE_ENCODE</span> <span class="o">=</span> <span class="s2">"ENCODER"</span>

<span class="k">class</span> <span class="nc">Tokeniser</span><span class="p">(</span><span class="n">MLModel</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokeniser</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer_type</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">TOKENIZER_TYPE_ENV_NAME</span><span class="p">,</span> <span class="n">TOKENIZER_TYPE_ENCODE</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ready</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ready</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_request</span><span class="p">:</span> <span class="n">InferenceRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceResponse</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer_type</span> <span class="o">==</span> <span class="n">TOKENIZER_TYPE_ENCODE</span><span class="p">:</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="n">StringRequestCodec</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inference_request</span><span class="p">)</span>
            <span class="n">tokenised</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokeniser</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"np"</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">payload</span> <span class="ow">in</span> <span class="n">tokenised</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">inference_output</span> <span class="o">=</span> <span class="n">NumpyCodec</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">payload</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
                <span class="c1"># Transformer's TF GPT2 model expects `INT32` inputs by default, so</span>
                <span class="c1"># let's enforce them</span>
                <span class="n">inference_output</span><span class="o">.</span><span class="n">datatype</span> <span class="o">=</span> <span class="s2">"INT32"</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inference_output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">NumpyCodec</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inference_request</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># take the best next token probability of the last token of input ( greedy approach)</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">next_token_str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokeniser</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                <span class="n">next_token</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">StringCodec</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"next_token"</span><span class="p">,</span> <span class="p">[</span><span class="n">next_token_str</span><span class="p">])]</span>

        <span class="k">return</span> <span class="n">InferenceResponse</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">version</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span>
        <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Overwriting</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">runtime</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Note that the pre-processing logic is implemented in the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method.
At the moment, the MLServer framework doesn’t expose the concept of pre- and post-processing.
However, it’s possible to implement this is a <em>“pseudo-model”</em>, thus relying on the service orchestrator of Seldon Core, who will be responsible of chaining the output of our tokeniser to the next model.</p>
<section id="requirements-and-default-model-settings">
<h3 id="requirements-and-default-model-settings">Requirements and default model settings<a class="headerlink" href="#requirements-and-default-model-settings" title="Permalink to this heading">¶</a></h3>
<p>Besides writing the logic of our custom runtime, we will also need to provide the extra requirements that will be used by our environment.
This can be done through a plain <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file.
Alternatively, for a finer control, it’d also be possible to leverage <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually">Conda’s environment files</a> to specify our environment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
<span class="n">mlserver</span><span class="o">==</span><span class="mf">1.0.1</span>
<span class="n">transformers</span><span class="o">==</span><span class="mf">4.12.3</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Overwriting</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>On top of this, we will also add a <code class="docutils literal notranslate"><span class="pre">model-settings.json</span></code> file with the default settings for our model.
MLServer uses these files to provide extra configuration (e.g. number of parallel workers, adaptive batching configuration, etc.) for each model.
In our case, we will use this file to tell MLServer that it should always use our custom runtime by default and name our models as <code class="docutils literal notranslate"><span class="pre">tokeniser</span></code> (unless other name is specified).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">model</span><span class="o">-</span><span class="n">settings</span><span class="o">.</span><span class="n">json</span>
<span class="p">{</span>
  <span class="s2">"implementation"</span><span class="p">:</span> <span class="s2">"runtime.Tokeniser"</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Overwriting</span> <span class="n">tokeniser</span><span class="o">/</span><span class="n">model</span><span class="o">-</span><span class="n">settings</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</section>
<section id="testing-our-tokeniser">
<h3 id="testing-our-tokeniser">Testing our tokeniser<a class="headerlink" href="#testing-our-tokeniser" title="Permalink to this heading">¶</a></h3>
<blockquote>
<div><dl class="simple">
<dt><strong>NOTE</strong><span class="classifier">To test our custom runtime locally, we will need to install the same set of dependencies that will be bundled and deployed remotely.</span></dt><dd><p>To achieve this, we can re-use the environment that was described on the previous section:</p>
</dd>
</dl>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>./tokeniser/requirements.txt
</pre></div>
</div>
<p>Since we’re leveraging MLServer to write our custom pre-processor, it should be <strong>easy to test it locally</strong>.
For this, we will start MLServer using the <cite>``mlserver start`</cite> subcommand &lt;<a class="reference external" href="https://mlserver.readthedocs.io/en/latest/reference/cli.html#mlserver-start">https://mlserver.readthedocs.io/en/latest/reference/cli.html#mlserver-start</a>&gt;`__.
Note that this command has to be carried out on a separate terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlserver<span class="w"> </span>start<span class="w"> </span>./tokeniser
</pre></div>
</div>
<p>We can then send a test request using <code class="docutils literal notranslate"><span class="pre">curl</span></code> as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%%bash
curl<span class="w"> </span>localhost:8080/v2/models/tokeniser/infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s1">'Content-Type: application/json'</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">'{"inputs": [{"name": "sentences", "datatype": "BYTES", "shape": [1, 11], "data": "hello world"}]}'</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="p">|</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>json.tool
</pre></div>
</div>
<p>As we can see above, the input <code class="docutils literal notranslate"><span class="pre">hello</span> <span class="pre">world</span></code> gets tokenised into <code class="docutils literal notranslate"><span class="pre">[31373,</span> <span class="pre">995]</span></code>, thus confirming that our custom runtime is working as expected locally.</p>
</section>
<section id="building-the-image">
<h3 id="building-the-image">Building the image<a class="headerlink" href="#building-the-image" title="Permalink to this heading">¶</a></h3>
<p>Once we have our custom code tested and ready, we should be able to build our custom image by using the <cite>``mlserver build`</cite> subcommand &lt;<a class="reference external" href="https://mlserver.readthedocs.io/en/latest/reference/cli.html#mlserver-build">https://mlserver.readthedocs.io/en/latest/reference/cli.html#mlserver-build</a>&gt;`__.
This image will be created under the <code class="docutils literal notranslate"><span class="pre">gpt2-tokeniser:0.1.0</span></code> tag.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%%bash
mlserver<span class="w"> </span>build<span class="w"> </span>./tokeniser<span class="w"> </span>--tag<span class="w"> </span>seldonio/gpt2-tokeniser:0.1.0
</pre></div>
</div>
</section>
</section>
<section id="deploying-our-inference-graph">
<h2 id="deploying-our-inference-graph">Deploying our inference graph<a class="headerlink" href="#deploying-our-inference-graph" title="Permalink to this heading">¶</a></h2>
<p>Now that we have our custom tokeniser built and ready, we are able to deploy it alongside our GPT-2 model.
This can be achieved through a <code class="docutils literal notranslate"><span class="pre">SeldonDeployment</span></code> manifest which <strong>links both models</strong>.
That is, our tokeniser, plus the actual GPT-2 model.</p>
<p>As outlined above, this manifest will re-use the image and resources built in the <a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/triton_gpt2_example.html">GPT-2 Model example</a>, which is accessible from GCS.</p>
<blockquote>
<div><p><strong>NOTE:</strong> This manifest expects that the <code class="docutils literal notranslate"><span class="pre">gpt2-tokeniser:0.1.0</span></code> image built in the previous section <strong>is accessible</strong> from within the cluster where Seldon Core has been installed. If you are <cite>using ``kind`</cite> &lt;<a class="reference external" href="https://docs.seldon.io/projects/seldon-core/en/latest/install/kind.html">https://docs.seldon.io/projects/seldon-core/en/latest/install/kind.html</a>&gt;`__, you should be able to load the image into your local cluster with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kind<span class="w"> </span>load<span class="w"> </span>docker-image<span class="w"> </span>gpt2-tokeniser:0.1.0
</pre></div>
</div>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">seldondeployment</span><span class="o">.</span><span class="n">yaml</span>
<span class="n">apiVersion</span><span class="p">:</span> <span class="n">machinelearning</span><span class="o">.</span><span class="n">seldon</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">v1</span>
<span class="n">kind</span><span class="p">:</span> <span class="n">SeldonDeployment</span>
<span class="n">metadata</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">gpt2</span>
<span class="n">spec</span><span class="p">:</span>
  <span class="n">protocol</span><span class="p">:</span> <span class="n">v2</span>
  <span class="n">predictors</span><span class="p">:</span>
    <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">default</span>
      <span class="n">graph</span><span class="p">:</span>
        <span class="n">name</span><span class="p">:</span> <span class="n">tokeniser</span><span class="o">-</span><span class="n">encoder</span>
        <span class="n">children</span><span class="p">:</span>
          <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">gpt2</span>
            <span class="n">implementation</span><span class="p">:</span> <span class="n">TRITON_SERVER</span>
            <span class="n">modelUri</span><span class="p">:</span> <span class="n">gs</span><span class="p">:</span><span class="o">//</span><span class="n">seldon</span><span class="o">-</span><span class="n">models</span><span class="o">/</span><span class="n">triton</span><span class="o">/</span><span class="n">onnx_gpt2</span>
            <span class="n">children</span><span class="p">:</span>
              <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">tokeniser</span><span class="o">-</span><span class="n">decoder</span>
      <span class="n">componentSpecs</span><span class="p">:</span>
        <span class="o">-</span> <span class="n">spec</span><span class="p">:</span>
            <span class="n">containers</span><span class="p">:</span>
              <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">tokeniser</span><span class="o">-</span><span class="n">encoder</span>
                <span class="n">image</span><span class="p">:</span> <span class="n">seldonio</span><span class="o">/</span><span class="n">gpt2</span><span class="o">-</span><span class="n">tokeniser</span><span class="p">:</span><span class="mf">0.1.0</span>
                <span class="n">env</span><span class="p">:</span>
                  <span class="c1"># Use always a writable HuggingFace cache location regardless of the user</span>
                  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">TRANSFORMERS_CACHE</span>
                    <span class="n">value</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">mlserver</span><span class="o">/.</span><span class="n">cache</span>
                  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">MLSERVER_MODEL_NAME</span>
                    <span class="n">value</span><span class="p">:</span> <span class="s2">"tokeniser-encoder"</span>
              <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">tokeniser</span><span class="o">-</span><span class="n">decoder</span>
                <span class="n">image</span><span class="p">:</span> <span class="n">seldonio</span><span class="o">/</span><span class="n">gpt2</span><span class="o">-</span><span class="n">tokeniser</span><span class="p">:</span><span class="mf">0.1.0</span>
                <span class="n">env</span><span class="p">:</span>
                  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">SELDON_TOKENIZER_TYPE</span>
                    <span class="n">value</span><span class="p">:</span> <span class="s2">"DECODER"</span>
                  <span class="c1"># Use always a writable HuggingFace cache location regardless of the user</span>
                  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">TRANSFORMERS_CACHE</span>
                    <span class="n">value</span><span class="p">:</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">mlserver</span><span class="o">/.</span><span class="n">cache</span>
                  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">MLSERVER_MODEL_NAME</span>
                    <span class="n">value</span><span class="p">:</span> <span class="s2">"tokeniser-decoder"</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Overwriting</span> <span class="n">seldondeployment</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
<p>The final step will be to apply this manifest into the cluster, where Seldon Core is running.
For example, to deploy the manifest into the <code class="docutils literal notranslate"><span class="pre">models</span></code> namespace, we could run the following command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!kubectl apply -f seldondeployment.yaml
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">seldondeployment</span><span class="o">.</span><span class="n">machinelearning</span><span class="o">.</span><span class="n">seldon</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">gpt2</span> <span class="n">configured</span>
</pre></div>
</div>
<section id="testing-our-deployed-inference-graph">
<h3 id="testing-our-deployed-inference-graph">Testing our deployed inference graph<a class="headerlink" href="#testing-our-deployed-inference-graph" title="Permalink to this heading">¶</a></h3>
<p>Finally, we can test that our deployed inference graph is working as expected by sending a request.
If we assume that our cluster can be reached in <code class="docutils literal notranslate"><span class="pre">localhost:8003</span></code>, we can send a request using <code class="docutils literal notranslate"><span class="pre">cURL</span></code> as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%%bash
curl<span class="w"> </span>localhost:80/seldon/default/gpt2/v2/models/infer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s1">'Content-Type: application/json'</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">'{"inputs": [{"name": "sentences", "datatype": "BYTES", "shape": [1, 11], "data": ["Seldon Technologies is very"]}]}'</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">"model_name"</span><span class="p">:</span><span class="s2">"tokeniser-decoder"</span><span class="p">,</span><span class="s2">"model_version"</span><span class="p">:</span><span class="n">null</span><span class="p">,</span><span class="s2">"id"</span><span class="p">:</span><span class="s2">"6f952cc2-3648-4180-bd70-163a731bdb86"</span><span class="p">,</span><span class="s2">"parameters"</span><span class="p">:</span><span class="n">null</span><span class="p">,</span><span class="s2">"outputs"</span><span class="p">:[{</span><span class="s2">"name"</span><span class="p">:</span><span class="s2">"next_token"</span><span class="p">,</span><span class="s2">"shape"</span><span class="p">:[</span><span class="mi">1</span><span class="p">],</span><span class="s2">"datatype"</span><span class="p">:</span><span class="s2">"BYTES"</span><span class="p">,</span><span class="s2">"parameters"</span><span class="p">:</span><span class="n">null</span><span class="p">,</span><span class="s2">"data"</span><span class="p">:[</span><span class="s2">"pleased"</span><span class="p">]}]}</span>

  <span class="o">%</span> <span class="n">Total</span>    <span class="o">%</span> <span class="n">Received</span> <span class="o">%</span> <span class="n">Xferd</span>  <span class="n">Average</span> <span class="n">Speed</span>   <span class="n">Time</span>    <span class="n">Time</span>     <span class="n">Time</span>  <span class="n">Current</span>
                                 <span class="n">Dload</span>  <span class="n">Upload</span>   <span class="n">Total</span>   <span class="n">Spent</span>    <span class="n">Left</span>  <span class="n">Speed</span>
<span class="mi">100</span>   <span class="mi">334</span>  <span class="mi">100</span>   <span class="mi">219</span>  <span class="mi">100</span>   <span class="mi">115</span>     <span class="mi">27</span>     <span class="mi">14</span>  <span class="mi">0</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">08</span>  <span class="mi">0</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">08</span> <span class="o">--</span><span class="p">:</span><span class="o">--</span><span class="p">:</span><span class="o">--</span>    <span class="mi">57</span>
</pre></div>
</div>
<p>As we can see above, our plain-text request is now going successfully through the <code class="docutils literal notranslate"><span class="pre">tokeniser</span></code>, acting as a pre-processor, whose output then gets routed to the actual GPT-2 model.</p>
</section>
</section>
</section>


          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2024, Seldon Technologies Ltd.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 7.1.2.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>